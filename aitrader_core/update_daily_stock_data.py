#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¯æ—¥è‚¡ç¥¨æ•°æ®å¢é‡æ›´æ–°è„šæœ¬
åŠŸèƒ½ï¼š
1. è‡ªåŠ¨è·å–æœ€æ–°äº¤æ˜“æ—¥
2. å¢é‡æ›´æ–°æ‰€æœ‰è‚¡ç¥¨çš„æœ€æ–°æ•°æ®ï¼ˆAè‚¡å…¨é‡ï¼‰
3. æ”¯æŒä¼ä¸šå¾®ä¿¡æ¨é€é€šçŸ¥

æ•°æ®æ¥æºï¼šTushare
é€‚ç”¨äºï¼šaitrader_v3.3é¡¹ç›®
"""

import tushare as ts
import pandas as pd
import os
import sys
import io
import time
import requests
import json
from datetime import datetime, timedelta
import logging
import re

# è®¾ç½®æ ‡å‡†è¾“å‡ºç¼–ç ä¸ºUTF-8
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

# è·å–é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))

# é…ç½®æ—¥å¿—
log_dir = os.path.join(PROJECT_ROOT, 'logs')
os.makedirs(log_dir, exist_ok=True)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(os.path.join(log_dir, 'daily_update.log')),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# é…ç½®å‚æ•°
# ä¼˜å…ˆçº§ï¼šç¯å¢ƒå˜é‡ > é¡¹ç›®ç›®å½• > ç”¨æˆ·ç›®å½•
if 'STOCK_DATA_DIR' in os.environ:
    STOCK_DATA_DIR = os.environ['STOCK_DATA_DIR']
    logger.info(f"âœ… ä½¿ç”¨ç¯å¢ƒå˜é‡æŒ‡å®šçš„æ•°æ®ç›®å½•: {STOCK_DATA_DIR}")
else:
    # ä¼˜å…ˆä½¿ç”¨é¡¹ç›®ç›®å½•ï¼ˆç»Ÿä¸€äº‘ç«¯å’Œæœ¬åœ°ï¼‰
    PROJECT_STOCK_DATA_DIR = os.path.join(os.path.dirname(PROJECT_ROOT), "data", "stock_data")
    USER_STOCK_DATA_DIR = os.path.expanduser("~/stock_data")
    
    if os.path.exists(PROJECT_STOCK_DATA_DIR):
        STOCK_DATA_DIR = PROJECT_STOCK_DATA_DIR
        logger.info(f"âœ… ä½¿ç”¨é¡¹ç›®ç›®å½•æ•°æ®: {STOCK_DATA_DIR}")
    elif os.path.exists(USER_STOCK_DATA_DIR):
        STOCK_DATA_DIR = USER_STOCK_DATA_DIR
        logger.info(f"âœ… ä½¿ç”¨ç”¨æˆ·ç›®å½•æ•°æ®: {STOCK_DATA_DIR}")
    else:
        # é»˜è®¤åˆ›å»ºé¡¹ç›®ç›®å½•
        STOCK_DATA_DIR = PROJECT_STOCK_DATA_DIR
        os.makedirs(STOCK_DATA_DIR, exist_ok=True)
        logger.info(f"âœ… åˆ›å»ºå¹¶ä½¿ç”¨é¡¹ç›®ç›®å½•: {STOCK_DATA_DIR}")

ADJUST_FLAG = "qfq"  # å‰å¤æƒ (Tushare)
WEBHOOK_URL = ""  # ä¼ä¸šå¾®ä¿¡Webhookåœ°å€ï¼ˆå¯é€‰ï¼‰

# åˆå§‹åŒ– Tushare API
try:
    # ä»ç¯å¢ƒå˜é‡è¯»å– Token
    tushare_token = os.environ.get('TUSHARE_TOKEN')
    if not tushare_token:
        logger.error("âŒ TUSHARE_TOKEN ç¯å¢ƒå˜é‡æœªè®¾ç½®ï¼Œè¯·åœ¨ .env æ–‡ä»¶ä¸­é…ç½®")
        print("âŒ é”™è¯¯ï¼šTUSHARE_TOKEN ç¯å¢ƒå˜é‡æœªè®¾ç½®")
        print("è¯·å‚è€ƒ .env.example æ–‡ä»¶é…ç½®ç¯å¢ƒå˜é‡")
        sys.exit(1)
    
    pro = ts.pro_api(tushare_token)
    logger.info("âœ… Tushare API åˆå§‹åŒ–æˆåŠŸ")
except Exception as e:
    logger.error(f"âŒ Tushare API åˆå§‹åŒ–å¤±è´¥: {e}")
    sys.exit(1)

def get_latest_trading_date():
    """è·å–æœ€è¿‘ä¸€ä¸ªäº¤æ˜“æ—¥ (ä½¿ç”¨Tushare)"""
    try:
        # ä½¿ç”¨ Tushare è·å–äº¤æ˜“æ—¥å†ï¼ˆç›´æ¥è°ƒç”¨ï¼Œä¸è®¾ç½®è¶…æ—¶ï¼‰
        today = datetime.now().strftime('%Y%m%d')
        logger.info(f"ğŸ” æ­£åœ¨è·å–äº¤æ˜“æ—¥å†ï¼ˆæˆªæ­¢{today}ï¼‰...")
        print(f"ğŸ” æ­£åœ¨è·å–äº¤æ˜“æ—¥å†ï¼ˆæˆªæ­¢{today}ï¼‰...")
        sys.stdout.flush()
        
        df = pro.trade_cal(exchange='SSE', end_date=today, is_open='1')
        
        if not df.empty:
            latest_date = df.iloc[0]['cal_date']
            formatted_date = f"{latest_date[:4]}-{latest_date[4:6]}-{latest_date[6:]}"
            logger.info(f"âœ… è·å–åˆ°æœ€æ–°äº¤æ˜“æ—¥: {formatted_date}")
            print(f"âœ… è·å–åˆ°æœ€æ–°äº¤æ˜“æ—¥: {formatted_date}")
            sys.stdout.flush()
            return formatted_date
    except Exception as e:
        logger.warning(f"âš ï¸ ä½¿ç”¨Tushareè·å–äº¤æ˜“æ—¥å¤±è´¥: {e}ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ³•")
        print(f"âš ï¸ Tushareè·å–å¤±è´¥: {e}")
        sys.stdout.flush()
    
    # å¤‡ç”¨æ–¹æ³•ï¼šç®€å•æ¨ç®—
    today = datetime.now()
    for i in range(7):
        check_date = today - timedelta(days=i)
        if check_date.weekday() < 5:  # 0-4 for Monday-Friday
            formatted_date = check_date.strftime('%Y-%m-%d')
            logger.info(f"ğŸ“… ä½¿ç”¨å¤‡ç”¨æ–¹æ³•æ¨ç®—äº¤æ˜“æ—¥: {formatted_date}")
            print(f"ğŸ“… ä½¿ç”¨å¤‡ç”¨æ–¹æ³•æ¨ç®—äº¤æ˜“æ—¥: {formatted_date}")
            sys.stdout.flush()
            return formatted_date
    return None

def send_wecom_notification(message):
    """å‘é€ä¼ä¸šå¾®ä¿¡é€šçŸ¥"""
    if not WEBHOOK_URL:
        return False
        
    data = {
        "msgtype": "text",
        "text": {
            "content": message
        }
    }
    
    try:
        response = requests.post(WEBHOOK_URL, json=data, timeout=10)
        logger.info(f"ä¼ä¸šå¾®ä¿¡é€šçŸ¥ç»“æœ: {response.status_code}")
        return response.status_code == 200
    except Exception as e:
        logger.error(f"ä¼ä¸šå¾®ä¿¡é€šçŸ¥å¤±è´¥: {e}")
        return False

def update_stock_data_incremental(ts_code, stock_name, latest_trading_date):
    """å¢é‡æ›´æ–°å•åªè‚¡ç¥¨æ•°æ® (ä½¿ç”¨Tushare)"""
    # æ¸…ç†æ–‡ä»¶åä¸­çš„éæ³•å­—ç¬¦
    file_name_sanitized = re.sub(r'[\\/:*?\"<>|]', '', stock_name)
    stock_id = ts_code.split('.')[0]
    file_path = os.path.join(STOCK_DATA_DIR, f"{stock_id}_{file_name_sanitized}.csv")

    if not os.path.exists(file_path):
        logger.warning(f"æ–‡ä»¶ {file_path} ä¸å­˜åœ¨ï¼Œè·³è¿‡æ›´æ–°ã€‚")
        return 0

    try:
        # è¯»å–ç°æœ‰æ•°æ®
        existing_df = pd.read_csv(file_path)
        if 'date' not in existing_df.columns and 'trade_date' not in existing_df.columns:
            logger.warning(f"æ–‡ä»¶ {file_path} ç¼ºå°‘æ—¥æœŸåˆ—ï¼Œè·³è¿‡æ›´æ–°ã€‚")
            return 0

        # ç»Ÿä¸€æ—¥æœŸåˆ—å
        date_col = 'trade_date' if 'trade_date' in existing_df.columns else 'date'
        existing_df[date_col] = pd.to_datetime(existing_df[date_col], format='%Y%m%d', errors='coerce')
        last_local_date = existing_df[date_col].max()

        # å¦‚æœæœ¬åœ°æ•°æ®å·²æ˜¯æœ€æ–°ï¼Œè·³è¿‡
        latest_date_dt = datetime.strptime(latest_trading_date, '%Y-%m-%d')
        if pd.notna(last_local_date) and last_local_date >= latest_date_dt:
            return 0

        # æŸ¥è¯¢èµ·å§‹æ—¥æœŸä¸ºæœ¬åœ°æœ€åæ—¥æœŸçš„ä¸‹ä¸€å¤©
        start_date_to_query = (last_local_date + timedelta(days=1)).strftime('%Y%m%d')
        end_date_query = latest_trading_date.replace('-', '')
        
        # ä½¿ç”¨TushareæŸ¥è¯¢æ–°æ•°æ®
        new_df = pro.daily(
            ts_code=ts_code,
            start_date=start_date_to_query,
            end_date=end_date_query,
            adj=ADJUST_FLAG
        )

        if new_df is not None and not new_df.empty:
            # åˆå¹¶å¹¶å»é‡
            updated_df = pd.concat([existing_df, new_df]).drop_duplicates(subset=['trade_date']).sort_values(by='trade_date')
            updated_df.to_csv(file_path, index=False)
            
            logger.info(f"{ts_code} {stock_name} æ–°å¢ {len(new_df)} æ¡è®°å½•")
            return len(new_df)
        else:
            return 0
            
    except Exception as e:
        logger.error(f"æ›´æ–°è‚¡ç¥¨ {ts_code} å¤±è´¥: {e}")
        return 0

def main():
    """ä¸»å‡½æ•°"""
    start_time = time.time()
    
    # ç«‹å³è¾“å‡ºå¯åŠ¨ä¿¡æ¯ï¼ˆç¡®ä¿ç”¨æˆ·èƒ½çœ‹åˆ°ï¼‰
    print("=" * 60)
    print("ğŸš€ Aè‚¡æ•°æ®æ›´æ–°ç¨‹åºå¯åŠ¨ä¸­...")
    print(f"ğŸ“‚ æ•°æ®ç›®å½•: {STOCK_DATA_DIR}")
    print(f"ğŸ“… å½“å‰æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)
    sys.stdout.flush()  # å¼ºåˆ¶åˆ·æ–°è¾“å‡º
    
    logger.info("=" * 60)
    logger.info("å¼€å§‹æ‰§è¡Œæ¯æ—¥è‚¡ç¥¨æ•°æ®å¢é‡æ›´æ–°ä»»åŠ¡ (ä½¿ç”¨Tushare)")
    logger.info("=" * 60)
    
    # ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨
    if not os.path.exists(STOCK_DATA_DIR):
        logger.error(f"è‚¡ç¥¨æ•°æ®ç›®å½• {STOCK_DATA_DIR} ä¸å­˜åœ¨ï¼")
        send_wecom_notification(f"âŒ è‚¡ç¥¨æ•°æ®æ›´æ–°å¤±è´¥ï¼šæ•°æ®ç›®å½•ä¸å­˜åœ¨\næ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        return

    latest_trading_date = get_latest_trading_date()
    
    if not latest_trading_date:
        logger.error("æ— æ³•è·å–æœ€æ–°äº¤æ˜“æ—¥ï¼Œé€€å‡ºæ•°æ®æ›´æ–°ã€‚")
        send_wecom_notification(f"âŒ è‚¡ç¥¨æ•°æ®æ›´æ–°å¤±è´¥ï¼šæ— æ³•è·å–æœ€æ–°äº¤æ˜“æ—¥\næ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        return

    logger.info(f"æœ€æ–°äº¤æ˜“æ—¥: {latest_trading_date}")
    
    # è·å–æ‰€æœ‰CSVæ–‡ä»¶
    stock_files = [f for f in os.listdir(STOCK_DATA_DIR) if f.endswith('.csv')]
    total_files = len(stock_files)
    updated_count = 0
    processed_count = 0
    error_count = 0
    
    logger.info(f"å¼€å§‹å¤„ç† {total_files} ä¸ªè‚¡ç¥¨æ–‡ä»¶")
    
    for file_name in stock_files:
        processed_count += 1
        try:
            # ä»æ–‡ä»¶åè§£æè‚¡ç¥¨ä»£ç 
            stock_id = file_name.split('_')[0]
            
            # æ ¹æ®è‚¡ç¥¨ä»£ç ç¡®å®šå¸‚åœºï¼ˆTushareæ ¼å¼ï¼šä»£ç .å¸‚åœºï¼‰
            if stock_id.startswith('6'):
                ts_code = f'{stock_id}.SH'
            elif stock_id.startswith(('0', '3')):
                ts_code = f'{stock_id}.SZ'
            elif stock_id.startswith(('8', '4')):  # åŒ—äº¤æ‰€
                ts_code = f'{stock_id}.BJ'
            else:
                logger.warning(f"æœªçŸ¥å¸‚åœºä»£ç : {stock_id}")
                continue

            stock_name = file_name.split('_', 1)[1].replace('.csv', '') if '_' in file_name else stock_id
            
            # æ˜¾ç¤ºè¿›åº¦ï¼ˆæ¯50åªæ˜¾ç¤ºä¸€æ¬¡ï¼‰
            if processed_count % 50 == 0:
                progress = processed_count / total_files * 100
                elapsed = time.time() - start_time
                eta = (elapsed / processed_count) * (total_files - processed_count)
                logger.info(f"[{progress:.1f}%] è¿›åº¦: {processed_count}/{total_files}, å·²æ›´æ–°: {updated_count}, é¢„è®¡å‰©ä½™: {eta/60:.1f}åˆ†é’Ÿ")
            
            # æ›´æ–°æ•°æ®
            added_rows = update_stock_data_incremental(ts_code, stock_name, latest_trading_date)
            if added_rows > 0:
                updated_count += 1
            
            # Tushare APIé™æµï¼šæ¯åˆ†é’Ÿ200æ¬¡
            if processed_count % 200 == 0:
                logger.info("è¾¾åˆ°APIè°ƒç”¨é™åˆ¶ï¼Œä¼‘æ¯60ç§’...")
                time.sleep(60)
            else:
                time.sleep(0.3)  # æ¯æ¬¡è¯·æ±‚é—´éš”0.3ç§’
                
        except Exception as e:
            error_count += 1
            logger.error(f"å¤„ç†æ–‡ä»¶ {file_name} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
    
    end_time = time.time()
    duration = end_time - start_time
    
    logger.info("-" * 60)
    logger.info("æ¯æ—¥æ•°æ®æ›´æ–°å®Œæˆ!")
    logger.info(f"æœ€æ–°äº¤æ˜“æ—¥: {latest_trading_date}")
    logger.info(f"æ€»æ–‡ä»¶æ•°: {total_files}")
    logger.info(f"æˆåŠŸæ£€æŸ¥: {processed_count}")
    logger.info(f"å®é™…æ›´æ–°: {updated_count}")
    logger.info(f"é”™è¯¯æ•°é‡: {error_count}")
    logger.info(f"æ€»è€—æ—¶: {duration:.2f} ç§’ ({duration/60:.1f} åˆ†é’Ÿ)")
    logger.info("=" * 60)
    
    # å‘é€ä¼ä¸šå¾®ä¿¡é€šçŸ¥
    if updated_count > 0 or error_count > 0:
        status_icon = "âœ…" if error_count == 0 else "âš ï¸"
        message = f"""{status_icon} è‚¡ç¥¨æ•°æ®æ›´æ–°å®Œæˆ

ğŸ“… æ›´æ–°æ—¥æœŸ: {latest_trading_date}
ğŸ“Š æ€»è‚¡ç¥¨æ•°: {total_files}
ğŸ”„ å®é™…æ›´æ–°: {updated_count}
âŒ é”™è¯¯æ•°é‡: {error_count}
â±ï¸ è€—æ—¶: {duration:.1f}ç§’

æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"""
        
        send_wecom_notification(message)
    else:
        logger.info("æ— æ–°æ•°æ®éœ€è¦æ›´æ–°ï¼Œè·³è¿‡é€šçŸ¥")

if __name__ == '__main__':
    main()

