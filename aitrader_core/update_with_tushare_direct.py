#!/usr/bin/env python3
"""
Aè‚¡å…¨é‡æ•°æ®æ›´æ–°è„šæœ¬ - Tushare Directç‰ˆæœ¬ï¼ˆé€‚ç”¨äºStreamlit Cloudï¼‰
ç›´æ¥è°ƒç”¨Tushare APIï¼Œæ— éœ€subprocessï¼Œé€‚åˆåœ¨Streamlitç•Œé¢ä¸­ç›´æ¥è°ƒç”¨

æ”¯æŒåŒæ­¥å’Œå¼‚æ­¥ä¸¤ç§æ¨¡å¼ï¼š
- åŒæ­¥æ¨¡å¼ï¼šupdate_data_direct() - å‘åå…¼å®¹
- å¼‚æ­¥æ¨¡å¼ï¼šupdate_data_direct_async() - æ€§èƒ½æå‡4-6å€
"""
import os
import sys
import pandas as pd
import tushare as ts
from pathlib import Path
from datetime import datetime, timedelta
import time
import asyncio
from typing import Optional, Callable, Dict, Any
from concurrent.futures import ThreadPoolExecutor

# å°è¯•å¯¼å…¥å¸¸é‡
try:
    import sys
    sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
    from modules.constants import (
        TUSHARE_RATE_LIMIT_CALLS,
        TUSHARE_RATE_LIMIT_PERIOD,
        TUSHARE_SLEEP_INTERVAL,
        ASYNC_MAX_WORKERS_DEFAULT,
        RETRY_MAX_ATTEMPTS,
        RETRY_WAIT_MIN,
        RETRY_WAIT_MAX,
        LOG_BATCH_SIZE,
        LOG_SKIP_INTERVAL,
        LOG_ERROR_DISPLAY_LIMIT
    )
    USE_CONSTANTS = True
except ImportError:
    # å›é€€åˆ°ç¡¬ç¼–ç å€¼
    USE_CONSTANTS = False
    TUSHARE_RATE_LIMIT_CALLS = 1500
    TUSHARE_RATE_LIMIT_PERIOD = 60
    TUSHARE_SLEEP_INTERVAL = 0.04
    ASYNC_MAX_WORKERS_DEFAULT = 10
    RETRY_MAX_ATTEMPTS = 3
    RETRY_WAIT_MIN = 2
    RETRY_WAIT_MAX = 10
    LOG_BATCH_SIZE = 50
    LOG_SKIP_INTERVAL = 100
    LOG_ERROR_DISPLAY_LIMIT = 5

# å°è¯•å¯¼å…¥å¼‚æ­¥å’Œé™æµç›¸å…³åº“
try:
    from tenacity import retry, stop_after_attempt, wait_exponential
    HAS_TENACITY = True
except ImportError:
    HAS_TENACITY = False

try:
    from ratelimit import limits, sleep_and_retry
    HAS_RATELIMIT = True
except ImportError:
    HAS_RATELIMIT = False

# å°è¯•å¯¼å…¥loggerï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨printä½œä¸ºfallback
try:
    import sys
    sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
    from modules.logger_config import get_logger
    logger = get_logger(__name__)
    USE_LOGGER = True
except ImportError:
    # å¦‚æœloggerä¸å¯ç”¨ï¼Œä½¿ç”¨print
    USE_LOGGER = False
    logger = None

# Tushare Token - ä»ç¯å¢ƒå˜é‡è¯»å–
TUSHARE_TOKEN = os.environ.get('TUSHARE_TOKEN')
if not TUSHARE_TOKEN:
    msg = "âŒ é”™è¯¯ï¼šTUSHARE_TOKEN ç¯å¢ƒå˜é‡æœªè®¾ç½®\nè¯·åœ¨ .env æ–‡ä»¶ä¸­é…ç½® TUSHARE_TOKEN\nå‚è€ƒ .env.example æ–‡ä»¶"
    if USE_LOGGER:
        logger.error(msg)
    else:
        print(msg)

def get_stock_data_dir():
    """è·å–æ•°æ®ç›®å½•"""
    # 1. ä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡
    if 'STOCK_DATA_DIR' in os.environ:
        data_dir = Path(os.environ['STOCK_DATA_DIR'])
    # 2. æ£€æµ‹ Streamlit Cloud ç¯å¢ƒ (é€šè¿‡æ£€æŸ¥é¡¹ç›®è·¯å¾„ç‰¹å¾)
    elif '/mount/src/' in str(Path(__file__).absolute()):
        # Streamlit Cloud ç¯å¢ƒï¼šä½¿ç”¨é¡¹ç›®å†…çš„ data ç›®å½•
        project_root = Path(__file__).parent.parent
        data_dir = project_root / "data" / "stock_data"
    # 3. æœ¬åœ°ç¯å¢ƒï¼šä½¿ç”¨ç”¨æˆ·ä¸»ç›®å½•
    else:
        data_dir = Path.home() / "stock_data"
    
    data_dir.mkdir(parents=True, exist_ok=True)
    return data_dir


def update_data_direct(progress_callback=None, log_callback=None):
    """
    ç›´æ¥æ›´æ–°æ•°æ®ï¼ˆé€‚ç”¨äºStreamlitç•Œé¢ç›´æ¥è°ƒç”¨ï¼‰
    
    Args:
        progress_callback: è¿›åº¦å›è°ƒå‡½æ•° callback(progress, current, total, message)
        log_callback: æ—¥å¿—å›è°ƒå‡½æ•° callback(message)
    
    Returns:
        dict: æ›´æ–°ç»“æœç»Ÿè®¡
    """
    def log(msg, level='info'):
        """æ—¥å¿—è¾“å‡º"""
        if USE_LOGGER:
            if level == 'error':
                logger.error(msg)
            elif level == 'warning':
                logger.warning(msg)
            elif level == 'debug':
                logger.debug(msg)
            else:
                logger.info(msg)
        else:
            print(msg)
            sys.stdout.flush()
        if log_callback:
            log_callback(msg)
    
    def update_progress(progress, current, total, msg=""):
        """æ›´æ–°è¿›åº¦"""
        if progress_callback:
            progress_callback(progress, current, total, msg)
    
    try:
        # åˆå§‹åŒ–Tushare API
        log("âœ… æ­£åœ¨åˆå§‹åŒ–Tushare API...")
        pro = ts.pro_api(TUSHARE_TOKEN)
        
        # è·å–æ•°æ®ç›®å½•
        data_dir = get_stock_data_dir()
        log(f"âœ… æ•°æ®ç›®å½•: {data_dir}")
        
        # è·å–è‚¡ç¥¨åˆ—è¡¨
        log("ğŸ” æ­£åœ¨è·å–Aè‚¡è‚¡ç¥¨åˆ—è¡¨...")
        stock_list = pro.stock_basic(exchange='', list_status='L', fields='ts_code,name')
        total_stocks = len(stock_list)
        log(f"âœ… è·å–åˆ° {total_stocks} åªAè‚¡è‚¡ç¥¨")
        
        # ç¡®å®šæ—¶é—´èŒƒå›´
        end_date = datetime.now().strftime('%Y%m%d')
        start_date = (datetime.now() - timedelta(days=365)).strftime('%Y%m%d')
        log(f"ğŸ“… æ›´æ–°æ—¶é—´èŒƒå›´: {start_date} ~ {end_date}")
        
        # ç»Ÿè®¡å˜é‡
        success_count = 0
        skip_count = 0
        error_count = 0
        
        # éå†æ›´æ–°æ¯åªè‚¡ç¥¨
        for idx, row in stock_list.iterrows():
            ts_code = row['ts_code']
            name = row['name']
            
            # æ›´æ–°è¿›åº¦
            progress = int((idx + 1) / total_stocks * 100)
            update_progress(progress, idx + 1, total_stocks, f"æ­£åœ¨æ›´æ–°: {name}")
            
            csv_file = data_dir / f"{ts_code}_{name}.csv"
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°
            start_date_incremental = start_date
            if csv_file.exists():
                try:
                    existing_df = pd.read_csv(csv_file)
                    if not existing_df.empty and 'trade_date' in existing_df.columns:
                        # è·å–æœ€åæ—¥æœŸå¹¶ç»Ÿä¸€è½¬æ¢ä¸º YYYYMMDD æ ¼å¼å­—ç¬¦ä¸²
                        last_date_raw = existing_df['trade_date'].max()
                        
                        # ç»Ÿä¸€è½¬æ¢ä¸º YYYYMMDD å­—ç¬¦ä¸²æ ¼å¼
                        try:
                            if pd.isna(last_date_raw):
                                # å¦‚æœæ˜¯ NaNï¼Œè·³è¿‡æ­¤æ–‡ä»¶
                                pass
                            elif isinstance(last_date_raw, (int, float)):
                                # å¦‚æœæ˜¯æ•°å­—ï¼Œè½¬ä¸ºå­—ç¬¦ä¸²
                                last_date = str(int(last_date_raw))
                            elif isinstance(last_date_raw, str):
                                # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œæ¸…ç†æ¨ªæ å’Œç©ºæ ¼
                                last_date = last_date_raw.replace('-', '').replace(' ', '').strip()[:8]
                            else:
                                # å…¶ä»–ç±»å‹ï¼ˆå¦‚datetimeï¼‰ï¼Œè½¬ä¸ºå­—ç¬¦ä¸²åæ¸…ç†
                                last_date = str(last_date_raw).replace('-', '').replace(' ', '').strip()[:8]
                            
                            # ç¡®ä¿ end_date ä¹Ÿæ˜¯çº¯å­—ç¬¦ä¸²æ ¼å¼
                            end_date_str = str(end_date).replace('-', '').strip()
                            
                            # éªŒè¯æ ¼å¼æ˜¯å¦æ­£ç¡®ï¼ˆ8ä½æ•°å­—ï¼‰
                            if len(last_date) == 8 and last_date.isdigit():
                                # å¦‚æœå·²æ˜¯æœ€æ–°ï¼Œè·³è¿‡
                                if last_date >= end_date_str:
                                    skip_count += 1
                                    if skip_count % 100 == 0:
                                        log(f"â© å·²è·³è¿‡ {skip_count} åªæœ€æ–°è‚¡ç¥¨")
                                    continue
                                start_date_incremental = last_date
                        except Exception:
                            # æ—¥æœŸè§£æå¤±è´¥ï¼Œé‡æ–°ä¸‹è½½å…¨éƒ¨æ•°æ®
                            pass
                except Exception:
                    # æ–‡ä»¶è¯»å–å¤±è´¥ï¼Œé‡æ–°ä¸‹è½½å…¨éƒ¨æ•°æ®
                    pass
            
            # ä¸‹è½½æ•°æ®
            try:
                df = pro.daily(
                    ts_code=ts_code,
                    start_date=start_date_incremental,
                    end_date=end_date,
                    adj='qfq'
                )
                
                if df is not None and not df.empty:
                    # åˆå¹¶æ•°æ®
                    if csv_file.exists():
                        existing_df = pd.read_csv(csv_file)
                        df = pd.concat([existing_df, df], ignore_index=True)
                        
                        # ç¡®ä¿ trade_date åˆ—ä¸ºç»Ÿä¸€æ ¼å¼ï¼ˆå­—ç¬¦ä¸²ï¼‰å†å»é‡å’Œæ’åº
                        df['trade_date'] = df['trade_date'].astype(str).str.replace('-', '').str.strip()
                        df = df.drop_duplicates(subset=['trade_date'], keep='last')
                        df = df.sort_values('trade_date')
                    else:
                        # æ–°æ–‡ä»¶ä¹Ÿéœ€è¦æ ¼å¼åŒ–æ—¥æœŸ
                        df['trade_date'] = df['trade_date'].astype(str).str.replace('-', '').str.strip()
                    
                    # ä¿å­˜
                    df.to_csv(csv_file, index=False, encoding='utf-8-sig')
                    success_count += 1
                    
                    if success_count % LOG_BATCH_SIZE == 0:
                        log(f"âœ… å·²æ›´æ–° {success_count} åªè‚¡ç¥¨")
                else:
                    skip_count += 1
                
                # APIé™æµä¼˜åŒ– (2000ç§¯åˆ†ç”¨æˆ·: 2000æ¬¡/åˆ†é’Ÿ)
                # 2000ç§¯åˆ† = 2000æ¬¡/åˆ†é’Ÿ = 60ç§’/2000æ¬¡ = 0.03ç§’/æ¬¡
                # ä¸ºäº†å®‰å…¨èµ·è§ï¼Œè®¾ç½®ä¸º 0.04ç§’/æ¬¡ (çº¦1500æ¬¡/åˆ†é’Ÿ)
                time.sleep(0.04)  # çº¦1500æ¬¡
            except Exception as e:
                error_count += 1
                error_msg = f"âŒ {name} æ›´æ–°å¤±è´¥: {str(e)[:50]}"
                if error_count <= LOG_ERROR_DISPLAY_LIMIT:
                    log(error_msg, level='error')
                elif USE_LOGGER:
                    # è¶…è¿‡5ä¸ªé”™è¯¯åï¼Œåªè®°å½•åˆ°æ—¥å¿—ï¼Œä¸æ˜¾ç¤ºç»™ç”¨æˆ·
                    logger.error(f"{name} æ›´æ–°å¤±è´¥: {str(e)}", exc_info=True)
        
        # å®Œæˆ
        update_progress(100, total_stocks, total_stocks, "æ›´æ–°å®Œæˆ")
        log("=" * 60)
        log(f"âœ… æ›´æ–°å®Œæˆ")
        log(f"   æˆåŠŸ: {success_count} åª")
        log(f"   è·³è¿‡: {skip_count} åª")
        log(f"   å¤±è´¥: {error_count} åª")
        log("=" * 60)
        
        return {
            'success': success_count,
            'skip': skip_count,
            'error': error_count,
            'total': total_stocks
        }
        
    except Exception as e:
        error_msg = f"âŒ æ›´æ–°å¤±è´¥: {e}"
        log(error_msg, level='error')
        import traceback
        if USE_LOGGER:
            logger.exception("æ•°æ®æ›´æ–°è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸")
        else:
            log(traceback.format_exc(), level='error')
        return None


# ========== å¼‚æ­¥ç‰ˆæœ¬ï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼‰ ==========

def _update_single_stock_sync(
    pro: Any,
    ts_code: str,
    name: str,
    csv_file: Path,
    start_date: str,
    end_date: str,
    start_date_incremental: str
) -> Dict[str, Any]:
    """
    åŒæ­¥æ›´æ–°å•åªè‚¡ç¥¨æ•°æ®ï¼ˆç”¨äºå¼‚æ­¥å¹¶å‘ï¼‰
    
    Returns:
        dict: {'status': 'success'|'skip'|'error', 'name': str}
    """
    def _call_api_with_retry():
        """å¸¦é‡è¯•çš„APIè°ƒç”¨"""
        if HAS_TENACITY:
            @retry(
                stop=stop_after_attempt(RETRY_MAX_ATTEMPTS),
                wait=wait_exponential(multiplier=1, min=RETRY_WAIT_MIN, max=RETRY_WAIT_MAX)
            )
            def _call():
                return pro.daily(
                    ts_code=ts_code,
                    start_date=start_date_incremental,
                    end_date=end_date,
                    adj='qfq'
                )
            return _call()
        else:
            # ç®€å•é‡è¯•é€»è¾‘
            for attempt in range(RETRY_MAX_ATTEMPTS):
                try:
                    return pro.daily(
                        ts_code=ts_code,
                        start_date=start_date_incremental,
                        end_date=end_date,
                        adj='qfq'
                    )
                except Exception as e:
                    if attempt == RETRY_MAX_ATTEMPTS - 1:
                        raise
                    time.sleep(RETRY_WAIT_MIN ** attempt)  # æŒ‡æ•°é€€é¿
            return None
    
    try:
        # ä½¿ç”¨é™æµè£…é¥°å™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if HAS_RATELIMIT:
            @sleep_and_retry
            @limits(calls=TUSHARE_RATE_LIMIT_CALLS, period=TUSHARE_RATE_LIMIT_PERIOD)
            def _call_with_limit():
                return _call_api_with_retry()
            df = _call_with_limit()
        else:
            # å›é€€åˆ°ç®€å•é™æµ
            df = _call_api_with_retry()
            time.sleep(TUSHARE_SLEEP_INTERVAL)
        
        if df is not None and not df.empty:
            # åˆå¹¶æ•°æ®
            if csv_file.exists():
                existing_df = pd.read_csv(csv_file)
                df = pd.concat([existing_df, df], ignore_index=True)
                
                # ç¡®ä¿ trade_date åˆ—ä¸ºç»Ÿä¸€æ ¼å¼ï¼ˆå­—ç¬¦ä¸²ï¼‰å†å»é‡å’Œæ’åº
                df['trade_date'] = df['trade_date'].astype(str).str.replace('-', '').str.strip()
                df = df.drop_duplicates(subset=['trade_date'], keep='last')
                df = df.sort_values('trade_date')
            else:
                # æ–°æ–‡ä»¶ä¹Ÿéœ€è¦æ ¼å¼åŒ–æ—¥æœŸ
                df['trade_date'] = df['trade_date'].astype(str).str.replace('-', '').str.strip()
            
            # ä¿å­˜
            df.to_csv(csv_file, index=False, encoding='utf-8-sig')
            return {'status': 'success', 'name': name, 'ts_code': ts_code}
        else:
            return {'status': 'skip', 'name': name, 'ts_code': ts_code}
            
    except Exception as e:
        return {'status': 'error', 'name': name, 'ts_code': ts_code, 'error': str(e)}


async def update_data_direct_async(
    progress_callback: Optional[Callable] = None,
    log_callback: Optional[Callable] = None,
    max_workers: int = ASYNC_MAX_WORKERS_DEFAULT
) -> Optional[Dict[str, int]]:
    """
    å¼‚æ­¥å¹¶å‘æ›´æ–°æ•°æ®ï¼ˆæ€§èƒ½ä¼˜åŒ–ç‰ˆæœ¬ï¼‰
    
    Args:
        progress_callback: è¿›åº¦å›è°ƒå‡½æ•° callback(progress, current, total, message)
        log_callback: æ—¥å¿—å›è°ƒå‡½æ•° callback(message)
        max_workers: æœ€å¤§å¹¶å‘æ•°ï¼ˆé»˜è®¤10ï¼Œå¯æ ¹æ®APIé™åˆ¶è°ƒæ•´ï¼‰
    
    Returns:
        dict: æ›´æ–°ç»“æœç»Ÿè®¡
    """
    def log(msg: str, level: str = 'info'):
        """æ—¥å¿—è¾“å‡º"""
        if USE_LOGGER:
            if level == 'error':
                logger.error(msg)
            elif level == 'warning':
                logger.warning(msg)
            elif level == 'debug':
                logger.debug(msg)
            else:
                logger.info(msg)
        else:
            print(msg)
            sys.stdout.flush()
        if log_callback:
            log_callback(msg)
    
    def update_progress(progress: int, current: int, total: int, msg: str = ""):
        """æ›´æ–°è¿›åº¦"""
        if progress_callback:
            progress_callback(progress, current, total, msg)
    
    try:
        # åˆå§‹åŒ–Tushare API
        log("âœ… æ­£åœ¨åˆå§‹åŒ–Tushare APIï¼ˆå¼‚æ­¥æ¨¡å¼ï¼‰...")
        pro = ts.pro_api(TUSHARE_TOKEN)
        
        # è·å–æ•°æ®ç›®å½•
        data_dir = get_stock_data_dir()
        log(f"âœ… æ•°æ®ç›®å½•: {data_dir}")
        
        # è·å–è‚¡ç¥¨åˆ—è¡¨
        log("ğŸ” æ­£åœ¨è·å–Aè‚¡è‚¡ç¥¨åˆ—è¡¨...")
        stock_list = pro.stock_basic(exchange='', list_status='L', fields='ts_code,name')
        total_stocks = len(stock_list)
        log(f"âœ… è·å–åˆ° {total_stocks} åªAè‚¡è‚¡ç¥¨")
        log(f"ğŸš€ ä½¿ç”¨å¼‚æ­¥å¹¶å‘æ¨¡å¼ï¼Œæœ€å¤§å¹¶å‘æ•°: {max_workers}")
        
        # ç¡®å®šæ—¶é—´èŒƒå›´
        end_date = datetime.now().strftime('%Y%m%d')
        start_date = (datetime.now() - timedelta(days=365)).strftime('%Y%m%d')
        log(f"ğŸ“… æ›´æ–°æ—¶é—´èŒƒå›´: {start_date} ~ {end_date}")
        
        # ç»Ÿè®¡å˜é‡
        success_count = 0
        skip_count_pre = 0  # é¢„å…ˆè·³è¿‡çš„ï¼ˆå·²æ˜¯æœ€æ–°ï¼‰
        skip_count = 0      # æ‰§è¡Œä¸­è·³è¿‡çš„ï¼ˆæ•°æ®ä¸ºç©ºï¼‰
        error_count = 0
        
        # å‡†å¤‡ä»»åŠ¡åˆ—è¡¨
        tasks = []
        for idx, row in stock_list.iterrows():
            ts_code = row['ts_code']
            name = row['name']
            csv_file = data_dir / f"{ts_code}_{name}.csv"
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°
            start_date_incremental = start_date
            if csv_file.exists():
                try:
                    existing_df = pd.read_csv(csv_file)
                    if not existing_df.empty and 'trade_date' in existing_df.columns:
                        last_date_raw = existing_df['trade_date'].max()
                        try:
                            if pd.isna(last_date_raw):
                                pass
                            elif isinstance(last_date_raw, (int, float)):
                                last_date = str(int(last_date_raw))
                            elif isinstance(last_date_raw, str):
                                last_date = last_date_raw.replace('-', '').replace(' ', '').strip()[:8]
                            else:
                                last_date = str(last_date_raw).replace('-', '').replace(' ', '').strip()[:8]
                            
                            end_date_str = str(end_date).replace('-', '').strip()
                            if len(last_date) == 8 and last_date.isdigit():
                                if last_date >= end_date_str:
                                    skip_count_pre += 1
                                    if skip_count_pre % LOG_SKIP_INTERVAL == 0:
                                        log(f"â© å·²è·³è¿‡ {skip_count_pre} åªæœ€æ–°è‚¡ç¥¨")
                                    continue
                                start_date_incremental = last_date
                        except Exception:
                            pass
                except Exception:
                    pass
            
            # åˆ›å»ºä»»åŠ¡
            tasks.append({
                'ts_code': ts_code,
                'name': name,
                'csv_file': csv_file,
                'start_date': start_date,
                'end_date': end_date,
                'start_date_incremental': start_date_incremental
            })
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘æ‰§è¡Œ
        log(f"ğŸ”„ å¼€å§‹å¹¶å‘æ›´æ–° {len(tasks)} åªè‚¡ç¥¨...")
        loop = asyncio.get_event_loop()
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # åˆ›å»ºå¼‚æ­¥ä»»åŠ¡
            async def process_task(task_data):
                return await loop.run_in_executor(
                    executor,
                    _update_single_stock_sync,
                    pro,
                    task_data['ts_code'],
                    task_data['name'],
                    task_data['csv_file'],
                    task_data['start_date'],
                    task_data['end_date'],
                    task_data['start_date_incremental']
                )
            
            # å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
            completed = 0
            async def process_all_tasks():
                nonlocal success_count, skip_count, error_count, completed
                results = await asyncio.gather(*[process_task(task) for task in tasks], return_exceptions=True)
                
                for result in results:
                    if isinstance(result, Exception):
                        error_count += 1
                        if USE_LOGGER:
                            logger.error(f"ä»»åŠ¡æ‰§è¡Œå¼‚å¸¸: {result}", exc_info=True)
                    elif isinstance(result, dict):
                        completed += 1
                        if result['status'] == 'success':
                            success_count += 1
                            if success_count % LOG_BATCH_SIZE == 0:
                                log(f"âœ… å·²æ›´æ–° {success_count} åªè‚¡ç¥¨")
                        elif result['status'] == 'skip':
                            skip_count += 1
                        elif result['status'] == 'error':
                            error_count += 1
                            if error_count <= LOG_ERROR_DISPLAY_LIMIT:
                                log(f"âŒ {result['name']} æ›´æ–°å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')[:50]}", level='error')
                        
                        # æ›´æ–°è¿›åº¦
                        progress = int(completed / len(tasks) * 100)
                        update_progress(progress, completed, len(tasks), f"å·²å¤„ç†: {completed}/{len(tasks)}")
            
            await process_all_tasks()
        
        # å®Œæˆ
        update_progress(100, len(tasks), len(tasks), "æ›´æ–°å®Œæˆ")
        total_skip = skip_count_pre + skip_count
        log("=" * 60)
        log(f"âœ… å¼‚æ­¥æ›´æ–°å®Œæˆ")
        log(f"   æˆåŠŸ: {success_count} åª")
        log(f"   è·³è¿‡: {total_skip} åªï¼ˆå·²æœ€æ–°: {skip_count_pre}, æ•°æ®ä¸ºç©º: {skip_count}ï¼‰")
        log(f"   å¤±è´¥: {error_count} åª")
        log("=" * 60)
        
        return {
            'success': success_count,
            'skip': total_skip,
            'error': error_count,
            'total': total_stocks
        }
        
    except Exception as e:
        error_msg = f"âŒ å¼‚æ­¥æ›´æ–°å¤±è´¥: {e}"
        log(error_msg, level='error')
        if USE_LOGGER:
            logger.exception("å¼‚æ­¥æ•°æ®æ›´æ–°è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸")
        return None


if __name__ == "__main__":
    # å‘½ä»¤è¡Œæ¨¡å¼
    import argparse
    
    parser = argparse.ArgumentParser(description='Aè‚¡æ•°æ®æ›´æ–°è„šæœ¬')
    parser.add_argument('--async-mode', '--async', dest='use_async', action='store_true', 
                       help='ä½¿ç”¨å¼‚æ­¥æ¨¡å¼ï¼ˆæ€§èƒ½æå‡4-6å€ï¼‰')
    parser.add_argument('--workers', type=int, default=10, help='å¼‚æ­¥æ¨¡å¼çš„æœ€å¤§å¹¶å‘æ•°ï¼ˆé»˜è®¤10ï¼‰')
    args = parser.parse_args()
    
    if args.use_async:
        # å¼‚æ­¥æ¨¡å¼
        result = asyncio.run(update_data_direct_async(max_workers=args.workers))
    else:
        # åŒæ­¥æ¨¡å¼ï¼ˆé»˜è®¤ï¼‰
        result = update_data_direct()
    
    if result:
        sys.exit(0)
    else:
        sys.exit(1)

