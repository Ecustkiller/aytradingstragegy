#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import requests
import json
import logging
import time
from datetime import datetime, timedelta
import schedule
from bs4 import BeautifulSoup
import feedparser
import re
from typing import List, Dict, Optional
import threading
import hashlib
import random
from urllib.parse import urljoin, urlparse

# å¯¼å…¥å¢å¼ºç‰ˆçˆ¬è™«
try:
    from .enhanced_news_crawler import EnhancedNewsCrawler
    ENHANCED_AVAILABLE = True
except ImportError:
    ENHANCED_AVAILABLE = False
    print("âš ï¸ å¢å¼ºç‰ˆçˆ¬è™«ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨åŸºç¡€ç‰ˆæœ¬")

class NewsCrawlerBot:
    """è´¢ç»æ–°é—»çˆ¬è™«æœºå™¨äºº"""
    
    def __init__(self, webhook_url: str):
        self.webhook_url = webhook_url
        self.setup_logging()
        self.news_sources = self.init_news_sources()
        self.seen_news = set()  # ç”¨äºå»é‡ï¼Œé¿å…é‡å¤å‘é€
        self.session = requests.Session()  # ä½¿ç”¨sessionæé«˜æ€§èƒ½
        self.setup_session_headers()
        
        # åˆå§‹åŒ–å¢å¼ºç‰ˆçˆ¬è™«
        if ENHANCED_AVAILABLE:
            self.enhanced_crawler = EnhancedNewsCrawler(webhook_url)
            self.logger.info("âœ… å¢å¼ºç‰ˆçˆ¬è™«å·²å¯ç”¨")
        else:
            self.enhanced_crawler = None
            self.logger.warning("âš ï¸ å¢å¼ºç‰ˆçˆ¬è™«ä¸å¯ç”¨ï¼Œä½¿ç”¨åŸºç¡€ç‰ˆæœ¬")
        
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s%(msecs)03d - %(levelname)s - %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S',
            handlers=[
                logging.FileHandler('news_crawler_bot.log', encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def setup_session_headers(self):
        """è®¾ç½®sessionçš„é€šç”¨headers"""
        user_agents = [
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0'
        ]
        
        self.session.headers.update({
            'User-Agent': random.choice(user_agents),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0'
        })
    
    def init_news_sources(self) -> Dict:
        """åˆå§‹åŒ–æ–°é—»æºé…ç½®"""
        return {
            'sina_finance': {
                'name': 'æ–°æµªè´¢ç»',
                'url': 'https://finance.sina.com.cn/roll/',
                'rss': 'https://feed.sina.com.cn/api/roll/get?pageid=153&lid=1686&k=&num=20&page=1',
                'type': 'rss'
            },
            'eastmoney': {
                'name': 'ä¸œæ–¹è´¢å¯Œ',
                'url': 'https://finance.eastmoney.com/news/',
                'api': 'https://np-anotice-stock.eastmoney.com/api/security/ann',
                'type': 'api'
            },
            'wallstreetcn': {
                'name': 'åå°”è¡—è§é—»',
                'url': 'https://wallstreetcn.com/news',
                'rss': 'https://api-prod.wallstreetcn.com/apiv1/content/articles',
                'type': 'api'
            },
            'cailianshe': {
                'name': 'è´¢è”ç¤¾',
                'url': 'https://www.cls.cn/telegraph',
                'type': 'web'
            },
            'yicai': {
                'name': 'ç¬¬ä¸€è´¢ç»',
                'url': 'https://www.yicai.com/news/',
                'rss': 'https://www.yicai.com/api/ajax/getlatest',
                'type': 'api'
            }
        }
    
    def crawl_sina_finance(self) -> List[Dict]:
        """çˆ¬å–æ–°æµªè´¢ç»æ–°é—»"""
        news_list = []
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                'Accept-Encoding': 'gzip, deflate, br',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1'
            }
            
            # ç›´æ¥çˆ¬å–æ–°æµªè´¢ç»é¦–é¡µ
            url = 'https://finance.sina.com.cn/'
            response = requests.get(url, headers=headers, timeout=15)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # æŸ¥æ‰¾æ–°é—»é“¾æ¥
                news_links = soup.find_all('a', href=True)
                
                for link in news_links[:50]:  # æ£€æŸ¥å‰50ä¸ªé“¾æ¥
                    title = link.get_text().strip()
                    href = link.get('href', '')
                    
                    # è¿‡æ»¤æœ‰æ•ˆçš„è´¢ç»æ–°é—»
                    if (len(title) > 10 and 
                        self.is_finance_related(title) and
                        ('finance.sina.com.cn' in href or href.startswith('//'))):
                        
                        # å¤„ç†ç›¸å¯¹é“¾æ¥
                        if href.startswith('//'):
                            href = 'https:' + href
                        elif href.startswith('/'):
                            href = 'https://finance.sina.com.cn' + href
                        
                        news_item = {
                            'title': title,
                            'url': href,
                            'time': datetime.now().strftime('%Y-%m-%d %H:%M'),
                            'source': 'æ–°æµªè´¢ç»'
                        }
                        news_list.append(news_item)
                        
                        if len(news_list) >= 10:  # é™åˆ¶æ•°é‡
                            break
            
            # å¦‚æœé¦–é¡µçˆ¬å–å¤±è´¥ï¼Œå°è¯•RSS
            if not news_list:
                rss_url = 'http://rss.sina.com.cn/finance/roll.xml'
                try:
                    feed = feedparser.parse(rss_url)
                    for entry in feed.entries[:10]:
                        if hasattr(entry, 'title') and self.is_finance_related(entry.title):
                            news_item = {
                                'title': entry.title,
                                'url': entry.link if hasattr(entry, 'link') else '',
                                'time': entry.published if hasattr(entry, 'published') else '',
                                'source': 'æ–°æµªè´¢ç»'
                            }
                            news_list.append(news_item)
                except:
                    pass
                            
        except Exception as e:
            self.logger.error(f"çˆ¬å–æ–°æµªè´¢ç»æ–°é—»å¤±è´¥: {e}")
            
        return news_list
    
    def crawl_eastmoney_news(self) -> List[Dict]:
        """çˆ¬å–ä¸œæ–¹è´¢å¯Œæ–°é—»"""
        news_list = []
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                'Referer': 'https://finance.eastmoney.com/'
            }
            
            # ç›´æ¥çˆ¬å–ä¸œæ–¹è´¢å¯Œè´¢ç»é¦–é¡µ
            url = 'https://finance.eastmoney.com/'
            response = requests.get(url, headers=headers, timeout=15)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # æŸ¥æ‰¾æ–°é—»æ ‡é¢˜å’Œé“¾æ¥
                news_elements = soup.find_all(['a', 'span'], class_=re.compile(r'.*title.*|.*news.*|.*article.*'))
                
                for element in news_elements[:50]:
                    title = element.get_text().strip()
                    
                    # è·å–é“¾æ¥
                    if element.name == 'a':
                        href = element.get('href', '')
                    else:
                        # å¦‚æœæ˜¯spanï¼ŒæŸ¥æ‰¾çˆ¶çº§æˆ–å…„å¼Ÿå…ƒç´ çš„é“¾æ¥
                        parent_a = element.find_parent('a')
                        href = parent_a.get('href', '') if parent_a else ''
                    
                    if (len(title) > 10 and 
                        self.is_finance_related(title) and
                        href):
                        
                        # å¤„ç†ç›¸å¯¹é“¾æ¥
                        if href.startswith('//'):
                            href = 'https:' + href
                        elif href.startswith('/'):
                            href = 'https://finance.eastmoney.com' + href
                        elif not href.startswith('http'):
                            continue
                        
                        news_item = {
                            'title': title,
                            'url': href,
                            'time': datetime.now().strftime('%Y-%m-%d %H:%M'),
                            'source': 'ä¸œæ–¹è´¢å¯Œ'
                        }
                        news_list.append(news_item)
                        
                        if len(news_list) >= 8:
                            break
            
            # å¦‚æœæ²¡æœ‰è·å–åˆ°è¶³å¤Ÿæ–°é—»ï¼Œå°è¯•ç§»åŠ¨ç«¯æ¥å£
            if len(news_list) < 3:
                try:
                    mobile_url = 'https://wap.eastmoney.com/news/'
                    mobile_response = self.session.get(mobile_url, timeout=10)
                    
                    if mobile_response.status_code == 200:
                        mobile_soup = BeautifulSoup(mobile_response.content, 'html.parser')
                        mobile_links = mobile_soup.find_all('a', href=True)
                        
                        for link in mobile_links[:20]:
                            title = link.get_text().strip()
                            href = link.get('href', '')
                            
                            if (len(title) > 10 and self.is_finance_related(title) and
                                ('eastmoney.com' in href or href.startswith('/'))):
                                
                                if href.startswith('/'):
                                    href = 'https://wap.eastmoney.com' + href
                                elif href.startswith('//'):
                                    href = 'https:' + href
                                
                                news_item = {
                                    'title': title,
                                    'url': href,
                                    'time': datetime.now().strftime('%Y-%m-%d %H:%M'),
                                    'source': 'ä¸œæ–¹è´¢å¯Œ'
                                }
                                news_list.append(news_item)
                                
                                if len(news_list) >= 8:
                                    break
                except Exception as e:
                    self.logger.warning(f"ä¸œæ–¹è´¢å¯Œç§»åŠ¨ç«¯çˆ¬å–å¤±è´¥: {e}")
                            
        except Exception as e:
            self.logger.error(f"çˆ¬å–ä¸œæ–¹è´¢å¯Œæ–°é—»å¤±è´¥: {e}")
            
        return news_list
    
    def crawl_cailianshe_news(self) -> List[Dict]:
        """çˆ¬å–è´¢è”ç¤¾å¿«è®¯ - ä½¿ç”¨çœŸå®APIå’Œå¤šç§æ–¹æ³•"""
        news_list = []
        
        # æ–¹æ³•1: å°è¯•è´¢è”ç¤¾APIæ¥å£
        try:
            api_news = self.crawl_cailianshe_api()
            news_list.extend(api_news)
            self.logger.info(f"è´¢è”ç¤¾APIè·å–åˆ° {len(api_news)} æ¡æ–°é—»")
        except Exception as e:
            self.logger.warning(f"è´¢è”ç¤¾APIçˆ¬å–å¤±è´¥: {e}")
        
        # æ–¹æ³•2: çˆ¬å–è´¢è”ç¤¾å¿«è®¯é¡µé¢
        if len(news_list) < 5:
            try:
                web_news = self.crawl_cailianshe_web()
                news_list.extend(web_news)
                self.logger.info(f"è´¢è”ç¤¾ç½‘é¡µçˆ¬å–è·å–åˆ° {len(web_news)} æ¡æ–°é—»")
            except Exception as e:
                self.logger.warning(f"è´¢è”ç¤¾ç½‘é¡µçˆ¬å–å¤±è´¥: {e}")
        
        # æ–¹æ³•3: çˆ¬å–è´¢è”ç¤¾RSS
        if len(news_list) < 3:
            try:
                rss_news = self.crawl_cailianshe_rss()
                news_list.extend(rss_news)
                self.logger.info(f"è´¢è”ç¤¾RSSè·å–åˆ° {len(rss_news)} æ¡æ–°é—»")
            except Exception as e:
                self.logger.warning(f"è´¢è”ç¤¾RSSçˆ¬å–å¤±è´¥: {e}")
        
        # å»é‡å¤„ç†
        unique_news = self.deduplicate_news(news_list)
        
        # å¦‚æœä»ç„¶æ²¡æœ‰è·å–åˆ°æ–°é—»ï¼Œè®°å½•é”™è¯¯ä½†ä¸ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
        if not unique_news:
            self.logger.error("è´¢è”ç¤¾æ‰€æœ‰æ•°æ®æºéƒ½æ— æ³•è·å–çœŸå®æ•°æ®ï¼Œè·³è¿‡æ­¤æ¬¡æ¨é€")
            return []
        
        return unique_news[:8]  # è¿”å›æœ€å¤š8æ¡æ–°é—»
    
    def crawl_cailianshe_api(self) -> List[Dict]:
        """ä½¿ç”¨è´¢è”ç¤¾APIæ¥å£è·å–æ–°é—»"""
        news_list = []
        try:
            # è´¢è”ç¤¾å¿«è®¯API
            api_url = 'https://www.cls.cn/api/sw?app=CailianpressWeb&os=web&sv=7.7.5'
            
            params = {
                'channel': 'telegraph',
                'limit': 20,
                'sign': self.generate_cls_sign()
            }
            
            response = self.session.get(api_url, params=params, timeout=10)
            
            if response.status_code == 200:
                data = response.json()
                
                if data.get('code') == 200 and 'data' in data:
                    items = data['data'].get('list', [])
                    
                    for item in items:
                        title = item.get('title', '').strip()
                        content = item.get('content', '').strip()
                        pub_time = item.get('publish_time', '')
                        news_id = item.get('id', '')
                        
                        # ä½¿ç”¨æ ‡é¢˜æˆ–å†…å®¹
                        final_title = title if title else content[:100]
                        
                        if (final_title and len(final_title) > 10 and 
                            self.is_finance_related(final_title)):
                            
                            news_item = {
                                'title': final_title,
                                'url': f'https://www.cls.cn/telegraph/{news_id}' if news_id else 'https://www.cls.cn/telegraph',
                                'time': self.format_time(pub_time),
                                'source': 'è´¢è”ç¤¾',
                                'id': news_id
                            }
                            news_list.append(news_item)
                            
        except Exception as e:
            self.logger.error(f"è´¢è”ç¤¾APIçˆ¬å–å¤±è´¥: {e}")
            
        return news_list
    
    def crawl_cailianshe_web(self) -> List[Dict]:
        """çˆ¬å–è´¢è”ç¤¾ç½‘é¡µç‰ˆå¿«è®¯"""
        news_list = []
        try:
            # æ›´æ–°headers
            self.session.headers.update({
                'Referer': 'https://www.cls.cn/',
                'Host': 'www.cls.cn'
            })
            
            url = 'https://www.cls.cn/telegraph'
            response = self.session.get(url, timeout=15)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # æŸ¥æ‰¾å¿«è®¯å†…å®¹çš„å¤šç§é€‰æ‹©å™¨
                selectors = [
                    '.telegraph-content-box .telegraph-content',
                    '.telegraph-item',
                    '[class*="telegraph"]',
                    '.news-item',
                    '.telegraph-content-box p',
                    '.telegraph-list .item'
                ]
                
                for selector in selectors:
                    elements = soup.select(selector)
                    if elements:
                        self.logger.info(f"ä½¿ç”¨é€‰æ‹©å™¨ {selector} æ‰¾åˆ° {len(elements)} ä¸ªå…ƒç´ ")
                        
                        for element in elements[:20]:
                            title = element.get_text().strip()
                            
                            # æ¸…ç†æ ‡é¢˜
                            title = re.sub(r'\s+', ' ', title)
                            title = re.sub(r'^[0-9:\-\s]+', '', title)  # ç§»é™¤å¼€å¤´çš„æ—¶é—´
                            
                            if (len(title) > 15 and len(title) < 200 and 
                                self.is_finance_related(title) and
                                not self.is_duplicate_news(title)):
                                
                                # å°è¯•è·å–é“¾æ¥
                                link_element = element.find('a') or element.find_parent('a')
                                news_url = 'https://www.cls.cn/telegraph'
                                if link_element and link_element.get('href'):
                                    href = link_element.get('href')
                                    if href.startswith('/'):
                                        news_url = 'https://www.cls.cn' + href
                                    elif href.startswith('http'):
                                        news_url = href
                                
                                news_item = {
                                    'title': title,
                                    'url': news_url,
                                    'time': datetime.now().strftime('%Y-%m-%d %H:%M'),
                                    'source': 'è´¢è”ç¤¾',
                                    'id': hashlib.md5(title.encode()).hexdigest()[:8]
                                }
                                news_list.append(news_item)
                                
                                if len(news_list) >= 10:
                                    break
                        
                        if news_list:
                            break
                            
        except Exception as e:
            self.logger.error(f"è´¢è”ç¤¾ç½‘é¡µçˆ¬å–å¤±è´¥: {e}")
            
        return news_list
    
    def crawl_cailianshe_rss(self) -> List[Dict]:
        """å°è¯•çˆ¬å–è´¢è”ç¤¾RSSæˆ–å…¶ä»–æ ¼å¼çš„æ•°æ®æº"""
        news_list = []
        try:
            # å°è¯•ç§»åŠ¨ç«¯API
            mobile_api = 'https://m.cls.cn/api/telegraph'
            params = {
                'limit': 20,
                'channel': 'all'
            }
            
            response = self.session.get(mobile_api, params=params, timeout=10)
            
            if response.status_code == 200:
                try:
                    data = response.json()
                    items = data.get('data', {}).get('list', [])
                    
                    for item in items:
                        title = item.get('title', '').strip()
                        if not title:
                            title = item.get('content', '').strip()[:100]
                        
                        if (title and len(title) > 10 and 
                            self.is_finance_related(title)):
                            
                            news_item = {
                                'title': title,
                                'url': f"https://www.cls.cn/telegraph/{item.get('id', '')}",
                                'time': self.format_time(item.get('publish_time', '')),
                                'source': 'è´¢è”ç¤¾',
                                'id': item.get('id', '')
                            }
                            news_list.append(news_item)
                            
                except json.JSONDecodeError:
                    pass
                    
        except Exception as e:
            self.logger.error(f"è´¢è”ç¤¾RSSçˆ¬å–å¤±è´¥: {e}")
            
        return news_list
    
    def generate_cls_sign(self) -> str:
        """ç”Ÿæˆè´¢è”ç¤¾APIç­¾åï¼ˆå¦‚æœéœ€è¦ï¼‰"""
        timestamp = str(int(time.time()))
        # ç®€å•çš„ç­¾åç”Ÿæˆï¼Œå®é™…å¯èƒ½éœ€è¦æ›´å¤æ‚çš„ç®—æ³•
        return hashlib.md5(f"cls{timestamp}".encode()).hexdigest()
    
    def format_time(self, time_str: str) -> str:
        """æ ¼å¼åŒ–æ—¶é—´å­—ç¬¦ä¸²"""
        if not time_str:
            return datetime.now().strftime('%Y-%m-%d %H:%M')
        
        try:
            # å°è¯•è§£æä¸åŒæ ¼å¼çš„æ—¶é—´
            if isinstance(time_str, (int, float)):
                return datetime.fromtimestamp(time_str).strftime('%Y-%m-%d %H:%M')
            elif 'T' in time_str:
                dt = datetime.fromisoformat(time_str.replace('Z', '+00:00'))
                return dt.strftime('%Y-%m-%d %H:%M')
            else:
                return time_str
        except:
            return datetime.now().strftime('%Y-%m-%d %H:%M')
    
    def is_duplicate_news(self, title: str) -> bool:
        """æ£€æŸ¥æ–°é—»æ˜¯å¦é‡å¤"""
        title_hash = hashlib.md5(title.encode()).hexdigest()
        if title_hash in self.seen_news:
            return True
        self.seen_news.add(title_hash)
        return False
    
    def deduplicate_news(self, news_list: List[Dict]) -> List[Dict]:
        """å»é‡æ–°é—»åˆ—è¡¨"""
        unique_news = []
        seen_titles = set()
        
        for news in news_list:
            title = news['title'].strip()
            title_normalized = re.sub(r'\s+', ' ', title).lower()
            
            if title_normalized not in seen_titles:
                seen_titles.add(title_normalized)
                unique_news.append(news)
        
        return unique_news
    
    def add_real_time_news_sources(self) -> List[Dict]:
        """æ·»åŠ æ›´å¤šçœŸå®çš„æ–°é—»æº"""
        news_list = []
        
        # å°è¯•æ–°åè´¢ç»API
        try:
            xinhua_news = self.crawl_xinhua_finance()
            news_list.extend(xinhua_news)
            self.logger.info(f"æ–°åè´¢ç»è·å–åˆ° {len(xinhua_news)} æ¡æ–°é—»")
        except Exception as e:
            self.logger.warning(f"æ–°åè´¢ç»çˆ¬å–å¤±è´¥: {e}")
        
        # å°è¯•è¯åˆ¸æ—¶æŠ¥API
        try:
            stcn_news = self.crawl_stcn_news()
            news_list.extend(stcn_news)
            self.logger.info(f"è¯åˆ¸æ—¶æŠ¥è·å–åˆ° {len(stcn_news)} æ¡æ–°é—»")
        except Exception as e:
            self.logger.warning(f"è¯åˆ¸æ—¶æŠ¥çˆ¬å–å¤±è´¥: {e}")
            
        return news_list
    
    def crawl_xinhua_finance(self) -> List[Dict]:
        """çˆ¬å–æ–°åè´¢ç»æ–°é—»"""
        news_list = []
        try:
            url = 'http://www.xinhuanet.com/money/index.htm'
            response = self.session.get(url, timeout=10)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # æŸ¥æ‰¾æ–°é—»é“¾æ¥
                news_links = soup.find_all('a', href=True)
                
                for link in news_links[:30]:
                    title = link.get_text().strip()
                    href = link.get('href', '')
                    
                    if (len(title) > 10 and self.is_finance_related(title) and
                        ('xinhuanet.com' in href or href.startswith('/'))):
                        
                        if href.startswith('/'):
                            href = 'http://www.xinhuanet.com' + href
                        
                        news_item = {
                            'title': title,
                            'url': href,
                            'time': datetime.now().strftime('%Y-%m-%d %H:%M'),
                            'source': 'æ–°åè´¢ç»'
                        }
                        news_list.append(news_item)
                        
                        if len(news_list) >= 5:
                            break
        except Exception as e:
            self.logger.error(f"æ–°åè´¢ç»çˆ¬å–å¤±è´¥: {e}")
            
        return news_list
    
    def crawl_stcn_news(self) -> List[Dict]:
        """çˆ¬å–è¯åˆ¸æ—¶æŠ¥æ–°é—»"""
        news_list = []
        try:
            url = 'https://www.stcn.com/'
            response = self.session.get(url, timeout=10)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # æŸ¥æ‰¾æ–°é—»
                news_elements = soup.find_all(['a', 'h3', 'h4'], href=True)
                
                for element in news_elements[:30]:
                    title = element.get_text().strip()
                    href = element.get('href', '')
                    
                    if (len(title) > 10 and self.is_finance_related(title) and
                        ('stcn.com' in href or href.startswith('/'))):
                        
                        if href.startswith('/'):
                            href = 'https://www.stcn.com' + href
                        
                        news_item = {
                            'title': title,
                            'url': href,
                            'time': datetime.now().strftime('%Y-%m-%d %H:%M'),
                            'source': 'è¯åˆ¸æ—¶æŠ¥'
                        }
                        news_list.append(news_item)
                        
                        if len(news_list) >= 5:
                            break
        except Exception as e:
            self.logger.error(f"è¯åˆ¸æ—¶æŠ¥çˆ¬å–å¤±è´¥: {e}")
            
        return news_list
    
    def is_finance_related(self, title: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºè´¢ç»ç›¸å…³æ–°é—»"""
        finance_keywords = [
            'è‚¡å¸‚', 'è‚¡ç¥¨', 'åŸºé‡‘', 'å€ºåˆ¸', 'æœŸè´§', 'å¤–æ±‡', 'é»„é‡‘',
            'é“¶è¡Œ', 'ä¿é™©', 'è¯åˆ¸', 'æŠ•èµ„', 'èèµ„', 'IPO', 'å¹¶è´­',
            'å¤®è¡Œ', 'è´§å¸æ”¿ç­–', 'åˆ©ç‡', 'æ±‡ç‡', 'é€šèƒ€', 'CPI', 'GDP',
            'ä¸Šå¸‚', 'é€€å¸‚', 'åœç‰Œ', 'å¤ç‰Œ', 'æ¶¨åœ', 'è·Œåœ',
            'è´¢æŠ¥', 'ä¸šç»©', 'è¥æ”¶', 'åˆ©æ¶¦', 'äºæŸ',
            'ç›‘ç®¡', 'è¯ç›‘ä¼š', 'é“¶ä¿ç›‘ä¼š', 'äº¤æ˜“æ‰€',
            'ç§‘æŠ€è‚¡', 'æ–°èƒ½æº', 'èŠ¯ç‰‡', 'åŒ»è¯', 'åœ°äº§', 'é‡‘è'
        ]
        
        return any(keyword in title for keyword in finance_keywords)
    
    def format_news_report(self, news_list: List[Dict], report_type: str) -> List[str]:
        """æ ¼å¼åŒ–æ–°é—»æŠ¥å‘Šï¼Œè¿”å›å¤šæ¡æ¶ˆæ¯åˆ—è¡¨"""
        current_time = datetime.now().strftime('%Y-%m-%d %H:%M')
        
        if report_type == 'morning':
            title = f"ğŸ“° æ—©é—´è´¢ç»æ–°é—»æ±‡æ€» ({current_time})"
            intro = "ğŸŒ… éš”å¤œé‡è¦è´¢ç»æ–°é—»ï¼ŒåŠ©æ‚¨æŠŠæ¡å¸‚åœºè„‰æ"
        else:
            title = f"ğŸ“° æ™šé—´è´¢ç»æ–°é—»æ±‡æ€» ({current_time})"
            intro = "ğŸŒ™ ä»Šæ—¥é‡è¦å¸‚åœºåŠ¨æ€ï¼Œä¸ºæ‚¨æ¢³ç†æŠ•èµ„è¦ç‚¹"
        
        if not news_list:
            report = f"{title}\n\n{intro}\n\n"
            report += "ğŸ“­ æ‰€æœ‰æ–°é—»æºæš‚æ—¶æ— æ³•è·å–çœŸå®æ•°æ®\n"
            report += "ğŸ”„ ç³»ç»Ÿå°†åœ¨ä¸‹æ¬¡æ¨é€æ—¶é‡æ–°å°è¯•\n"
            report += "âš ï¸ ç»ä¸ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®ï¼Œç¡®ä¿ä¿¡æ¯çœŸå®æ€§\n"
            return [report]
        
        # æŒ‰æ¥æºåˆ†ç»„
        sources = {}
        for news in news_list:
            source = news['source']
            if source not in sources:
                sources[source] = []
            sources[source].append(news)
        
        # å°†æ–°é—»åˆ†æˆå¤šæ¡æ¶ˆæ¯ï¼Œæ¯æ¡æ¶ˆæ¯åŒ…å«éƒ¨åˆ†æ¥æº
        messages = []
        
        # ç¬¬ä¸€æ¡æ¶ˆæ¯åŒ…å«æ ‡é¢˜å’Œä»‹ç»
        first_message = f"{title}\n\n{intro}\n\n"
        messages.append(first_message)
        
        # å°†æ¥æºåˆ†æˆå¤šç»„ï¼Œæ¯ç»„3-4ä¸ªæ¥æº
        source_groups = []
        current_group = []
        for source, news_items in sources.items():
            current_group.append((source, news_items))
            if len(current_group) >= 3:  # æ¯3ä¸ªæ¥æºä¸ºä¸€ç»„
                source_groups.append(current_group)
                current_group = []
        
        # æ·»åŠ å‰©ä½™çš„æ¥æº
        if current_group:
            source_groups.append(current_group)
        
        # ä¸ºæ¯ç»„æ¥æºç”Ÿæˆä¸€æ¡æ¶ˆæ¯
        for i, group in enumerate(source_groups):
            group_message = f"ğŸ“° è´¢ç»æ–°é—»æ±‡æ€» (ç¬¬{i+1}éƒ¨åˆ†)\n\n"
            
            for source, source_news in group:
                group_message += f"## ğŸ“Š {source}\n"
                for j, news in enumerate(source_news[:5], 1):  # æ¯ä¸ªæ¥æºæœ€å¤š5æ¡
                    title = news['title'][:50] + '...' if len(news['title']) > 50 else news['title']
                    url = news.get('url', '')
                    
                    # æ·»åŠ é“¾æ¥
                    if url:
                        group_message += f"{j}. [{title}]({url})\n"
                    else:
                        group_message += f"{j}. {title}\n"
                group_message += "\n"
            
            messages.append(group_message)
        
        return messages
    
    def collect_all_news(self) -> List[Dict]:
        """æ”¶é›†æ‰€æœ‰æ–°é—»æºçš„æ–°é—» - ä¼˜å…ˆä½¿ç”¨å¢å¼ºç‰ˆ"""
        
        # ä¼˜å…ˆä½¿ç”¨å¢å¼ºç‰ˆçˆ¬è™«
        if self.enhanced_crawler:
            self.logger.info("ğŸš€ ä½¿ç”¨å¢å¼ºç‰ˆçˆ¬è™«æ”¶é›†æ–°é—»...")
            try:
                enhanced_news = self.enhanced_crawler.collect_all_news_enhanced()
                if enhanced_news:
                    self.logger.info(f"âœ… å¢å¼ºç‰ˆçˆ¬è™«è·å–åˆ° {len(enhanced_news)} æ¡æ–°é—»")
                    return enhanced_news
                else:
                    self.logger.warning("âš ï¸ å¢å¼ºç‰ˆçˆ¬è™«æœªè·å–åˆ°æ–°é—»ï¼Œå°è¯•åŸºç¡€ç‰ˆæœ¬")
            except Exception as e:
                self.logger.error(f"âŒ å¢å¼ºç‰ˆçˆ¬è™«å¤±è´¥: {e}ï¼Œå°è¯•åŸºç¡€ç‰ˆæœ¬")
        
        # åŸºç¡€ç‰ˆæœ¬çˆ¬è™«ä½œä¸ºå¤‡ç”¨
        self.logger.info("ğŸ“° ä½¿ç”¨åŸºç¡€ç‰ˆçˆ¬è™«æ”¶é›†æ–°é—»...")
        all_news = []
        
        # ä¸»è¦æ–°é—»æºçˆ¬è™«
        main_crawlers = [
            self.crawl_sina_finance,
            self.crawl_eastmoney_news,
            self.crawl_cailianshe_news
        ]
        
        for crawler in main_crawlers:
            try:
                news = crawler()
                all_news.extend(news)
                self.logger.info(f"{crawler.__name__} è·å–åˆ° {len(news)} æ¡æ–°é—»")
                time.sleep(random.uniform(1, 3))  # éšæœºå»¶è¿Ÿé¿å…è¢«å°
            except Exception as e:
                self.logger.error(f"çˆ¬å–æ–°é—»å¤±è´¥ {crawler.__name__}: {e}")
        
        # å¦‚æœä¸»è¦æºæ–°é—»ä¸è¶³ï¼Œå°è¯•é¢å¤–çš„æ–°é—»æº
        if len(all_news) < 5:
            self.logger.info("ä¸»è¦æ–°é—»æºæ•°æ®ä¸è¶³ï¼Œå°è¯•é¢å¤–æ–°é—»æº...")
            try:
                additional_news = self.add_real_time_news_sources()
                all_news.extend(additional_news)
                self.logger.info(f"é¢å¤–æ–°é—»æºè·å–åˆ° {len(additional_news)} æ¡æ–°é—»")
            except Exception as e:
                self.logger.error(f"é¢å¤–æ–°é—»æºçˆ¬å–å¤±è´¥: {e}")
        
        # å»é‡å’Œè¿‡æ»¤
        unique_news = self.deduplicate_news(all_news)
        
        # æŒ‰æ—¶é—´æ’åº
        try:
            unique_news.sort(key=lambda x: x.get('time', ''), reverse=True)
        except Exception as e:
            self.logger.warning(f"æ–°é—»æ’åºå¤±è´¥: {e}")
        
        if not unique_news:
            self.logger.error("âŒ æ‰€æœ‰æ–°é—»æºéƒ½æ— æ³•è·å–çœŸå®æ•°æ®ï¼Œæœ¬æ¬¡æ¨é€å–æ¶ˆ")
            return []
        
        self.logger.info(f"âœ… åŸºç¡€ç‰ˆæˆåŠŸæ”¶é›†åˆ° {len(unique_news)} æ¡çœŸå®è´¢ç»æ–°é—»")
        return unique_news[:15]  # è¿”å›æœ€æ–°çš„15æ¡
    
    def send_markdown(self, content: str) -> bool:
        """å‘é€å•æ¡Markdownæ¶ˆæ¯åˆ°ä¼ä¸šå¾®ä¿¡"""
        try:
            data = {
                "msgtype": "markdown",
                "markdown": {
                    "content": content
                }
            }
            
            response = requests.post(
                self.webhook_url,
                json=data,
                headers={'Content-Type': 'application/json'},
                timeout=10
            )
            
            if response.status_code == 200:
                result = response.json()
                if result.get('errcode') == 0:
                    self.logger.info("æ–°é—»æŠ¥å‘Šå‘é€æˆåŠŸ")
                    return True
                else:
                    self.logger.error(f"å‘é€å¤±è´¥: {result}")
                    return False
            else:
                self.logger.error(f"HTTPé”™è¯¯: {response.status_code}")
                return False
                
        except Exception as e:
            self.logger.error(f"å‘é€æ¶ˆæ¯å¼‚å¸¸: {e}")
            return False
    
    def send_messages(self, messages: List[str]) -> bool:
        """å‘é€å¤šæ¡æ¶ˆæ¯åˆ°ä¼ä¸šå¾®ä¿¡"""
        if not messages:
            self.logger.error("æ²¡æœ‰æ¶ˆæ¯å¯å‘é€")
            return False
            
        success_count = 0
        for i, message in enumerate(messages):
            try:
                if i > 0:
                    # æ¶ˆæ¯ä¹‹é—´æ·»åŠ çŸ­æš‚å»¶è¿Ÿï¼Œé¿å…é¢‘ç‡é™åˆ¶
                    time.sleep(1)
                    
                success = self.send_markdown(message)
                if success:
                    success_count += 1
                else:
                    self.logger.error(f"ç¬¬{i+1}æ¡æ¶ˆæ¯å‘é€å¤±è´¥")
            except Exception as e:
                self.logger.error(f"å‘é€ç¬¬{i+1}æ¡æ¶ˆæ¯å¼‚å¸¸: {e}")
                
        self.logger.info(f"æˆåŠŸå‘é€ {success_count}/{len(messages)} æ¡æ¶ˆæ¯")
        return success_count == len(messages)
    
    def send_morning_news(self):
        """å‘é€æ—©é—´æ–°é—» - åªå‘é€çœŸå®æ•°æ®"""
        self.logger.info("å¼€å§‹å‘é€æ—©é—´è´¢ç»æ–°é—»...")
        news_list = self.collect_all_news()
        
        if not news_list:
            error_msg = f"""âŒ **æ—©é—´æ–°é—»æ¨é€å¤±è´¥**

ğŸ“… æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M')}
ğŸš« åŸå› : æ‰€æœ‰æ–°é—»æºéƒ½æ— æ³•è·å–çœŸå®æ•°æ®
ğŸ”„ ä¸‹æ¬¡æ¨é€: {datetime.now().strftime('%Y-%m-%d')} 20:00
âš ï¸ ç³»ç»Ÿç»ä¸ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®ï¼Œç¡®ä¿ä¿¡æ¯çœŸå®æ€§"""
            
            self.send_markdown(error_msg)
            self.logger.error("æ—©é—´æ–°é—»: æ— çœŸå®æ•°æ®å¯å‘é€")
            return
        
        messages = self.format_news_report(news_list, 'morning')
        success = self.send_messages(messages)
        
        if success:
            self.logger.info(f"âœ… æ—©é—´è´¢ç»æ–°é—»å‘é€æˆåŠŸ ({len(news_list)} æ¡çœŸå®æ–°é—»)")
        else:
            self.logger.error("æ—©é—´è´¢ç»æ–°é—»å‘é€å¤±è´¥")
    
    def send_midday_news(self):
        """å‘é€åˆé—´æ–°é—»"""
        self.logger.info("å¼€å§‹å‘é€åˆé—´è´¢ç»æ–°é—»...")
        news_list = self.collect_all_news()
        messages = self.format_news_report(news_list, 'midday')
        success = self.send_messages(messages)
        
        if success:
            self.logger.info("åˆé—´è´¢ç»æ–°é—»å‘é€æˆåŠŸ")
        else:
            self.logger.error("åˆé—´è´¢ç»æ–°é—»å‘é€å¤±è´¥")
    
    def send_afternoon_news(self):
        """å‘é€ä¸‹åˆæ–°é—»"""
        self.logger.info("å¼€å§‹å‘é€ä¸‹åˆè´¢ç»æ–°é—»...")
        news_list = self.collect_all_news()
        messages = self.format_news_report(news_list, 'afternoon')
        success = self.send_messages(messages)
        
        if success:
            self.logger.info("ä¸‹åˆè´¢ç»æ–°é—»å‘é€æˆåŠŸ")
        else:
            self.logger.error("ä¸‹åˆè´¢ç»æ–°é—»å‘é€å¤±è´¥")
    
    def send_evening_news(self):
        """å‘é€æ™šé—´æ–°é—» - åªå‘é€çœŸå®æ•°æ®"""
        self.logger.info("å¼€å§‹å‘é€æ™šé—´è´¢ç»æ–°é—»...")
        news_list = self.collect_all_news()
        
        if not news_list:
            error_msg = f"""âŒ **æ™šé—´æ–°é—»æ¨é€å¤±è´¥**

ğŸ“… æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M')}
ğŸš« åŸå› : æ‰€æœ‰æ–°é—»æºéƒ½æ— æ³•è·å–çœŸå®æ•°æ®
ğŸ”„ ä¸‹æ¬¡æ¨é€: {(datetime.now() + timedelta(days=1)).strftime('%Y-%m-%d')} 08:00
âš ï¸ ç³»ç»Ÿç»ä¸ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®ï¼Œç¡®ä¿ä¿¡æ¯çœŸå®æ€§"""
            
            self.send_markdown(error_msg)
            self.logger.error("æ™šé—´æ–°é—»: æ— çœŸå®æ•°æ®å¯å‘é€")
            return
        
        messages = self.format_news_report(news_list, 'evening')
        success = self.send_messages(messages)
        
        if success:
            self.logger.info(f"âœ… æ™šé—´è´¢ç»æ–°é—»å‘é€æˆåŠŸ ({len(news_list)} æ¡çœŸå®æ–°é—»)")
        else:
            self.logger.error("æ™šé—´è´¢ç»æ–°é—»å‘é€å¤±è´¥")
    
    def send_night_news(self):
        """å‘é€å¤œé—´æ–°é—»"""
        self.logger.info("å¼€å§‹å‘é€å¤œé—´è´¢ç»æ–°é—»...")
        news_list = self.collect_all_news()
        messages = self.format_news_report(news_list, 'night')
        success = self.send_messages(messages)
        
        if success:
            self.logger.info("å¤œé—´è´¢ç»æ–°é—»å‘é€æˆåŠŸ")
        else:
            self.logger.error("å¤œé—´è´¢ç»æ–°é—»å‘é€å¤±è´¥")
    
    def start_scheduler(self):
        """å¯åŠ¨å®šæ—¶ä»»åŠ¡"""
        self.logger.info("å¯åŠ¨æ–°é—»çˆ¬è™«å®šæ—¶ä»»åŠ¡...")
        
        # è®¾ç½®å®šæ—¶ä»»åŠ¡ - å¢åŠ æ¨é€é¢‘ç‡
        schedule.every().day.at("08:00").do(self.send_morning_news)
        schedule.every().day.at("12:30").do(self.send_midday_news)  # æ–°å¢åˆé—´æ–°é—»
        schedule.every().day.at("15:30").do(self.send_afternoon_news)  # æ–°å¢ä¸‹åˆæ–°é—»
        schedule.every().day.at("20:00").do(self.send_evening_news)
        schedule.every().day.at("22:00").do(self.send_night_news)  # æ–°å¢å¤œé—´æ–°é—»
        
        self.logger.info("å®šæ—¶ä»»åŠ¡å·²è®¾ç½®:")
        self.logger.info("- æ—©é—´æ–°é—»: æ¯æ—¥ 08:00")
        self.logger.info("- åˆé—´æ–°é—»: æ¯æ—¥ 12:30")
        self.logger.info("- ä¸‹åˆæ–°é—»: æ¯æ—¥ 15:30")
        self.logger.info("- æ™šé—´æ–°é—»: æ¯æ—¥ 20:00")
        self.logger.info("- å¤œé—´æ–°é—»: æ¯æ—¥ 22:00")
        
        # å‘é€å¯åŠ¨é€šçŸ¥
        startup_msg = f"""ğŸ¤– **æ–°é—»çˆ¬è™«æœºå™¨äººå·²å¯åŠ¨**

ğŸ“… **æ¨é€æ—¶é—´è¡¨**
â€¢ 08:00 - æ—©é—´è´¢ç»æ–°é—»æ±‡æ€»
â€¢ 20:00 - æ™šé—´è´¢ç»æ–°é—»æ±‡æ€»

ğŸ“° **æ–°é—»æºè¦†ç›–**
â€¢ æ–°æµªè´¢ç»ã€ä¸œæ–¹è´¢å¯Œã€è´¢è”ç¤¾
â€¢ é‡ç‚¹å…³æ³¨è‚¡å¸‚ã€æ”¿ç­–ã€è¡Œä¸šåŠ¨æ€

ğŸš€ æœºå™¨äººå·²å¼€å§‹å·¥ä½œï¼Œä¸ºæ‚¨æä¾›åŠæ—¶çš„è´¢ç»èµ„è®¯ï¼

â° å¯åŠ¨æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"""
        
        self.send_markdown(startup_msg)
        
        # è¿è¡Œå®šæ—¶ä»»åŠ¡
        while True:
            try:
                schedule.run_pending()
                time.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
            except KeyboardInterrupt:
                self.logger.info("æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œæ­£åœ¨å…³é—­æ–°é—»çˆ¬è™«æœºå™¨äºº...")
                break
            except Exception as e:
                self.logger.error(f"å®šæ—¶ä»»åŠ¡æ‰§è¡Œå¼‚å¸¸: {e}")
                time.sleep(60)

if __name__ == "__main__":
    # æµ‹è¯•ç”¨é…ç½®
    webhook_url = "https://qyapi.weixin.qq.com/cgi-bin/webhook/send?key=6d5ddff9-8787-4863-922a-6b2c1ab9f450"
    
    bot = NewsCrawlerBot(webhook_url)
    
    # æµ‹è¯•æ–°é—»æ”¶é›†
    print("æµ‹è¯•æ–°é—»æ”¶é›†...")
    news = bot.collect_all_news()
    print(f"æ”¶é›†åˆ° {len(news)} æ¡æ–°é—»")
    
    # æµ‹è¯•æŠ¥å‘Šç”Ÿæˆ
    report = bot.format_news_report(news, 'morning')
    print("ç”Ÿæˆçš„æŠ¥å‘Š:")
    print(report)