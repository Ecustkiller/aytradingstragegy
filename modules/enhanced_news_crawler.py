#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
å¢å¼ºç‰ˆæ–°é—»çˆ¬è™« - æ”¯æŒæ›´å¤šæ•°æ®æºå’Œæ¨¡æ‹Ÿæµè§ˆå™¨
"""

import requests
import json
import logging
import time
from datetime import datetime, timedelta
import schedule
from bs4 import BeautifulSoup
import feedparser
import re
from typing import List, Dict, Optional
import threading
import hashlib
import random
from urllib.parse import urljoin, urlparse
import asyncio
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.action_chains import ActionChains

class EnhancedNewsCrawler:
    """å¢å¼ºç‰ˆè´¢ç»æ–°é—»çˆ¬è™« - æ”¯æŒæ›´å¤šæ•°æ®æº"""
    
    def __init__(self, webhook_url: str):
        self.webhook_url = webhook_url
        self.setup_logging()
        self.seen_news = set()
        self.session = requests.Session()
        self.setup_session_headers()
        self.chrome_options = self.setup_chrome_options()
        
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s%(msecs)03d - %(levelname)s - %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S',
            handlers=[
                logging.FileHandler('enhanced_news_crawler.log', encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def setup_session_headers(self):
        """è®¾ç½®sessionçš„é€šç”¨headers"""
        user_agents = [
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0'
        ]
        
        self.session.headers.update({
            'User-Agent': random.choice(user_agents),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0'
        })
    
    def setup_chrome_options(self):
        """è®¾ç½®Chromeé€‰é¡¹"""
        options = Options()
        options.add_argument('--headless')  # æ— å¤´æ¨¡å¼
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-gpu')
        options.add_argument('--window-size=1920,1080')
        options.add_argument('--user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
        return options
    
    def create_driver(self):
        """åˆ›å»ºWebDriverå®ä¾‹"""
        try:
            driver = webdriver.Chrome(options=self.chrome_options)
            driver.implicitly_wait(10)
            return driver
        except Exception as e:
            self.logger.error(f"åˆ›å»ºWebDriverå¤±è´¥: {e}")
            return None
    
    # ==================== è´¢è”ç¤¾æ–°é—»æº ====================
    def crawl_cailianshe_selenium(self) -> List[Dict]:
        """ä½¿ç”¨Seleniumçˆ¬å–è´¢è”ç¤¾å¿«è®¯"""
        news_list = []
        driver = None
        
        try:
            driver = self.create_driver()
            if not driver:
                return news_list
            
            self.logger.info("ä½¿ç”¨Seleniumè®¿é—®è´¢è”ç¤¾...")
            driver.get('https://www.cls.cn/telegraph')
            
            # ç­‰å¾…é¡µé¢åŠ è½½
            time.sleep(3)
            
            # å°è¯•ç‚¹å‡»åŠ è½½æ›´å¤šæŒ‰é’®
            try:
                load_more_btn = WebDriverWait(driver, 5).until(
                    EC.element_to_be_clickable((By.CLASS_NAME, "load-more"))
                )
                ActionChains(driver).move_to_element(load_more_btn).click().perform()
                time.sleep(2)
            except:
                pass
            
            # è·å–å¿«è®¯å†…å®¹
            selectors = [
                '.telegraph-item',
                '.telegraph-content',
                '[class*="telegraph"]',
                '.news-item',
                '.item-content'
            ]
            
            for selector in selectors:
                try:
                    elements = driver.find_elements(By.CSS_SELECTOR, selector)
                    if elements:
                        self.logger.info(f"è´¢è”ç¤¾Seleniumæ‰¾åˆ° {len(elements)} ä¸ªå…ƒç´  (é€‰æ‹©å™¨: {selector})")
                        
                        for element in elements[:30]:
                            try:
                                title = element.text.strip()
                                
                                # æ¸…ç†æ ‡é¢˜
                                title = re.sub(r'\s+', ' ', title)
                                title = re.sub(r'^[0-9:\-\s]+', '', title)
                                
                                if (len(title) > 15 and len(title) < 300 and 
                                    self.is_finance_related(title) and
                                    not self.is_duplicate_news(title)):
                                    
                                    # å°è¯•è·å–é“¾æ¥
                                    try:
                                        link_element = element.find_element(By.TAG_NAME, 'a')
                                        news_url = link_element.get_attribute('href')
                                    except:
                                        news_url = 'https://www.cls.cn/telegraph'
                                    
                                    news_item = {
                                        'title': title,
                                        'url': news_url,
                                        'time': datetime.now().strftime('%Y-%m-%d %H:%M'),
                                        'source': 'è´¢è”ç¤¾',
                                        'method': 'selenium'
                                    }
                                    news_list.append(news_item)
                                    
                                    if len(news_list) >= 15:
                                        break
                            except Exception as e:
                                continue
                        
                        if news_list:
                            break
                except Exception as e:
                    continue
                    
        except Exception as e:
            self.logger.error(f"è´¢è”ç¤¾Seleniumçˆ¬å–å¤±è´¥: {e}")
        finally:
            if driver:
                driver.quit()
        
        return news_list
    
    def crawl_cailianshe_api_enhanced(self) -> List[Dict]:
        """å¢å¼ºç‰ˆè´¢è”ç¤¾APIçˆ¬å–"""
        news_list = []
        
        # å¤šä¸ªAPIç«¯ç‚¹
        api_endpoints = [
            {
                'url': 'https://www.cls.cn/api/sw',
                'params': {
                    'app': 'CailianpressWeb',
                    'os': 'web',
                    'sv': '7.7.5',
                    'channel': 'telegraph',
                    'limit': 30
                }
            },
            {
                'url': 'https://m.cls.cn/api/telegraph',
                'params': {
                    'limit': 30,
                    'channel': 'all'
                }
            },
            {
                'url': 'https://www.cls.cn/nodeapi/telegraphList',
                'params': {
                    'app': 'CailianpressWeb',
                    'limit': 30,
                    'lastTime': int(time.time())
                }
            }
        ]
        
        for endpoint in api_endpoints:
            try:
                response = self.session.get(endpoint['url'], params=endpoint['params'], timeout=10)
                
                if response.status_code == 200:
                    data = response.json()
                    
                    # å¤„ç†ä¸åŒAPIçš„å“åº”æ ¼å¼
                    items = []
                    if 'data' in data and isinstance(data['data'], dict) and 'list' in data['data']:
                        items = data['data']['list']
                    elif 'data' in data and isinstance(data['data'], list):
                        items = data['data']
                    elif isinstance(data, list):
                        items = data
                    
                    for item in items:
                        title = item.get('title', '').strip() or item.get('content', '').strip()[:100]
                        
                        if (title and len(title) > 10 and 
                            self.is_finance_related(title) and
                            not self.is_duplicate_news(title)):
                            
                            news_item = {
                                'title': title,
                                'url': f"https://www.cls.cn/telegraph/{item.get('id', '')}",
                                'time': self.format_time(item.get('publish_time', item.get('ctime', ''))),
                                'source': 'è´¢è”ç¤¾',
                                'method': 'api'
                            }
                            news_list.append(news_item)
                    
                    if news_list:
                        self.logger.info(f"è´¢è”ç¤¾APIè·å–åˆ° {len(news_list)} æ¡æ–°é—»")
                        break
                        
            except Exception as e:
                self.logger.warning(f"è´¢è”ç¤¾APIç«¯ç‚¹å¤±è´¥: {e}")
                continue
        
        return news_list
    
    # ==================== ä¸œæ–¹è´¢å¯Œæ–°é—»æº ====================
    def crawl_eastmoney_enhanced(self) -> List[Dict]:
        """å¢å¼ºç‰ˆä¸œæ–¹è´¢å¯Œæ–°é—»çˆ¬å–"""
        news_list = []
        
        # å¤šä¸ªä¸œæ–¹è´¢å¯Œæ•°æ®æº
        sources = [
            {
                'name': 'ä¸œæ–¹è´¢å¯Œå¿«è®¯',
                'url': 'https://finance.eastmoney.com/news/cjkx.html',
                'selectors': ['.newslist li a', '.news-item a', '.content-list a']
            },
            {
                'name': 'ä¸œæ–¹è´¢å¯Œè¦é—»',
                'url': 'https://finance.eastmoney.com/',
                'selectors': ['.news-list a', '.important-news a', '.finance-news a']
            },
            {
                'name': 'ä¸œæ–¹è´¢å¯Œç§»åŠ¨ç«¯',
                'url': 'https://wap.eastmoney.com/news/',
                'selectors': ['.news-item a', '.article-item a', '.list-item a']
            }
        ]
        
        for source in sources:
            try:
                response = self.session.get(source['url'], timeout=15)
                if response.status_code == 200:
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    for selector in source['selectors']:
                        elements = soup.select(selector)
                        if elements:
                            for element in elements[:20]:
                                title = element.get_text().strip()
                                href = element.get('href', '')
                                
                                if (len(title) > 10 and self.is_finance_related(title) and
                                    not self.is_duplicate_news(title)):
                                    
                                    # å¤„ç†ç›¸å¯¹é“¾æ¥
                                    if href.startswith('/'):
                                        href = 'https://finance.eastmoney.com' + href
                                    elif href.startswith('//'):
                                        href = 'https:' + href
                                    elif not href.startswith('http'):
                                        continue
                                    
                                    news_item = {
                                        'title': title,
                                        'url': href,
                                        'time': datetime.now().strftime('%Y-%m-%d %H:%M'),
                                        'source': source['name'],
                                        'method': 'web'
                                    }
                                    news_list.append(news_item)
                                    
                                    if len(news_list) >= 15:
                                        break
                            
                            if len(news_list) >= 10:
                                break
                    
                    if len(news_list) >= 8:
                        break
                        
            except Exception as e:
                self.logger.warning(f"ä¸œæ–¹è´¢å¯Œæº {source['name']} å¤±è´¥: {e}")
                continue
        
        return news_list
    
    # ==================== æ–°æµªè´¢ç»æ–°é—»æº ====================
    def crawl_sina_finance_enhanced(self) -> List[Dict]:
        """å¢å¼ºç‰ˆæ–°æµªè´¢ç»æ–°é—»çˆ¬å–"""
        news_list = []
        
        sources = [
            {
                'name': 'æ–°æµªè´¢ç»',
                'url': 'https://finance.sina.com.cn/',
                'selectors': ['.news-list a', '.important-news a', '.finance-list a']
            },
            {
                'name': 'æ–°æµªè´¢ç»æ»šåŠ¨',
                'url': 'https://finance.sina.com.cn/roll/',
                'selectors': ['.d_list_txt a', '.news-item a', '.list-item a']
            },
            {
                'name': 'æ–°æµªè´¢ç»è¦é—»',
                'url': 'https://finance.sina.com.cn/china/',
                'selectors': ['.blk_03 a', '.news-list a', '.content-list a']
            }
        ]
        
        for source in sources:
            try:
                response = self.session.get(source['url'], timeout=15)
                if response.status_code == 200:
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    # è·å–æ‰€æœ‰é“¾æ¥
                    all_links = soup.find_all('a', href=True)
                    
                    for link in all_links[:50]:
                        title = link.get_text().strip()
                        href = link.get('href', '')
                        
                        if (len(title) > 10 and self.is_finance_related(title) and
                            ('finance.sina.com.cn' in href or href.startswith('/')) and
                            not self.is_duplicate_news(title)):
                            
                            # å¤„ç†ç›¸å¯¹é“¾æ¥
                            if href.startswith('//'):
                                href = 'https:' + href
                            elif href.startswith('/'):
                                href = 'https://finance.sina.com.cn' + href
                            
                            news_item = {
                                'title': title,
                                'url': href,
                                'time': datetime.now().strftime('%Y-%m-%d %H:%M'),
                                'source': source['name'],
                                'method': 'web'
                            }
                            news_list.append(news_item)
                            
                            if len(news_list) >= 10:
                                break
                    
                    if len(news_list) >= 5:
                        break
                        
            except Exception as e:
                self.logger.warning(f"æ–°æµªè´¢ç»æº {source['name']} å¤±è´¥: {e}")
                continue
        
        return news_list
    
    # ==================== æ›´å¤šæ–°é—»æº ====================
    def crawl_wallstreetcn_news(self) -> List[Dict]:
        """çˆ¬å–åå°”è¡—è§é—»"""
        news_list = []
        
        try:
            # åå°”è¡—è§é—»API
            api_url = 'https://api-prod.wallstreetcn.com/apiv1/content/articles'
            params = {
                'limit': 20,
                'platform': 'web',
                'fields': 'title,summary,display_time,uri'
            }
            
            response = self.session.get(api_url, params=params, timeout=10)
            
            if response.status_code == 200:
                data = response.json()
                items = data.get('data', {}).get('items', [])
                
                for item in items:
                    title = item.get('title', '').strip()
                    
                    if (title and len(title) > 10 and 
                        self.is_finance_related(title) and
                        not self.is_duplicate_news(title)):
                        
                        news_item = {
                            'title': title,
                            'url': f"https://wallstreetcn.com/articles/{item.get('id', '')}",
                            'time': self.format_time(item.get('display_time', '')),
                            'source': 'åå°”è¡—è§é—»',
                            'method': 'api'
                        }
                        news_list.append(news_item)
                        
        except Exception as e:
            self.logger.error(f"åå°”è¡—è§é—»çˆ¬å–å¤±è´¥: {e}")
        
        return news_list
    
    def crawl_yicai_news(self) -> List[Dict]:
        """çˆ¬å–ç¬¬ä¸€è´¢ç»"""
        news_list = []
        
        try:
            url = 'https://www.yicai.com/api/ajax/getlatest'
            params = {
                'page': 1,
                'pagesize': 20
            }
            
            response = self.session.get(url, params=params, timeout=10)
            
            if response.status_code == 200:
                data = response.json()
                items = data.get('data', [])
                
                # ç¡®ä¿itemsæ˜¯åˆ—è¡¨
                if not isinstance(items, list):
                    items = []
                
                for item in items:
                    title = item.get('NewsTitle', '').strip()
                    
                    if (title and len(title) > 10 and 
                        self.is_finance_related(title) and
                        not self.is_duplicate_news(title)):
                        
                        news_item = {
                            'title': title,
                            'url': f"https://www.yicai.com/news/{item.get('NewsID', '')}",
                            'time': self.format_time(item.get('CreateTime', '')),
                            'source': 'ç¬¬ä¸€è´¢ç»',
                            'method': 'api'
                        }
                        news_list.append(news_item)
                        
        except Exception as e:
            self.logger.error(f"ç¬¬ä¸€è´¢ç»çˆ¬å–å¤±è´¥: {e}")
        
        return news_list
    
    def crawl_jrj_news(self) -> List[Dict]:
        """çˆ¬å–é‡‘èç•Œ"""
        news_list = []
        
        try:
            url = 'https://finance.jrj.com.cn/'
            response = self.session.get(url, timeout=10)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # æŸ¥æ‰¾æ–°é—»é“¾æ¥
                links = soup.find_all('a', href=True)
                
                for link in links[:40]:
                    title = link.get_text().strip()
                    href = link.get('href', '')
                    
                    if (len(title) > 10 and self.is_finance_related(title) and
                        ('jrj.com.cn' in href or href.startswith('/')) and
                        not self.is_duplicate_news(title)):
                        
                        if href.startswith('/'):
                            href = 'https://finance.jrj.com.cn' + href
                        elif not href.startswith('http'):
                            continue
                        
                        news_item = {
                            'title': title,
                            'url': href,
                            'time': datetime.now().strftime('%Y-%m-%d %H:%M'),
                            'source': 'é‡‘èç•Œ',
                            'method': 'web'
                        }
                        news_list.append(news_item)
                        
                        if len(news_list) >= 8:
                            break
                            
        except Exception as e:
            self.logger.error(f"é‡‘èç•Œçˆ¬å–å¤±è´¥: {e}")
        
        return news_list
    
    def crawl_cnstock_news(self) -> List[Dict]:
        """çˆ¬å–ä¸­å›½è¯åˆ¸ç½‘"""
        news_list = []
        
        try:
            url = 'https://www.cnstock.com/'
            response = self.session.get(url, timeout=10)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                
                links = soup.find_all('a', href=True)
                
                for link in links[:40]:
                    title = link.get_text().strip()
                    href = link.get('href', '')
                    
                    if (len(title) > 10 and self.is_finance_related(title) and
                        ('cnstock.com' in href or href.startswith('/')) and
                        not self.is_duplicate_news(title)):
                        
                        if href.startswith('/'):
                            href = 'https://www.cnstock.com' + href
                        elif not href.startswith('http'):
                            continue
                        
                        news_item = {
                            'title': title,
                            'url': href,
                            'time': datetime.now().strftime('%Y-%m-%d %H:%M'),
                            'source': 'ä¸­å›½è¯åˆ¸ç½‘',
                            'method': 'web'
                        }
                        news_list.append(news_item)
                        
                        if len(news_list) >= 8:
                            break
                            
        except Exception as e:
            self.logger.error(f"ä¸­å›½è¯åˆ¸ç½‘çˆ¬å–å¤±è´¥: {e}")
        
        return news_list
    
    def crawl_stcn_enhanced(self) -> List[Dict]:
        """å¢å¼ºç‰ˆè¯åˆ¸æ—¶æŠ¥çˆ¬å–"""
        news_list = []
        
        sources = [
            'https://www.stcn.com/',
            'https://kuaixun.stcn.com/',
            'https://news.stcn.com/'
        ]
        
        for url in sources:
            try:
                response = self.session.get(url, timeout=10)
                
                if response.status_code == 200:
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    links = soup.find_all('a', href=True)
                    
                    for link in links[:30]:
                        title = link.get_text().strip()
                        href = link.get('href', '')
                        
                        if (len(title) > 10 and self.is_finance_related(title) and
                            ('stcn.com' in href or href.startswith('/')) and
                            not self.is_duplicate_news(title)):
                            
                            if href.startswith('/'):
                                href = 'https://www.stcn.com' + href
                            elif not href.startswith('http'):
                                continue
                            
                            news_item = {
                                'title': title,
                                'url': href,
                                'time': datetime.now().strftime('%Y-%m-%d %H:%M'),
                                'source': 'è¯åˆ¸æ—¶æŠ¥',
                                'method': 'web'
                            }
                            news_list.append(news_item)
                            
                            if len(news_list) >= 5:
                                break
                
                if len(news_list) >= 3:
                    break
                    
            except Exception as e:
                self.logger.warning(f"è¯åˆ¸æ—¶æŠ¥æºå¤±è´¥: {e}")
                continue
        
        return news_list
    
    # ==================== æ›´å¤šä¸­å›½å›½å†…æ–°é—»æº ====================
    def crawl_people_finance(self) -> List[Dict]:
        """çˆ¬å–äººæ°‘ç½‘è´¢ç»"""
        news_list = []
        
        try:
            url = 'http://finance.people.com.cn/'
            response = self.session.get(url, timeout=10)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # æŸ¥æ‰¾è´¢ç»æ–°é—»é“¾æ¥
                selectors = [
                    '.news_list a',
                    '.list_14 a', 
                    '.w1000_320_left a',
                    '.clearfix a'
                ]
                
                for selector in selectors:
                    links = soup.select(selector)
                    if links:
                        for link in links[:20]:
                            title = link.get_text().strip()
                            href = link.get('href', '')
                            
                            if (len(title) > 10 and self.is_finance_related(title) and
                                not self.is_duplicate_news(title)):
                                
                                if href.startswith('/'):
                                    href = 'http://finance.people.com.cn' + href
                                elif not href.startswith('http'):
                                    continue
                                
                                news_item = {
                                    'title': title,
                                    'url': href,
                                    'time': datetime.now().strftime('%Y-%m-%d %H:%M'),
                                    'source': 'äººæ°‘ç½‘è´¢ç»',
                                    'method': 'web'
                                }
                                news_list.append(news_item)
                                
                                if len(news_list) >= 8:
                                    break
                        
                        if news_list:
                            break
                            
        except Exception as e:
            self.logger.error(f"äººæ°‘ç½‘è´¢ç»çˆ¬å–å¤±è´¥: {e}")
        
        return news_list
    
    def crawl_xinhua_finance_enhanced(self) -> List[Dict]:
        """å¢å¼ºç‰ˆæ–°åç½‘è´¢ç»"""
        news_list = []
        
        sources = [
            'http://www.xinhuanet.com/fortune/',
            'http://www.xinhuanet.com/money/',
            'http://www.xinhuanet.com/stock/'
        ]
        
        for url in sources:
            try:
                response = self.session.get(url, timeout=10)
                
                if response.status_code == 200:
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    links = soup.find_all('a', href=True)
                    
                    for link in links[:30]:
                        title = link.get_text().strip()
                        href = link.get('href', '')
                        
                        if (len(title) > 10 and self.is_finance_related(title) and
                            ('xinhuanet.com' in href or href.startswith('/')) and
                            not self.is_duplicate_news(title)):
                            
                            if href.startswith('/'):
                                href = 'http://www.xinhuanet.com' + href
                            elif not href.startswith('http'):
                                continue
                            
                            news_item = {
                                'title': title,
                                'url': href,
                                'time': datetime.now().strftime('%Y-%m-%d %H:%M'),
                                'source': 'æ–°åç½‘è´¢ç»',
                                'method': 'web'
                            }
                            news_list.append(news_item)
                            
                            if len(news_list) >= 5:
                                break
                
                if len(news_list) >= 3:
                    break
                    
            except Exception as e:
                self.logger.warning(f"æ–°åç½‘è´¢ç»æºå¤±è´¥: {e}")
                continue
        
        return news_list
    
    def crawl_cctv_finance(self) -> List[Dict]:
        """çˆ¬å–å¤®è§†è´¢ç»"""
        news_list = []
        
        try:
            # å¤®è§†è´¢ç»é¢‘é“
            url = 'https://tv.cctv.com/lm/jjxx/'
            response = self.session.get(url, timeout=10)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # æŸ¥æ‰¾æ–°é—»é“¾æ¥
                links = soup.find_all('a', href=True)
                
                for link in links[:30]:
                    title = link.get_text().strip()
                    href = link.get('href', '')
                    
                    if (len(title) > 10 and self.is_finance_related(title) and
                        not self.is_duplicate_news(title)):
                        
                        if href.startswith('/'):
                            href = 'https://tv.cctv.com' + href
                        elif not href.startswith('http'):
                            continue
                        
                        news_item = {
                            'title': title,
                            'url': href,
                            'time': datetime.now().strftime('%Y-%m-%d %H:%M'),
                            'source': 'å¤®è§†è´¢ç»',
                            'method': 'web'
                        }
                        news_list.append(news_item)
                        
                        if len(news_list) >= 8:
                            break
                            
        except Exception as e:
            self.logger.error(f"å¤®è§†è´¢ç»çˆ¬å–å¤±è´¥: {e}")
        
        return news_list
    
    def crawl_caixin_news(self) -> List[Dict]:
        """çˆ¬å–è´¢æ–°ç½‘"""
        news_list = []
        
        try:
            url = 'https://www.caixin.com/'
            response = self.session.get(url, timeout=10)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # æŸ¥æ‰¾æ–°é—»é“¾æ¥
                selectors = [
                    '.news-list a',
                    '.article-list a',
                    '.content-list a'
                ]
                
                for selector in selectors:
                    links = soup.select(selector)
                    if links:
                        for link in links[:20]:
                            title = link.get_text().strip()
                            href = link.get('href', '')
                            
                            if (len(title) > 10 and self.is_finance_related(title) and
                                not self.is_duplicate_news(title)):
                                
                                if href.startswith('/'):
                                    href = 'https://www.caixin.com' + href
                                elif not href.startswith('http'):
                                    continue
                                
                                news_item = {
                                    'title': title,
                                    'url': href,
                                    'time': datetime.now().strftime('%Y-%m-%d %H:%M'),
                                    'source': 'è´¢æ–°ç½‘',
                                    'method': 'web'
                                }
                                news_list.append(news_item)
                                
                                if len(news_list) >= 8:
                                    break
                        
                        if news_list:
                            break
                            
        except Exception as e:
            self.logger.error(f"è´¢æ–°ç½‘çˆ¬å–å¤±è´¥: {e}")
        
        return news_list
    
    def crawl_21jingji_news(self) -> List[Dict]:
        """çˆ¬å–21ä¸–çºªç»æµæŠ¥é“"""
        news_list = []
        
        try:
            url = 'https://www.21jingji.com/'
            response = self.session.get(url, timeout=10)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                
                links = soup.find_all('a', href=True)
                
                for link in links[:40]:
                    title = link.get_text().strip()
                    href = link.get('href', '')
                    
                    if (len(title) > 10 and self.is_finance_related(title) and
                        ('21jingji.com' in href or href.startswith('/')) and
                        not self.is_duplicate_news(title)):
                        
                        if href.startswith('/'):
                            href = 'https://www.21jingji.com' + href
                        elif not href.startswith('http'):
                            continue
                        
                        news_item = {
                            'title': title,
                            'url': href,
                            'time': datetime.now().strftime('%Y-%m-%d %H:%M'),
                            'source': '21ä¸–çºªç»æµæŠ¥é“',
                            'method': 'web'
                        }
                        news_list.append(news_item)
                        
                        if len(news_list) >= 8:
                            break
                            
        except Exception as e:
            self.logger.error(f"21ä¸–çºªç»æµæŠ¥é“çˆ¬å–å¤±è´¥: {e}")
        
        return news_list
    
    def crawl_jiemian_finance(self) -> List[Dict]:
        """çˆ¬å–ç•Œé¢æ–°é—»è´¢ç»"""
        news_list = []
        
        try:
            url = 'https://www.jiemian.com/lists/2.html'  # è´¢ç»é¢‘é“
            response = self.session.get(url, timeout=10)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                
                links = soup.find_all('a', href=True)
                
                for link in links[:40]:
                    title = link.get_text().strip()
                    href = link.get('href', '')
                    
                    if (len(title) > 10 and self.is_finance_related(title) and
                        ('jiemian.com' in href or href.startswith('/')) and
                        not self.is_duplicate_news(title)):
                        
                        if href.startswith('/'):
                            href = 'https://www.jiemian.com' + href
                        elif not href.startswith('http'):
                            continue
                        
                        news_item = {
                            'title': title,
                            'url': href,
                            'time': datetime.now().strftime('%Y-%m-%d %H:%M'),
                            'source': 'ç•Œé¢æ–°é—»',
                            'method': 'web'
                        }
                        news_list.append(news_item)
                        
                        if len(news_list) >= 8:
                            break
                            
        except Exception as e:
            self.logger.error(f"ç•Œé¢æ–°é—»çˆ¬å–å¤±è´¥: {e}")
        
        return news_list
    
    def crawl_thepaper_finance(self) -> List[Dict]:
        """çˆ¬å–æ¾æ¹ƒæ–°é—»è´¢ç»"""
        news_list = []
        
        try:
            url = 'https://www.thepaper.cn/channel_25950'  # è´¢ç»é¢‘é“
            response = self.session.get(url, timeout=10)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                
                links = soup.find_all('a', href=True)
                
                for link in links[:40]:
                    title = link.get_text().strip()
                    href = link.get('href', '')
                    
                    if (len(title) > 10 and self.is_finance_related(title) and
                        ('thepaper.cn' in href or href.startswith('/')) and
                        not self.is_duplicate_news(title)):
                        
                        if href.startswith('/'):
                            href = 'https://www.thepaper.cn' + href
                        elif not href.startswith('http'):
                            continue
                        
                        news_item = {
                            'title': title,
                            'url': href,
                            'time': datetime.now().strftime('%Y-%m-%d %H:%M'),
                            'source': 'æ¾æ¹ƒæ–°é—»',
                            'method': 'web'
                        }
                        news_list.append(news_item)
                        
                        if len(news_list) >= 8:
                            break
                            
        except Exception as e:
            self.logger.error(f"æ¾æ¹ƒæ–°é—»çˆ¬å–å¤±è´¥: {e}")
        
        return news_list
    
    def crawl_nbd_news(self) -> List[Dict]:
        """çˆ¬å–æ¯æ—¥ç»æµæ–°é—»"""
        news_list = []
        
        try:
            url = 'https://www.nbd.com.cn/'
            response = self.session.get(url, timeout=10)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                
                links = soup.find_all('a', href=True)
                
                for link in links[:40]:
                    title = link.get_text().strip()
                    href = link.get('href', '')
                    
                    if (len(title) > 10 and self.is_finance_related(title) and
                        ('nbd.com.cn' in href or href.startswith('/')) and
                        not self.is_duplicate_news(title)):
                        
                        if href.startswith('/'):
                            href = 'https://www.nbd.com.cn' + href
                        elif not href.startswith('http'):
                            continue
                        
                        news_item = {
                            'title': title,
                            'url': href,
                            'time': datetime.now().strftime('%Y-%m-%d %H:%M'),
                            'source': 'æ¯æ—¥ç»æµæ–°é—»',
                            'method': 'web'
                        }
                        news_list.append(news_item)
                        
                        if len(news_list) >= 8:
                            break
                            
        except Exception as e:
            self.logger.error(f"æ¯æ—¥ç»æµæ–°é—»çˆ¬å–å¤±è´¥: {e}")
        
        return news_list
    
    def crawl_cs_com_cn(self) -> List[Dict]:
        """çˆ¬å–ä¸­è¯ç½‘"""
        news_list = []
        
        try:
            url = 'https://www.cs.com.cn/'
            response = self.session.get(url, timeout=10)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                
                links = soup.find_all('a', href=True)
                
                for link in links[:40]:
                    title = link.get_text().strip()
                    href = link.get('href', '')
                    
                    if (len(title) > 10 and self.is_finance_related(title) and
                        ('cs.com.cn' in href or href.startswith('/')) and
                        not self.is_duplicate_news(title)):
                        
                        if href.startswith('/'):
                            href = 'https://www.cs.com.cn' + href
                        elif not href.startswith('http'):
                            continue
                        
                        news_item = {
                            'title': title,
                            'url': href,
                            'time': datetime.now().strftime('%Y-%m-%d %H:%M'),
                            'source': 'ä¸­è¯ç½‘',
                            'method': 'web'
                        }
                        news_list.append(news_item)
                        
                        if len(news_list) >= 8:
                            break
                            
        except Exception as e:
            self.logger.error(f"ä¸­è¯ç½‘çˆ¬å–å¤±è´¥: {e}")
        
        return news_list
    
    # ==================== å·¥å…·å‡½æ•° ====================
    def is_finance_related(self, title: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºè´¢ç»ç›¸å…³æ–°é—»"""
        finance_keywords = [
            'è‚¡å¸‚', 'è‚¡ç¥¨', 'åŸºé‡‘', 'å€ºåˆ¸', 'æœŸè´§', 'å¤–æ±‡', 'é»„é‡‘', 'é“¶è¡Œ', 'ä¿é™©', 'è¯åˆ¸',
            'æŠ•èµ„', 'èèµ„', 'IPO', 'å¹¶è´­', 'å¤®è¡Œ', 'è´§å¸æ”¿ç­–', 'åˆ©ç‡', 'æ±‡ç‡', 'é€šèƒ€',
            'CPI', 'GDP', 'ä¸Šå¸‚', 'é€€å¸‚', 'åœç‰Œ', 'å¤ç‰Œ', 'æ¶¨åœ', 'è·Œåœ', 'è´¢æŠ¥', 'ä¸šç»©',
            'è¥æ”¶', 'åˆ©æ¶¦', 'äºæŸ', 'ç›‘ç®¡', 'è¯ç›‘ä¼š', 'é“¶ä¿ç›‘ä¼š', 'äº¤æ˜“æ‰€', 'ç§‘æŠ€è‚¡',
            'æ–°èƒ½æº', 'èŠ¯ç‰‡', 'åŒ»è¯', 'åœ°äº§', 'é‡‘è', 'æ¶ˆè´¹', 'åˆ¶é€ ä¸š', 'æœåŠ¡ä¸š',
            'å®è§‚ç»æµ', 'å¾®è§‚ç»æµ', 'å¸‚åœº', 'è¡Œä¸š', 'æ¿å—', 'æ¦‚å¿µè‚¡', 'é¢˜æè‚¡',
            'æœºæ„', 'åŸºé‡‘å…¬å¸', 'åˆ¸å•†', 'ä¿¡æ‰˜', 'ç§å‹Ÿ', 'å…¬å‹Ÿ', 'èµ„ç®¡', 'ç†è´¢'
        ]
        
        return any(keyword in title for keyword in finance_keywords)
    
    def is_duplicate_news(self, title: str) -> bool:
        """æ£€æŸ¥æ–°é—»æ˜¯å¦é‡å¤"""
        title_hash = hashlib.md5(title.encode()).hexdigest()
        if title_hash in self.seen_news:
            return True
        self.seen_news.add(title_hash)
        return False
    
    def format_time(self, time_str: str) -> str:
        """æ ¼å¼åŒ–æ—¶é—´å­—ç¬¦ä¸²"""
        if not time_str:
            return datetime.now().strftime('%Y-%m-%d %H:%M')
        
        try:
            if isinstance(time_str, (int, float)):
                return datetime.fromtimestamp(time_str).strftime('%Y-%m-%d %H:%M')
            elif 'T' in time_str:
                dt = datetime.fromisoformat(time_str.replace('Z', '+00:00'))
                return dt.strftime('%Y-%m-%d %H:%M')
            else:
                return time_str
        except:
            return datetime.now().strftime('%Y-%m-%d %H:%M')
    
    def deduplicate_news(self, news_list: List[Dict]) -> List[Dict]:
        """å»é‡æ–°é—»åˆ—è¡¨"""
        unique_news = []
        seen_titles = set()
        
        for news in news_list:
            title = news['title'].strip()
            title_normalized = re.sub(r'\s+', ' ', title).lower()
            
            if title_normalized not in seen_titles:
                seen_titles.add(title_normalized)
                unique_news.append(news)
        
        return unique_news
    
    # ==================== ä¸»è¦æ”¶é›†å‡½æ•° ====================
    def collect_all_news_enhanced(self) -> List[Dict]:
        """æ”¶é›†æ‰€æœ‰æ–°é—»æºçš„æ–°é—» - å¢å¼ºç‰ˆ"""
        all_news = []
        
        self.logger.info("ğŸš€ å¼€å§‹å¢å¼ºç‰ˆæ–°é—»æ”¶é›†...")
        
        # æ‰€æœ‰çˆ¬è™«å‡½æ•° - é‡ç‚¹åŠ å¼ºä¸­å›½å›½å†…æ•°æ®æº
        crawlers = [
            # è´¢è”ç¤¾ (å¤šç§æ–¹æ³•)
            self.crawl_cailianshe_selenium,
            self.crawl_cailianshe_api_enhanced,
            
            # ä¸»æµè´¢ç»ç½‘ç«™
            self.crawl_eastmoney_enhanced,
            self.crawl_sina_finance_enhanced,
            
            # å®˜æ–¹æƒå¨åª’ä½“
            self.crawl_people_finance,        # äººæ°‘ç½‘è´¢ç»
            self.crawl_xinhua_finance_enhanced,  # æ–°åç½‘è´¢ç»
            self.crawl_cctv_finance,          # å¤®è§†è´¢ç»
            
            # ä¸“ä¸šè´¢ç»åª’ä½“
            self.crawl_caixin_news,           # è´¢æ–°ç½‘
            self.crawl_21jingji_news,         # 21ä¸–çºªç»æµæŠ¥é“
            self.crawl_jiemian_finance,       # ç•Œé¢æ–°é—»
            self.crawl_thepaper_finance,      # æ¾æ¹ƒæ–°é—»
            self.crawl_nbd_news,              # æ¯æ—¥ç»æµæ–°é—»
            self.crawl_cs_com_cn,             # ä¸­è¯ç½‘
            
            # è¯åˆ¸ä¸“ä¸šåª’ä½“
            self.crawl_cnstock_news,          # ä¸­å›½è¯åˆ¸ç½‘
            self.crawl_stcn_enhanced,         # è¯åˆ¸æ—¶æŠ¥
            
            # å›½é™…è´¢ç»åª’ä½“
            self.crawl_wallstreetcn_news,     # åå°”è¡—è§é—»
            self.crawl_yicai_news,            # ç¬¬ä¸€è´¢ç»
            self.crawl_jrj_news,              # é‡‘èç•Œ
        ]
        
        for crawler in crawlers:
            try:
                start_time = time.time()
                news = crawler()
                end_time = time.time()
                
                if news:
                    all_news.extend(news)
                    self.logger.info(f"âœ… {crawler.__name__} è·å–åˆ° {len(news)} æ¡æ–°é—» (è€—æ—¶: {end_time-start_time:.2f}s)")
                else:
                    self.logger.warning(f"âŒ {crawler.__name__} æœªè·å–åˆ°æ–°é—»")
                
                # éšæœºå»¶è¿Ÿé¿å…è¢«å°
                time.sleep(random.uniform(0.5, 2.0))
                
            except Exception as e:
                self.logger.error(f"âŒ {crawler.__name__} æ‰§è¡Œå¤±è´¥: {e}")
        
        # å»é‡å’Œè¿‡æ»¤
        unique_news = self.deduplicate_news(all_news)
        
        # æŒ‰æ—¶é—´æ’åº
        try:
            unique_news.sort(key=lambda x: x.get('time', ''), reverse=True)
        except Exception as e:
            self.logger.warning(f"æ–°é—»æ’åºå¤±è´¥: {e}")
        
        if not unique_news:
            self.logger.error("âŒ æ‰€æœ‰æ–°é—»æºéƒ½æ— æ³•è·å–çœŸå®æ•°æ®")
            return []
        
        self.logger.info(f"ğŸ‰ å¢å¼ºç‰ˆæ”¶é›†å®Œæˆï¼Œè·å–åˆ° {len(unique_news)} æ¡çœŸå®è´¢ç»æ–°é—»")
        
        # æ˜¾ç¤ºæ¥æºç»Ÿè®¡
        sources = {}
        for news in unique_news:
            source = news['source']
            method = news.get('method', 'unknown')
            key = f"{source}({method})"
            sources[key] = sources.get(key, 0) + 1
        
        self.logger.info("ğŸ“Š æ–°é—»æ¥æºåˆ†å¸ƒ:")
        for source, count in sources.items():
            self.logger.info(f"   â€¢ {source}: {count} æ¡")
        
        return unique_news[:50]  # è¿”å›æœ€æ–°çš„50æ¡
    
    def send_markdown(self, content: str) -> bool:
        """å‘é€Markdownæ¶ˆæ¯åˆ°ä¼ä¸šå¾®ä¿¡"""
        try:
            data = {
                "msgtype": "markdown",
                "markdown": {
                    "content": content
                }
            }
            
            response = requests.post(
                self.webhook_url,
                json=data,
                headers={'Content-Type': 'application/json'},
                timeout=10
            )
            
            if response.status_code == 200:
                result = response.json()
                if result.get('errcode') == 0:
                    self.logger.info("æ–°é—»æŠ¥å‘Šå‘é€æˆåŠŸ")
                    return True
                else:
                    self.logger.error(f"å‘é€å¤±è´¥: {result}")
                    return False
            else:
                self.logger.error(f"HTTPé”™è¯¯: {response.status_code}")
                return False
                
        except Exception as e:
            self.logger.error(f"å‘é€æ¶ˆæ¯å¼‚å¸¸: {e}")
            return False
    
    def format_news_report(self, news_list: List[Dict], report_type: str) -> str:
        """æ ¼å¼åŒ–æ–°é—»æŠ¥å‘Š"""
        current_time = datetime.now().strftime('%Y-%m-%d %H:%M')
        
        if report_type == 'morning':
            title = f"ğŸ“° æ—©é—´è´¢ç»æ–°é—»æ±‡æ€» ({current_time})"
            intro = "ğŸŒ… éš”å¤œé‡è¦è´¢ç»æ–°é—»ï¼ŒåŠ©æ‚¨æŠŠæ¡å¸‚åœºè„‰æ"
        else:
            title = f"ğŸ“° æ™šé—´è´¢ç»æ–°é—»æ±‡æ€» ({current_time})"
            intro = "ğŸŒ™ ä»Šæ—¥é‡è¦å¸‚åœºåŠ¨æ€ï¼Œä¸ºæ‚¨æ¢³ç†æŠ•èµ„è¦ç‚¹"
        
        report = f"{title}\n\n{intro}\n\n"
        
        if not news_list:
            report += "ğŸ“­ æ‰€æœ‰æ–°é—»æºæš‚æ—¶æ— æ³•è·å–çœŸå®æ•°æ®\n"
            report += "ğŸ”„ ç³»ç»Ÿå°†åœ¨ä¸‹æ¬¡æ¨é€æ—¶é‡æ–°å°è¯•\n"
            report += "âš ï¸ ç»ä¸ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®ï¼Œç¡®ä¿ä¿¡æ¯çœŸå®æ€§\n"
            return report
        
        # æŒ‰æ¥æºåˆ†ç»„
        sources = {}
        for news in news_list:
            source = news['source']
            if source not in sources:
                sources[source] = []
            sources[source].append(news)
        
        # ä¸¥æ ¼é™åˆ¶æ€»æ•°ï¼Œé¿å…è¶…å‡ºä¼ä¸šå¾®ä¿¡4096å­—ç¬¦é™åˆ¶
        max_sources = min(5, len(sources))  # æœ€å¤š5ä¸ªæ¥æº
        top_sources = list(sources.items())[:max_sources]
        
        # ç”ŸæˆæŠ¥å‘Š
        for source, source_news in top_sources:
            report += f"## ğŸ“Š {source}\n"
            for i, news in enumerate(source_news[:3], 1):  # æ¯ä¸ªæ¥æºæœ€å¤š3æ¡
                title = news['title'][:30] + '...' if len(news['title']) > 30 else news['title']
                url = news.get('url', '')
                method_icon = "ğŸ¤–" if news.get('method') == 'selenium' else "ğŸŒ" if news.get('method') == 'api' else "ğŸ“„"
                
                # æ·»åŠ é“¾æ¥ - ä½¿ç”¨çŸ­URLæ ¼å¼
                if url:
                    # æå–URLçš„ä¸»è¦éƒ¨åˆ†ï¼Œç¼©çŸ­æ˜¾ç¤º
                    domain = urlparse(url).netloc
                    report += f"{i}. {method_icon} [{title}]({url}) [{domain}]\n"
                else:
                    report += f"{i}. {method_icon} {title}\n"
            report += "\n"
        
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        total_sources = len(sources)
        total_news = len(news_list)
        report += f"ğŸ“ˆ **æ•°æ®ç»Ÿè®¡**\n"
        report += f"â€¢ æ–°é—»æºæ•°é‡: {total_sources} ä¸ª\n"
        report += f"â€¢ æ–°é—»æ€»æ•°: {total_news} æ¡\n"
        report += f"â€¢ æ•°æ®è·å–: 100% çœŸå®æ•°æ®"
        
        return report

if __name__ == "__main__":
    # æµ‹è¯•ç”¨é…ç½®
    test_webhook = "https://qyapi.weixin.qq.com/cgi-bin/webhook/send?key=test"
    
    crawler = EnhancedNewsCrawler(test_webhook)
    
    print("ğŸ§ª æµ‹è¯•å¢å¼ºç‰ˆæ–°é—»çˆ¬è™«...")
    news = crawler.collect_all_news_enhanced()
    print(f"ğŸ“Š æ”¶é›†åˆ° {len(news)} æ¡æ–°é—»")
    
    # ç”ŸæˆæŠ¥å‘Š
    report = crawler.format_news_report(news, 'morning')
    print("ğŸ“‹ ç”Ÿæˆçš„æŠ¥å‘Š:")
    print(report)
