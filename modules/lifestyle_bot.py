"""
ç»¼åˆç”Ÿæ´»ç±»æœºå™¨äºº
åŠŸèƒ½: å¤©æ°”é¢„æŠ¥ã€ç©¿è¡£å»ºè®®ã€ç”µå½±æ¨èã€çƒ­æœç›‘æ§ã€èŠ‚å‡æ—¥æé†’ç­‰
"""

import requests
import json
import time
import random
import re
from datetime import datetime, timedelta
import schedule
import logging
from typing import Dict, List, Any
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import TimeoutException, NoSuchElementException

class LifestyleBot:
    def __init__(self, webhook_url: str):
        self.webhook_url = webhook_url
        self.setup_logging()
        
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('lifestyle_bot.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def translate_text(self, text: str, target_lang: str = 'zh') -> str:
        """ä½¿ç”¨å¤šç¿»è¯‘æºå¯¹æ¯”ï¼Œé€‰æ‹©æœ€ä½³ç¿»è¯‘ç»“æœ"""
        try:
            # è·å–å¤šä¸ªç¿»è¯‘æºçš„ç»“æœ
            translations = []
            
            # 1. Google Translate (å…è´¹æ¥å£)
            google_result = self.translate_with_google(text, target_lang)
            if google_result:
                translations.append(('Google', google_result))
            
            # 2. MyMemory API
            mymemory_result = self.translate_with_mymemory(text, target_lang)
            if mymemory_result:
                translations.append(('MyMemory', mymemory_result))
            
            # 3. é€‰æ‹©æœ€ä½³ç¿»è¯‘
            if translations:
                best_translation = self.select_best_translation(text, translations)
                return best_translation
                
        except Exception as e:
            self.logger.warning(f"å¤šæºç¿»è¯‘å¤±è´¥: {e}")
        
        return ""
    
    def translate_with_google(self, text: str, target_lang: str = 'zh') -> str:
        """ä½¿ç”¨Google Translateç¿»è¯‘"""
        try:
            import urllib.parse
            encoded_text = urllib.parse.quote(text)
            
            # Google Translateå…è´¹æ¥å£
            api_url = f"https://translate.googleapis.com/translate_a/single?client=gtx&sl=en&tl={target_lang}&dt=t&q={encoded_text}"
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            }
            
            response = requests.get(api_url, headers=headers, timeout=10)
            if response.status_code == 200:
                try:
                    import json
                    # Googleè¿”å›çš„æ˜¯ä¸€ä¸ªå¤æ‚çš„æ•°ç»„ç»“æ„
                    result = json.loads(response.text)
                    if result and len(result) > 0 and len(result[0]) > 0:
                        translated = result[0][0][0]  # ç¬¬ä¸€ä¸ªç¿»è¯‘ç»“æœ
                        if translated and translated != text:
                            return translated
                        
                except Exception as e:
                    self.logger.warning(f"è§£æGoogleç¿»è¯‘ç»“æœå¤±è´¥: {e}")
                    
        except Exception as e:
            self.logger.warning(f"Googleç¿»è¯‘APIå¤±è´¥: {e}")
        
        return ""
    
    def translate_with_mymemory(self, text: str, target_lang: str = 'zh') -> str:
        """ä½¿ç”¨MyMemory APIç¿»è¯‘"""
        try:
            import urllib.parse
            encoded_text = urllib.parse.quote(text)
            
            api_url = f"https://api.mymemory.translated.net/get?q={encoded_text}&langpair=en|{target_lang}"
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            }
            
            response = requests.get(api_url, headers=headers, timeout=10)
            if response.status_code == 200:
                try:
                    data = response.json()
                    if data.get('responseStatus') == 200:
                        translated = data.get('responseData', {}).get('translatedText', '')
                        if translated and translated != text:
                            return translated
                        
                except Exception as e:
                    self.logger.warning(f"è§£æMyMemoryç¿»è¯‘ç»“æœå¤±è´¥: {e}")
                    
        except Exception as e:
            self.logger.warning(f"MyMemoryç¿»è¯‘APIå¤±è´¥: {e}")
        
        return ""
    
    def select_best_translation(self, original_text: str, translations: list) -> str:
        """é€‰æ‹©æœ€ä½³ç¿»è¯‘ç»“æœ"""
        try:
            if not translations:
                return ""
            
            if len(translations) == 1:
                return translations[0][1]
            
            # ç¿»è¯‘è´¨é‡è¯„åˆ†è§„åˆ™
            scores = []
            
            for source, translation in translations:
                score = 0
                
                # 1. é•¿åº¦åˆç†æ€§ (ç¿»è¯‘ä¸åº”è¯¥è¿‡çŸ­æˆ–è¿‡é•¿)
                length_ratio = len(translation) / len(original_text)
                if 0.5 <= length_ratio <= 2.0:
                    score += 2
                elif 0.3 <= length_ratio <= 3.0:
                    score += 1
                
                # 2. åŒ…å«ä¸­æ–‡å­—ç¬¦ (ç¡®ä¿æ˜¯ä¸­æ–‡ç¿»è¯‘)
                chinese_chars = sum(1 for char in translation if '\u4e00' <= char <= '\u9fff')
                if chinese_chars > 0:
                    score += 3
                    # ä¸­æ–‡å­—ç¬¦æ¯”ä¾‹åˆç†
                    chinese_ratio = chinese_chars / len(translation)
                    if chinese_ratio > 0.3:
                        score += 2
                
                # 3. é¿å…æ˜æ˜¾çš„ç›´è¯‘ç—•è¿¹
                # æ£€æŸ¥æ˜¯å¦åŒ…å«è¿‡å¤šçš„è‹±æ–‡å•è¯æ®‹ç•™
                english_words = sum(1 for char in translation if char.isalpha() and ord(char) < 128)
                if english_words == 0:
                    score += 2
                elif english_words < len(translation) * 0.1:
                    score += 1
                
                # 4. è¯­ä¹‰å®Œæ•´æ€§æ£€æŸ¥
                # æ£€æŸ¥ç¿»è¯‘æ˜¯å¦åŒ…å«å…³é”®ä¿¡æ¯
                score += self.check_semantic_completeness(original_text, translation)
                
                # 5. è‡ªç„¶åº¦æ£€æŸ¥ (ä¸­æ–‡è¡¨è¾¾ä¹ æƒ¯)
                score += self.check_naturalness(translation)
                
                # 6. Googleç¿»è¯‘é€šå¸¸è´¨é‡æ›´å¥½ï¼Œç»™äºˆè½»å¾®åŠ åˆ†
                if source == 'Google':
                    score += 1
                
                # 7. æ£€æŸ¥æ ‡ç‚¹ç¬¦å·çš„åˆç†æ€§
                if 'ã€‚' in translation or 'ï¼Œ' in translation or 'ï¼Ÿ' in translation:
                    score += 1
                
                scores.append((score, source, translation))
                self.logger.info(f"ç¿»è¯‘è¯„åˆ† - {source}: {score}åˆ† - {translation}")
            
            # é€‰æ‹©å¾—åˆ†æœ€é«˜çš„ç¿»è¯‘
            scores.sort(key=lambda x: x[0], reverse=True)
            best_score, best_source, best_translation = scores[0]
            
            self.logger.info(f"é€‰æ‹©æœ€ä½³ç¿»è¯‘: {best_source} ({best_score}åˆ†) - {best_translation}")
            return best_translation
            
        except Exception as e:
            self.logger.warning(f"é€‰æ‹©æœ€ä½³ç¿»è¯‘å¤±è´¥: {e}")
            # å¦‚æœè¯„åˆ†å¤±è´¥ï¼Œè¿”å›ç¬¬ä¸€ä¸ªç¿»è¯‘
            return translations[0][1] if translations else ""
    
    def check_semantic_completeness(self, original: str, translation: str) -> int:
        """æ£€æŸ¥è¯­ä¹‰å®Œæ•´æ€§"""
        score = 0
        
        # æ£€æŸ¥æ•°å­—æ˜¯å¦ä¿æŒä¸€è‡´
        import re
        original_numbers = re.findall(r'\d+', original)
        translation_numbers = re.findall(r'\d+', translation)
        
        if len(original_numbers) == len(translation_numbers):
            if original_numbers == translation_numbers:
                score += 2  # æ•°å­—å®Œå…¨ä¸€è‡´
            else:
                score += 1  # æ•°å­—æ•°é‡ä¸€è‡´
        
        # æ£€æŸ¥é—®å·ç­‰å…³é”®æ ‡ç‚¹
        if '?' in original and ('ï¼Ÿ' in translation or 'å—' in translation):
            score += 1
        
        # æ£€æŸ¥å¦å®šè¯
        negative_words_en = ['not', "n't", 'no', 'never', 'none']
        negative_words_zh = ['ä¸', 'æ²¡', 'é', 'æ— ', 'æœª']
        
        has_negative_en = any(word in original.lower() for word in negative_words_en)
        has_negative_zh = any(word in translation for word in negative_words_zh)
        
        if has_negative_en == has_negative_zh:
            score += 1
        
        return score
    
    def check_naturalness(self, translation: str) -> int:
        """æ£€æŸ¥ä¸­æ–‡è¡¨è¾¾çš„è‡ªç„¶åº¦"""
        score = 0
        
        # æ£€æŸ¥å¸¸è§çš„è‡ªç„¶è¡¨è¾¾
        natural_patterns = [
            'å¯ä»¥', 'èƒ½å¤Ÿ', 'åº”è¯¥', 'éœ€è¦', 'å¿…é¡»',
            'ä¸€ä¸ª', 'ä¸€ç§', 'ä¸€äº›', 'è¿™ä¸ª', 'é‚£ä¸ª',
            'çš„è¯', 'çš„æ—¶å€™', 'å› ä¸º', 'æ‰€ä»¥', 'ä½†æ˜¯',
            'å¤§çº¦', 'å·®ä¸å¤š', 'å·¦å³', 'æˆ–è€…', 'è¿˜æ˜¯'
        ]
        
        natural_count = sum(1 for pattern in natural_patterns if pattern in translation)
        if natural_count >= 2:
            score += 2
        elif natural_count >= 1:
            score += 1
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ä¸è‡ªç„¶çš„ç›´è¯‘ç—•è¿¹
        unnatural_patterns = [
            'çš„çš„', 'åœ¨åœ¨', 'æ˜¯æ˜¯',  # é‡å¤è¯
            'ä¸€åŒ¹é©¬å¯ä»¥ç”¨ä¸€åªçœ¼ç›ä¸å¦ä¸€åªçœ¼ç›ä¸€èµ·',  # æ˜æ˜¾çš„ç›´è¯‘
        ]
        
        if any(pattern in translation for pattern in unnatural_patterns):
            score -= 2
        
        # æ£€æŸ¥è¯­åºæ˜¯å¦è‡ªç„¶
        if translation.count('çš„') > len(translation) * 0.15:  # è¿‡å¤šçš„"çš„"å­—
            score -= 1
        
        return score
    
    def setup_selenium_driver(self):
        """è®¾ç½®Seleniumæµè§ˆå™¨é©±åŠ¨"""
        try:
            chrome_options = Options()
            chrome_options.add_argument('--headless')  # æ— å¤´æ¨¡å¼
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--window-size=1920,1080')
            chrome_options.add_argument('--user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
            
            driver = webdriver.Chrome(options=chrome_options)
            driver.set_page_load_timeout(30)
            return driver
        except Exception as e:
            self.logger.warning(f"è®¾ç½®Seleniumé©±åŠ¨å¤±è´¥: {e}")
            return None

    def send_message(self, content: str):
        """å‘é€æ¶ˆæ¯åˆ°ä¼ä¸šå¾®ä¿¡"""
        try:
            data = {
                "msgtype": "text",
                "text": {
                    "content": content
                }
            }
            response = requests.post(self.webhook_url, json=data, timeout=10)
            if response.status_code == 200:
                self.logger.info("æ¶ˆæ¯å‘é€æˆåŠŸ")
            else:
                self.logger.error(f"æ¶ˆæ¯å‘é€å¤±è´¥: {response.status_code}")
        except Exception as e:
            self.logger.error(f"å‘é€æ¶ˆæ¯å¼‚å¸¸: {e}")

    def fetch_real_weather(self) -> Dict[str, Any]:
        """è·å–çœŸå®å¤©æ°”æ•°æ®"""
        # ä¼˜å…ˆä½¿ç”¨ä¸­å›½å¤©æ°”ç½‘APIï¼Œæ•°æ®æ›´å‡†ç¡®
        try:
            url = "http://t.weather.sojson.com/api/weather/city/101020100"  # ä¸Šæµ·åŸå¸‚ä»£ç 
            response = requests.get(url, timeout=10)
            if response.status_code == 200:
                data = response.json()
                if data['status'] == 200:
                    today_data = data['data']['forecast'][0]
                    
                    weather_data = {
                        "city": "ä¸Šæµ·",
                        "temperature": float(data['data']['wendu']),
                        "weather": today_data['type'],
                        "wind": f"{today_data['fx']}{today_data['fl']}",
                        "humidity": int(data['data']['shidu'].replace('%', '')),
                        "air_quality": data['data']['quality'],
                        "pm25": data['data'].get('pm25', 0),
                        "pm10": data['data'].get('pm10', 0),
                        "uv_index": 5,  # é»˜è®¤å€¼
                        "visibility": 15,  # é»˜è®¤å€¼
                        "feels_like": float(data['data']['wendu']),
                        "high_temp": int(today_data['high'].replace('é«˜æ¸© ', '').replace('â„ƒ', '')),
                        "low_temp": int(today_data['low'].replace('ä½æ¸© ', '').replace('â„ƒ', '')),
                        "sunrise": today_data.get('sunrise', '06:00'),
                        "sunset": today_data.get('sunset', '18:00'),
                        "notice": today_data.get('notice', '')
                    }
                    
                    # è·å–æœªæ¥3å¤©é¢„æŠ¥
                    forecast = []
                    for i, day_data in enumerate(data['data']['forecast'][:3]):
                        forecast.append({
                            "date": day_data['ymd'],
                            "week": day_data['week'],
                            "weather": day_data['type'],
                            "high_temp": int(day_data['high'].replace('é«˜æ¸© ', '').replace('â„ƒ', '')),
                            "low_temp": int(day_data['low'].replace('ä½æ¸© ', '').replace('â„ƒ', '')),
                            "wind": f"{day_data['fx']}{day_data['fl']}",
                            "notice": day_data.get('notice', '')
                        })
                    
                    weather_data['forecast'] = forecast
                    return weather_data
                    
        except Exception as e:
            self.logger.error(f"è·å–ä¸­å›½å¤©æ°”ç½‘æ•°æ®å¤±è´¥: {e}")
            
        # å¤‡ç”¨æ–¹æ¡ˆ: ä½¿ç”¨wttr.inæœåŠ¡
        try:
            url = "http://wttr.in/Shanghai?format=j1"
            response = requests.get(url, timeout=10)
            if response.status_code == 200:
                data = response.json()
                
                # è§£æwttr.inæ•°æ®æ ¼å¼
                current = data['current_condition'][0]
                today = data['weather'][0]
                
                # è½¬æ¢å¤©æ°”æè¿°ä¸ºä¸­æ–‡
                weather_desc_map = {
                    'Sunny': 'æ™´',
                    'Clear': 'æ™´',
                    'Partly cloudy': 'å¤šäº‘',
                    'Cloudy': 'é˜´',
                    'Overcast': 'é˜´',
                    'Light rain': 'å°é›¨',
                    'Moderate rain': 'ä¸­é›¨',
                    'Heavy rain': 'å¤§é›¨',
                    'Thundery outbreaks possible': 'å¯èƒ½æœ‰é›·é˜µé›¨',
                    'Patchy rain possible': 'å¯èƒ½æœ‰å°é›¨',
                    'Light drizzle': 'æ¯›æ¯›é›¨',
                    'Fog': 'é›¾',
                    'Mist': 'è–„é›¾'
                }
                
                weather_desc = current['weatherDesc'][0]['value']
                chinese_weather = weather_desc_map.get(weather_desc, weather_desc)
                
                # æ„å»ºå¤©æ°”æ•°æ®
                weather_data = {
                    "city": "ä¸Šæµ·",
                    "temperature": int(current['temp_C']),
                    "weather": chinese_weather,
                    "wind": f"{current['windspeedKmph']}km/h",
                    "humidity": int(current['humidity']),
                    "air_quality": "è‰¯",  # wttr.inä¸æä¾›ç©ºæ°”è´¨é‡ï¼Œä½¿ç”¨é»˜è®¤å€¼
                    "uv_index": int(current.get('uvIndex', 5)),
                    "visibility": int(current['visibility']),
                    "feels_like": int(current['FeelsLikeC']),
                    "high_temp": int(current['temp_C']) + 2,  # ä¼°ç®—
                    "low_temp": int(current['temp_C']) - 5,   # ä¼°ç®—
                    "sunrise": "06:00",
                    "sunset": "18:00",
                    "notice": ""
                }
                
                # å…¨å¤©é¢„æŠ¥
                hourly = today['hourly']
                forecast = []
                for i, hour_data in enumerate(hourly[::8]):  # æ¯8å°æ—¶å–ä¸€ä¸ªç‚¹
                    time_periods = ['ä¸Šåˆ', 'ä¸‹åˆ', 'æ™šä¸Š']
                    if i < len(time_periods):
                        weather_desc = hour_data['weatherDesc'][0]['value']
                        chinese_weather = weather_desc_map.get(weather_desc, weather_desc)
                        forecast.append({
                            "time": time_periods[i],
                            "weather": chinese_weather,
                            "temp": int(hour_data['tempC'])
                        })
                
                weather_data['forecast'] = forecast
                return weather_data
                
        except Exception as e:
            self.logger.error(f"è·å–wttr.inå¤©æ°”æ•°æ®å¤±è´¥: {e}")
        
        return {}

    def get_weather_info(self) -> str:
        """è·å–ä¸Šæµ·å¤©æ°”ä¿¡æ¯å’Œç©¿è¡£å»ºè®®"""
        try:
            # è·å–çœŸå®å¤©æ°”æ•°æ®
            weather_data = self.fetch_real_weather()
            
            if not weather_data:
                return "ğŸŒ¤ï¸ å¤©æ°”ä¿¡æ¯è·å–å¤±è´¥ï¼Œè¯·æ³¨æ„å…³æ³¨å¤©æ°”å˜åŒ–ï¼Œå»ºè®®å¸¦ä¼ä»¥é˜²ä¸‡ä¸€ï¼"
            
            # ç©¿è¡£å»ºè®®
            temp = int(weather_data["temperature"])
            if temp >= 28:
                clothing = "ğŸ‘• çŸ­è¢–ã€çŸ­è£¤ã€å‡‰é‹"
            elif temp >= 23:
                clothing = "ğŸ‘” é•¿è¢–è¡¬è¡«ã€è–„å¤–å¥—"
            elif temp >= 18:
                clothing = "ğŸ§¥ å¤–å¥—ã€é•¿è£¤ã€è¿åŠ¨é‹"
            elif temp >= 10:
                clothing = "ğŸ§¥ åšå¤–å¥—ã€æ¯›è¡£ã€é•¿è£¤"
            else:
                clothing = "ğŸ§¥ ç¾½ç»’æœã€åšæ¯›è¡£ã€ä¿æš–è£¤"
            
            # å¢å¼ºçš„å¼‚å¸¸å¤©æ°”æ£€æµ‹å’Œæé†’
            current_weather = weather_data["weather"]
            weather_warning = ""
            
            # æ£€æŸ¥å½“å‰å¤©æ°”
            if any(keyword in current_weather for keyword in ["é›·é˜µé›¨", "é›·é›¨", "æš´é›¨", "é›·æš´"]):
                weather_warning += "\nâš ï¸ ã€é‡è¦æé†’ã€‘ä»Šå¤©æœ‰é›·é›¨å¤©æ°”ï¼Œè¯·åŠ¡å¿…å¸¦ä¼ï¼é¿å…åœ¨ç©ºæ—·åœ°å¸¦æ´»åŠ¨ï¼"
            elif any(keyword in current_weather for keyword in ["å¤§é›¨", "ä¸­é›¨"]):
                weather_warning += "\nâ˜” ã€å‡ºè¡Œæé†’ã€‘ä»Šå¤©æœ‰é›¨ï¼Œè®°å¾—å¸¦ä¼ï¼Œæ³¨æ„è·¯é¢æ¹¿æ»‘ï¼"
            elif any(keyword in current_weather for keyword in ["å°é›¨", "æ¯›æ¯›é›¨"]):
                weather_warning += "\nğŸŒ§ï¸ ã€æ¸©é¦¨æç¤ºã€‘ä»Šå¤©æœ‰å°é›¨ï¼Œå»ºè®®å¸¦æŠŠä¼ä»¥é˜²ä¸‡ä¸€ï¼"
            elif any(keyword in current_weather for keyword in ["é›¾", "éœ¾", "è–„é›¾", "æµ“é›¾"]):
                weather_warning += "\nğŸ˜· ã€å¥åº·æé†’ã€‘ä»Šå¤©æœ‰é›¾éœ¾ï¼Œå»ºè®®ä½©æˆ´å£ç½©ï¼Œå‡å°‘æˆ·å¤–æ´»åŠ¨ï¼"
            
            # æ£€æŸ¥é£åŠ›
            wind_str = str(weather_data.get("wind", ""))
            if any(keyword in wind_str for keyword in ["å¤§é£", "å¼ºé£", "6çº§", "7çº§", "8çº§"]):
                weather_warning += "\nğŸ’¨ ã€å¤§é£æé†’ã€‘ä»Šå¤©é£åŠ›è¾ƒå¤§ï¼Œæ³¨æ„å®‰å…¨ï¼Œå°å¿ƒé«˜ç©ºå ç‰©ï¼"
            
            # æ£€æŸ¥æœªæ¥å‡ å¤©çš„å¼‚å¸¸å¤©æ°”
            forecast = weather_data.get('forecast', [])
            future_warnings = []
            for day in forecast[:3]:  # æ£€æŸ¥æœªæ¥3å¤©
                day_weather = day.get('weather', '')
                day_date = day.get('date', '')
                if any(keyword in day_weather for keyword in ["æš´é›¨", "é›·é›¨", "é›·é˜µé›¨"]):
                    future_warnings.append(f"âš ï¸ {day_date}æœ‰{day_weather}ï¼Œè¯·æå‰å‡†å¤‡ï¼")
                elif any(keyword in day_weather for keyword in ["å¤§é›¨", "ä¸­é›¨"]):
                    future_warnings.append(f"â˜” {day_date}æœ‰{day_weather}ï¼Œè®°å¾—å¸¦ä¼ï¼")
            
            if future_warnings:
                weather_warning += "\n\nğŸ“… æœªæ¥å¤©æ°”æé†’:"
                for warning in future_warnings:
                    weather_warning += f"\n{warning}"
            
            # ç©ºæ°”è´¨é‡æé†’
            air_quality = weather_data.get('air_quality', 'è‰¯')
            pm25 = weather_data.get('pm25', 0)
            if air_quality in ['è½»åº¦æ±¡æŸ“', 'ä¸­åº¦æ±¡æŸ“', 'é‡åº¦æ±¡æŸ“', 'ä¸¥é‡æ±¡æŸ“'] or pm25 > 75:
                weather_warning += f"\nğŸ˜· ã€ç©ºæ°”è´¨é‡æé†’ã€‘ä»Šæ—¥ç©ºæ°”è´¨é‡{air_quality}ï¼Œå»ºè®®å‡å°‘æˆ·å¤–æ´»åŠ¨ï¼"
            
            # ç´«å¤–çº¿æé†’
            uv_warning = ""
            uv_index = int(weather_data.get("uv_index", 5))
            if uv_index >= 8:
                uv_warning = "\nâ˜€ï¸ ã€é˜²æ™’æé†’ã€‘ç´«å¤–çº¿å¾ˆå¼ºï¼Œè¯·åšå¥½é˜²æ™’æªæ–½ï¼"
            elif uv_index >= 6:
                uv_warning = "\nğŸ•¶ï¸ ã€é˜²æ™’å»ºè®®ã€‘ç´«å¤–çº¿è¾ƒå¼ºï¼Œå»ºè®®æ¶‚æŠ¹é˜²æ™’éœœï¼"
            
            # æ¸©åº¦æé†’
            temp_warning = ""
            if temp >= 35:
                temp_warning = "\nğŸŒ¡ï¸ ã€é«˜æ¸©æé†’ã€‘ä»Šå¤©æ¸©åº¦å¾ˆé«˜ï¼Œæ³¨æ„é˜²æš‘é™æ¸©ï¼"
            elif temp <= 5:
                temp_warning = "\nâ„ï¸ ã€ä½æ¸©æé†’ã€‘ä»Šå¤©æ¸©åº¦è¾ƒä½ï¼Œæ³¨æ„ä¿æš–ï¼"
            
            # ç»„è£…å¤©æ°”é¢„æŠ¥
            weather_msg = f"""ğŸŒ¤ï¸ ä¸Šæµ·å¤©æ°”é¢„æŠ¥
ğŸ“ {weather_data['city']}
ğŸŒ¡ï¸ å½“å‰æ¸©åº¦: {weather_data['temperature']}Â°C
ğŸ“Š ä»Šæ—¥æ¸©åº¦: {weather_data.get('low_temp', 'N/A')}Â°C - {weather_data.get('high_temp', 'N/A')}Â°C
â˜ï¸ å¤©æ°”: {weather_data['weather']}
ğŸ’¨ é£åŠ›: {weather_data['wind']}
ğŸ’§ æ¹¿åº¦: {weather_data['humidity']}%
ğŸŒ¬ï¸ ç©ºæ°”è´¨é‡: {weather_data['air_quality']} (PM2.5: {pm25})
â˜€ï¸ ç´«å¤–çº¿æŒ‡æ•°: {uv_index}/10
ğŸ‘ï¸ èƒ½è§åº¦: {weather_data.get('visibility', 'N/A')}km
ğŸŒ… æ—¥å‡º: {weather_data.get('sunrise', 'N/A')} | ğŸŒ‡ æ—¥è½: {weather_data.get('sunset', 'N/A')}

ğŸ‘— ç©¿è¡£å»ºè®®: {clothing}"""
            
            # æ·»åŠ å®˜æ–¹æé†’
            if weather_data.get('notice'):
                weather_msg += f"\n\nğŸ“¢ å®˜æ–¹æé†’: {weather_data['notice']}"
            
            # æ·»åŠ æœªæ¥3å¤©é¢„æŠ¥
            if forecast:
                weather_msg += "\n\nğŸ“… æœªæ¥3å¤©é¢„æŠ¥:"
                for day in forecast[:3]:
                    weather_msg += f"\n{day.get('date', 'N/A')} {day.get('week', 'N/A')}: {day.get('weather', 'N/A')} {day.get('low_temp', 'N/A')}Â°C-{day.get('high_temp', 'N/A')}Â°C"
            
            # æ·»åŠ æ‰€æœ‰æé†’
            weather_msg += weather_warning + uv_warning + temp_warning
            
            return weather_msg
            
        except Exception as e:
            self.logger.error(f"è·å–å¤©æ°”ä¿¡æ¯å¤±è´¥: {e}")
            return "ğŸŒ¤ï¸ å¤©æ°”ä¿¡æ¯è·å–å¤±è´¥ï¼Œè¯·æ³¨æ„å…³æ³¨å¤©æ°”å˜åŒ–ï¼Œå»ºè®®å¸¦ä¼ä»¥é˜²ä¸‡ä¸€ï¼"

    def crawl_daily_taboo(self) -> str:
        """çˆ¬å–æ¯æ—¥ç¦å¿Œä¿¡æ¯"""
        try:
            # å°è¯•å¤šä¸ªé»„å†ç½‘ç«™
            taboo_sources = [
                self.crawl_huangli_net,
                self.crawl_laohuangli_com,
                self.crawl_wnl_com
            ]
            
            for source_func in taboo_sources:
                try:
                    result = source_func()
                    if result and len(result) > 10:  # ç¡®ä¿è·å–åˆ°æœ‰æ•ˆå†…å®¹
                        return result
                except Exception as e:
                    self.logger.warning(f"çˆ¬å–ç¦å¿Œå¤±è´¥: {e}")
                    continue
            
            # å¦‚æœæ‰€æœ‰çˆ¬è™«éƒ½å¤±è´¥ï¼Œå¦‚å®è¿”å›å¤±è´¥ä¿¡æ¯
            return "ğŸš« æ¯æ—¥ç¦å¿Œè·å–å¤±è´¥ - æ‰€æœ‰æ•°æ®æºéƒ½æ— æ³•è®¿é—®"
            
        except Exception as e:
            self.logger.error(f"è·å–æ¯æ—¥ç¦å¿Œå¤±è´¥: {e}")
            return f"ğŸš« æ¯æ—¥ç¦å¿Œè·å–å¤±è´¥ - ç³»ç»Ÿé”™è¯¯: {str(e)}"
    
    def crawl_huangli_net(self) -> str:
        """çˆ¬å–é»„å†ç½‘æ¯æ—¥ç¦å¿Œ"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                'Accept-Encoding': 'gzip, deflate, br',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1'
            }
            
            # å°è¯•å¤šä¸ªé»„å†ç½‘ç«™
            urls = [
                "https://www.huangli.com/",
                "https://www.laohuangli.com/",
                "https://www.wnl.com/",
                "https://www.rili.com.cn/",
                "https://www.51wnl.com/"
            ]
            
            for url in urls:
                try:
                    response = requests.get(url, headers=headers, timeout=15)
                    if response.status_code == 200:
                        soup = BeautifulSoup(response.content, 'html.parser')
                        
                        # æŸ¥æ‰¾ç¦å¿Œä¿¡æ¯ - å¤šç§é€‰æ‹©å™¨
                        taboo_selectors = [
                            {'tag': 'div', 'class': re.compile(r'.*å¿Œ.*|.*ä¸å®œ.*|.*ç¦å¿Œ.*')},
                            {'tag': 'span', 'class': re.compile(r'.*å¿Œ.*|.*ä¸å®œ.*|.*ç¦å¿Œ.*')},
                            {'tag': 'p', 'class': re.compile(r'.*å¿Œ.*|.*ä¸å®œ.*|.*ç¦å¿Œ.*')},
                            {'tag': 'li', 'class': re.compile(r'.*å¿Œ.*|.*ä¸å®œ.*|.*ç¦å¿Œ.*')},
                            {'tag': 'div', 'text': re.compile(r'.*å¿Œ.*|.*ä¸å®œ.*|.*ç¦å¿Œ.*')},
                            {'tag': 'span', 'text': re.compile(r'.*å¿Œ.*|.*ä¸å®œ.*|.*ç¦å¿Œ.*')}
                        ]
                        
                        taboos = []
                        for selector in taboo_selectors:
                            if 'class' in selector:
                                elements = soup.find_all(selector['tag'], class_=selector['class'])
                            else:
                                elements = soup.find_all(selector['tag'], text=selector['text'])
                            
                            for element in elements:
                                text = element.get_text().strip()
                                if text and len(text) > 3 and len(text) < 50:
                                    if any(keyword in text for keyword in ['å¿Œ', 'ä¸å®œ', 'é¿å…', 'ä¸è¦', 'ç¦å¿Œ']):
                                        taboos.append(text)
                        
                        if taboos:
                            # å»é‡å¹¶é€‰æ‹©å‰3æ¡
                            unique_taboos = list(dict.fromkeys(taboos))[:3]
                            taboo_msg = f"ğŸš« ä»Šæ—¥ç¦å¿Œ\n\n"
                            for i, taboo in enumerate(unique_taboos, 1):
                                taboo_msg += f"{i}. {taboo}\n"
                            return taboo_msg
            
                except Exception as e:
                    self.logger.warning(f"çˆ¬å–{url}å¤±è´¥: {e}")
                    continue
                    
        except Exception as e:
            self.logger.warning(f"çˆ¬å–é»„å†ç½‘å¤±è´¥: {e}")
        
        return ""
    
    def crawl_laohuangli_com(self) -> str:
        """çˆ¬å–è€é»„å†ç½‘æ¯æ—¥ç¦å¿Œ"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8'
            }
            
            url = "https://www.laohuangli.com/"
            response = requests.get(url, headers=headers, timeout=10)
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # æŸ¥æ‰¾ç¦å¿Œä¿¡æ¯
                taboo_elements = soup.find_all(['div', 'span', 'li'], text=re.compile(r'.*å¿Œ.*|.*ä¸å®œ.*'))
                
                taboos = []
                for element in taboo_elements:
                    text = element.get_text().strip()
                    if text and len(text) > 3 and len(text) < 50:
                        taboos.append(text)
                
                if taboos:
                    selected_taboos = taboos[:3]
                    taboo_msg = f"ğŸš« ä»Šæ—¥ç¦å¿Œ\n\n"
                    for i, taboo in enumerate(selected_taboos, 1):
                        taboo_msg += f"{i}. {taboo}\n"
                    taboo_msg += "\n"
                    return taboo_msg
                    
        except Exception as e:
            self.logger.warning(f"çˆ¬å–è€é»„å†ç½‘å¤±è´¥: {e}")
        
        return ""
    
    def crawl_wnl_com(self) -> str:
        """çˆ¬å–ä¸‡å¹´å†ç½‘æ¯æ—¥ç¦å¿Œ"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8'
            }
            
            url = "https://www.wnl.com/"
            response = requests.get(url, headers=headers, timeout=10)
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # æŸ¥æ‰¾ç¦å¿Œä¿¡æ¯
                taboo_elements = soup.find_all(['div', 'span', 'p'], text=re.compile(r'.*å¿Œ.*|.*ä¸å®œ.*'))
                
                taboos = []
                for element in taboo_elements:
                    text = element.get_text().strip()
                    if text and len(text) > 3 and len(text) < 50:
                        taboos.append(text)
                
                if taboos:
                    selected_taboos = taboos[:3]
                    taboo_msg = f"ğŸš« ä»Šæ—¥ç¦å¿Œ\n\n"
                    for i, taboo in enumerate(selected_taboos, 1):
                        taboo_msg += f"{i}. {taboo}\n"
                    taboo_msg += "\n"
                    return taboo_msg
                    
        except Exception as e:
            self.logger.warning(f"çˆ¬å–ä¸‡å¹´å†ç½‘å¤±è´¥: {e}")
        
        return ""
    
    
    def get_daily_taboo(self) -> str:
        """è·å–æ¯æ—¥ç¦å¿Œï¼ˆä¸»å…¥å£ï¼‰"""
        return self.crawl_daily_taboo()
    
    def crawl_tarot_reading(self) -> str:
        """çˆ¬å–å¡”ç½—ç‰Œå åœä¿¡æ¯"""
        try:
            # é¦–å…ˆå°è¯•ç®€å•çš„APIæ–¹æ³•
            simple_sources = [
                self.crawl_simple_tarot_api
            ]
            
            for source_func in simple_sources:
                try:
                    result = source_func()
                    if result and len(result) > 20:
                        return result
                except Exception as e:
                    self.logger.warning(f"çˆ¬å–ç®€å•å¡”ç½—ç‰Œå¤±è´¥: {e}")
                    continue
            
            # ç„¶åå°è¯•Seleniumæ–¹æ³•
            tarot_sources = [
                self.crawl_tarot_com,
                self.crawl_tarot_online,
                self.crawl_daily_tarot
            ]
            
            for source_func in tarot_sources:
                try:
                    result = source_func()
                    if result and len(result) > 20:  # ç¡®ä¿è·å–åˆ°æœ‰æ•ˆå†…å®¹
                        return result
                except Exception as e:
                    self.logger.warning(f"çˆ¬å–å¡”ç½—ç‰Œå¤±è´¥: {e}")
                    continue
            
            # å¦‚æœæ‰€æœ‰çˆ¬è™«éƒ½å¤±è´¥ï¼Œå¦‚å®è¿”å›å¤±è´¥ä¿¡æ¯
            return "ğŸ”® å¡”ç½—ç‰Œå åœè·å–å¤±è´¥ - æ‰€æœ‰æ•°æ®æºéƒ½æ— æ³•è®¿é—®"
            
        except Exception as e:
            self.logger.error(f"è·å–å¡”ç½—ç‰Œå åœå¤±è´¥: {e}")
            return f"ğŸ”® å¡”ç½—ç‰Œå åœè·å–å¤±è´¥ - ç³»ç»Ÿé”™è¯¯: {str(e)}"
    
    def crawl_simple_tarot_api(self) -> str:
        """ä½¿ç”¨GitHubå¡”ç½—ç‰Œæ•°æ®API"""
        try:
            # ä½¿ç”¨GitHubä¸Šçš„çœŸå®å¡”ç½—ç‰Œæ•°æ®
            api_url = "https://raw.githubusercontent.com/MinatoAquaCrews/nonebot_plugin_tarot/main/nonebot_plugin_tarot/tarot.json"
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            }
            
            response = requests.get(api_url, headers=headers, timeout=15)
            if response.status_code == 200:
                try:
                    tarot_data = response.json()
                    cards = tarot_data.get("cards", {})
                    
                    if cards:
                        # éšæœºé€‰æ‹©ä¸€å¼ å¡”ç½—ç‰Œ
                        import random
                        card_keys = list(cards.keys())
                        selected_key = random.choice(card_keys)
                        selected_card = cards[selected_key]
                        
                        # è·å–å¡ç‰Œä¿¡æ¯
                        name_cn = selected_card.get("name_cn", "æœªçŸ¥")
                        name_en = selected_card.get("name_en", "Unknown")
                        meaning = selected_card.get("meaning", {})
                        
                        # éšæœºé€‰æ‹©æ­£ä½æˆ–é€†ä½
                        is_upright = random.choice([True, False])
                        position = "æ­£ä½" if is_upright else "é€†ä½"
                        card_meaning = meaning.get("up" if is_upright else "down", "")
                        
                        # æ ¼å¼åŒ–è¾“å‡º
                        result = f"ğŸ”® ä»Šæ—¥å¡”ç½—ç‰Œ\n\n"
                        result += f"ğŸƒ {name_cn} ({name_en})\n"
                        result += f"ğŸ”„ {position}\n\n"
                        result += f"ğŸ’« {card_meaning}"
                        
                        return result
                        
                except Exception as e:
                    self.logger.warning(f"è§£æå¡”ç½—ç‰ŒJSONå¤±è´¥: {e}")
                    
        except Exception as e:
            self.logger.warning(f"GitHubå¡”ç½—ç‰ŒAPIå¤±è´¥: {e}")
        
        return ""
    
    
    def crawl_tarot_com(self) -> str:
        """çˆ¬å–å¡”ç½—ç‰Œç½‘ç«™ - ä½¿ç”¨Seleniumæ¨¡æ‹Ÿç‚¹å‡»"""
        driver = None
        try:
            driver = self.setup_selenium_driver()
            if not driver:
                return ""
            
            # å°è¯•è®¿é—®å¡”ç½—ç‰Œç½‘ç«™
            tarot_sources = [
                self.crawl_tarot_com_selenium,
                self.crawl_astro_com_selenium,
                self.crawl_horoscope_com_selenium
            ]
            
            for source_func in tarot_sources:
                try:
                    result = source_func(driver)
                    if result and len(result) > 20:
                        return result
                except Exception as e:
                    self.logger.warning(f"çˆ¬å–å¡”ç½—ç‰Œå¤±è´¥: {e}")
                    continue
                    
        except Exception as e:
            self.logger.warning(f"çˆ¬å–å¡”ç½—ç‰Œç½‘ç«™å¤±è´¥: {e}")
        finally:
            if driver:
                driver.quit()
        
        return ""
    
    def crawl_tarot_com_selenium(self, driver) -> str:
        """ä½¿ç”¨Seleniumçˆ¬å–tarot.com - æ”¹ç”¨ä¸€å¡å åœ"""
        try:
            driver.get("https://www.tarot.com/tarot/one-card")
            time.sleep(5)
            
            # ç­‰å¾…é¡µé¢åŠ è½½
            wait = WebDriverWait(driver, 15)
            
            # å°è¯•ç‚¹å‡»æŠ½ç‰ŒæŒ‰é’®
            click_selectors = [
                "a[href*='card']",
                ".btn",
                ".button",
                "button",
                "[onclick*='card']",
                "a:contains('Draw')",
                "a:contains('Pick')",
                "a:contains('Select')",
                "a:contains('One Card')"
            ]
            
            for selector in click_selectors:
                try:
                    if ":contains" in selector:
                        # ä½¿ç”¨XPathæŸ¥æ‰¾åŒ…å«æ–‡æœ¬çš„æŒ‰é’®
                        text = selector.split("'")[1]
                        button = driver.find_element(By.XPATH, f"//a[contains(text(), '{text}')]")
                    else:
                        button = driver.find_element(By.CSS_SELECTOR, selector)
                    
                    if button.is_displayed() and button.is_enabled():
                        driver.execute_script("arguments[0].click();", button)
                        time.sleep(3)
                        break
                except:
                    continue
            
            # ç­‰å¾…ç»“æœåŠ è½½
            time.sleep(5)
            
            # æŸ¥æ‰¾ç»“æœå†…å®¹
            result_selectors = [
                ".tarot-reading",
                ".card-reading",
                ".daily-reading",
                ".reading-result",
                ".card-result",
                ".tarot-result",
                "[class*='reading']",
                "[class*='result']",
                "[class*='card']",
                "[class*='tarot']",
                "p",
                "div"
            ]
            
            for selector in result_selectors:
                try:
                    elements = driver.find_elements(By.CSS_SELECTOR, selector)
                    for element in elements:
                        text = element.text.strip()
                        if text and len(text) > 30 and len(text) < 800:
                            if any(keyword in text.lower() for keyword in ['tarot', 'card', 'reading', 'fortune', 'spiritual', 'guidance']):
                                # æ¸…ç†æ–‡æœ¬
                                if not any(skip_word in text.lower() for skip_word in ['cookie', 'privacy', 'terms', 'contact', 'sign up', 'login']):
                                    return f"ğŸ”® ä»Šæ—¥å¡”ç½—ç‰Œå åœ\n\n{text}\n\n"
                except:
                    continue
                    
        except Exception as e:
            self.logger.warning(f"çˆ¬å–tarot.comå¤±è´¥: {e}")
        
        return ""
    
    def crawl_astro_com_selenium(self, driver) -> str:
        """ä½¿ç”¨Seleniumçˆ¬å–astro.com"""
        try:
            driver.get("https://www.astro.com/daily-horoscope")
            time.sleep(3)
            
            # ç­‰å¾…é¡µé¢åŠ è½½
            wait = WebDriverWait(driver, 10)
            
            # æŸ¥æ‰¾å æ˜Ÿå†…å®¹
            result_selectors = [
                ".horoscope",
                ".daily-horoscope",
                ".astrology",
                ".zodiac",
                "[class*='horoscope']",
                "[class*='astrology']"
            ]
            
            for selector in result_selectors:
                try:
                    elements = driver.find_elements(By.CSS_SELECTOR, selector)
                    for element in elements:
                        text = element.text.strip()
                        if text and len(text) > 20 and len(text) < 500:
                            if any(keyword in text.lower() for keyword in ['horoscope', 'astrology', 'zodiac', 'fortune']):
                                return f"ğŸ”® ä»Šæ—¥å¡”ç½—ç‰Œå åœ\n\n{text}\n\n"
                except:
                    continue
                    
        except Exception as e:
            self.logger.warning(f"çˆ¬å–astro.comå¤±è´¥: {e}")
        
        return ""
    
    def crawl_horoscope_com_selenium(self, driver) -> str:
        """ä½¿ç”¨Seleniumçˆ¬å–horoscope.com"""
        try:
            driver.get("https://www.horoscope.com/daily-horoscope")
            time.sleep(3)
            
            # ç­‰å¾…é¡µé¢åŠ è½½
            wait = WebDriverWait(driver, 10)
            
            # æŸ¥æ‰¾å æ˜Ÿå†…å®¹
            result_selectors = [
                ".horoscope",
                ".daily-horoscope",
                ".astrology",
                ".zodiac",
                "[class*='horoscope']",
                "[class*='astrology']"
            ]
            
            for selector in result_selectors:
                try:
                    elements = driver.find_elements(By.CSS_SELECTOR, selector)
                    for element in elements:
                        text = element.text.strip()
                        if text and len(text) > 20 and len(text) < 500:
                            if any(keyword in text.lower() for keyword in ['horoscope', 'astrology', 'zodiac', 'fortune']):
                                return f"ğŸ”® ä»Šæ—¥å¡”ç½—ç‰Œå åœ\n\n{text}\n\n"
                except:
                    continue
                    
        except Exception as e:
            self.logger.warning(f"çˆ¬å–horoscope.comå¤±è´¥: {e}")
        
        return ""
    
    def crawl_tarot_online(self) -> str:
        """çˆ¬å–åœ¨çº¿å¡”ç½—ç‰Œç½‘ç«™"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8'
            }
            
            # å°è¯•è®¿é—®å…¶ä»–å¡”ç½—ç‰Œç½‘ç«™
            urls = [
                "https://www.tarot-online.com/daily",
                "https://www.free-tarot-reading.net/daily",
                "https://www.tarotreading.com/daily"
            ]
            
            for url in urls:
                try:
                    response = requests.get(url, headers=headers, timeout=10)
                    if response.status_code == 200:
                        soup = BeautifulSoup(response.content, 'html.parser')
                        
                        # æŸ¥æ‰¾å¡”ç½—ç‰Œä¿¡æ¯
                        card_elements = soup.find_all(['div', 'span', 'p'], text=re.compile(r'.*card.*|.*tarot.*'))
                        
                        for element in card_elements:
                            text = element.get_text().strip()
                            if text and len(text) > 20 and len(text) < 200:
                                return f"ğŸ”® ä»Šæ—¥å¡”ç½—ç‰Œå åœ\n\n{text}\n\n"
                except:
                    continue
                    
        except Exception as e:
            self.logger.warning(f"çˆ¬å–åœ¨çº¿å¡”ç½—ç‰Œå¤±è´¥: {e}")
        
        return ""
    
    def crawl_daily_tarot(self) -> str:
        """çˆ¬å–æ¯æ—¥å¡”ç½—ç‰Œ"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8'
            }
            
            # å°è¯•è®¿é—®æ¯æ—¥å¡”ç½—ç‰Œç½‘ç«™
            urls = [
                "https://www.dailytarot.com",
                "https://www.tarotdaily.com",
                "https://www.free-tarot.com/daily"
            ]
            
            for url in urls:
                try:
                    response = requests.get(url, headers=headers, timeout=10)
                    if response.status_code == 200:
                        soup = BeautifulSoup(response.content, 'html.parser')
                        
                        # æŸ¥æ‰¾å¡”ç½—ç‰Œä¿¡æ¯
                        card_elements = soup.find_all(['div', 'span', 'p'], text=re.compile(r'.*card.*|.*tarot.*'))
                        
                        for element in card_elements:
                            text = element.get_text().strip()
                            if text and len(text) > 20 and len(text) < 200:
                                return f"ğŸ”® ä»Šæ—¥å¡”ç½—ç‰Œå åœ\n\n{text}\n\n"
                except:
                    continue
                    
        except Exception as e:
            self.logger.warning(f"çˆ¬å–æ¯æ—¥å¡”ç½—ç‰Œå¤±è´¥: {e}")
        
        return ""
    
    
    def get_tarot_card(self) -> str:
        """è·å–å¡”ç½—ç‰Œå åœï¼ˆä¸»å…¥å£ï¼‰"""
        return self.crawl_tarot_reading()
    
    def crawl_daily_fortune(self) -> str:
        """çˆ¬å–æ¯æ—¥è¿åŠ¿ä¿¡æ¯"""
        try:
            # é¦–å…ˆå°è¯•å…è´¹çš„æ˜Ÿåº§è¿åŠ¿API
            api_sources = [
                self.crawl_horoscope_api,
                self.crawl_aztro_api
            ]
            
            for source_func in api_sources:
                try:
                    result = source_func()
                    if result and len(result) > 20:
                        return result
                except Exception as e:
                    self.logger.warning(f"çˆ¬å–è¿åŠ¿APIå¤±è´¥: {e}")
                    continue
            
            # ç„¶åå°è¯•Seleniumæ–¹æ³•
            fortune_sources = [
                self.crawl_xingzuo_com,
                self.crawl_astro_com,
                self.crawl_fortune_net
            ]
            
            for source_func in fortune_sources:
                try:
                    result = source_func()
                    if result and len(result) > 20:  # ç¡®ä¿è·å–åˆ°æœ‰æ•ˆå†…å®¹
                        return result
                except Exception as e:
                    self.logger.warning(f"çˆ¬å–è¿åŠ¿å¤±è´¥: {e}")
                    continue
            
            # å¦‚æœæ‰€æœ‰çˆ¬è™«éƒ½å¤±è´¥ï¼Œå¦‚å®è¿”å›å¤±è´¥ä¿¡æ¯
            return "ğŸŒŸ æ¯æ—¥è¿åŠ¿è·å–å¤±è´¥ - æ‰€æœ‰æ•°æ®æºéƒ½æ— æ³•è®¿é—®"
            
        except Exception as e:
            self.logger.error(f"è·å–æ¯æ—¥è¿åŠ¿å¤±è´¥: {e}")
            return f"ğŸŒŸ æ¯æ—¥è¿åŠ¿è·å–å¤±è´¥ - ç³»ç»Ÿé”™è¯¯: {str(e)}"
    
    def crawl_horoscope_api(self) -> str:
        """ä½¿ç”¨å…è´¹çš„æ˜Ÿåº§è¿åŠ¿API"""
        try:
            import random
            
            # 12ä¸ªæ˜Ÿåº§
            zodiac_signs = ['aries', 'taurus', 'gemini', 'cancer', 'leo', 'virgo', 
                           'libra', 'scorpio', 'sagittarius', 'capricorn', 'aquarius', 'pisces']
            
            # æ˜Ÿåº§ä¸­æ–‡åç§°
            zodiac_names = {
                'aries': 'ç™½ç¾Šåº§', 'taurus': 'é‡‘ç‰›åº§', 'gemini': 'åŒå­åº§', 
                'cancer': 'å·¨èŸ¹åº§', 'leo': 'ç‹®å­åº§', 'virgo': 'å¤„å¥³åº§',
                'libra': 'å¤©ç§¤åº§', 'scorpio': 'å¤©èåº§', 'sagittarius': 'å°„æ‰‹åº§',
                'capricorn': 'æ‘©ç¾¯åº§', 'aquarius': 'æ°´ç“¶åº§', 'pisces': 'åŒé±¼åº§'
            }
            
            # éšæœºé€‰æ‹©ä¸€ä¸ªæ˜Ÿåº§
            selected_sign = random.choice(zodiac_signs)
            sign_name = zodiac_names.get(selected_sign, selected_sign)
            
            # è°ƒç”¨API
            api_url = f"https://horoscope-app-api.vercel.app/api/v1/get-horoscope/daily?sign={selected_sign}&day=today"
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            }
            
            response = requests.get(api_url, headers=headers, timeout=10)
            if response.status_code == 200:
                try:
                    data = response.json()
                    if data.get('success') and data.get('data'):
                        horoscope_data = data['data']
                        date = horoscope_data.get('date', 'ä»Šæ—¥')
                        horoscope_text = horoscope_data.get('horoscope_data', '')
                        
                        if horoscope_text:
                            # å°è¯•ç¿»è¯‘æˆä¸­æ–‡
                            chinese_text = self.translate_text(horoscope_text)
                            
                            result = f"ğŸŒŸ ä»Šæ—¥è¿åŠ¿\n\n"
                            result += f"â™ˆ {sign_name} ({selected_sign.title()})\n"
                            result += f"ğŸ“… {date}\n\n"
                            result += f"ğŸ‡¬ğŸ‡§ {horoscope_text}\n\n"
                            if chinese_text:
                                result += f"ğŸ‡¨ğŸ‡³ {chinese_text}"
                            else:
                                result += f"ğŸ‡¨ğŸ‡³ ç¿»è¯‘å¤±è´¥ï¼Œä»…æ˜¾ç¤ºè‹±æ–‡åŸæ–‡"
                            
                            return result
                            
                except Exception as e:
                    self.logger.warning(f"è§£ææ˜Ÿåº§è¿åŠ¿APIå¤±è´¥: {e}")
                    
        except Exception as e:
            self.logger.warning(f"æ˜Ÿåº§è¿åŠ¿APIå¤±è´¥: {e}")
        
        return ""
    
    def crawl_aztro_api(self) -> str:
        """ä½¿ç”¨Aztroæ˜Ÿåº§è¿åŠ¿API"""
        try:
            import random
            
            # 12ä¸ªæ˜Ÿåº§
            zodiac_signs = ['aries', 'taurus', 'gemini', 'cancer', 'leo', 'virgo', 
                           'libra', 'scorpio', 'sagittarius', 'capricorn', 'aquarius', 'pisces']
            
            # æ˜Ÿåº§ä¸­æ–‡åç§°
            zodiac_names = {
                'aries': 'ç™½ç¾Šåº§', 'taurus': 'é‡‘ç‰›åº§', 'gemini': 'åŒå­åº§', 
                'cancer': 'å·¨èŸ¹åº§', 'leo': 'ç‹®å­åº§', 'virgo': 'å¤„å¥³åº§',
                'libra': 'å¤©ç§¤åº§', 'scorpio': 'å¤©èåº§', 'sagittarius': 'å°„æ‰‹åº§',
                'capricorn': 'æ‘©ç¾¯åº§', 'aquarius': 'æ°´ç“¶åº§', 'pisces': 'åŒé±¼åº§'
            }
            
            # éšæœºé€‰æ‹©ä¸€ä¸ªæ˜Ÿåº§
            selected_sign = random.choice(zodiac_signs)
            sign_name = zodiac_names.get(selected_sign, selected_sign)
            
            # è°ƒç”¨Aztro API (POSTæ–¹æ³•)
            api_url = f"https://aztro.sameerkumar.website/?sign={selected_sign}&day=today"
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            }
            
            response = requests.post(api_url, headers=headers, timeout=10)
            if response.status_code == 200:
                try:
                    data = response.json()
                    
                    description = data.get('description', '')
                    date_range = data.get('date_range', '')
                    current_date = data.get('current_date', 'ä»Šæ—¥')
                    compatibility = data.get('compatibility', '')
                    mood = data.get('mood', '')
                    color = data.get('color', '')
                    lucky_number = data.get('lucky_number', '')
                    
                    if description:
                        # å°è¯•ç¿»è¯‘æˆä¸­æ–‡
                        chinese_description = self.translate_text(description)
                        chinese_mood = self.translate_text(mood) if mood else ""
                        chinese_color = self.translate_text(color) if color else ""
                        chinese_compatibility = self.translate_text(compatibility) if compatibility else ""
                        
                        result = f"ğŸŒŸ ä»Šæ—¥è¿åŠ¿\n\n"
                        result += f"â™ˆ {sign_name} ({selected_sign.title()})\n"
                        result += f"ğŸ“… {current_date} ({date_range})\n\n"
                        result += f"ğŸ‡¬ğŸ‡§ {description}\n\n"
                        if chinese_description:
                            result += f"ğŸ‡¨ğŸ‡³ {chinese_description}\n\n"
                        else:
                            result += f"ğŸ‡¨ğŸ‡³ ç¿»è¯‘å¤±è´¥ï¼Œä»…æ˜¾ç¤ºè‹±æ–‡åŸæ–‡\n\n"
                        
                        if mood:
                            result += f"ğŸ‡¬ğŸ‡§ ğŸ˜Š ä»Šæ—¥å¿ƒæƒ…: {mood}\n"
                            if chinese_mood:
                                result += f"ğŸ‡¨ğŸ‡³ ğŸ˜Š ä»Šæ—¥å¿ƒæƒ…: {chinese_mood}\n"
                        if color:
                            result += f"ğŸ‡¬ğŸ‡§ ğŸŒˆ å¹¸è¿è‰²å½©: {color}\n"
                            if chinese_color:
                                result += f"ğŸ‡¨ğŸ‡³ ğŸŒˆ å¹¸è¿è‰²å½©: {chinese_color}\n"
                        if lucky_number:
                            result += f"ğŸ€ å¹¸è¿æ•°å­—: {lucky_number}\n"
                        if compatibility:
                            result += f"ğŸ‡¬ğŸ‡§ ğŸ’• æ˜Ÿåº§é…å¯¹: {compatibility}\n"
                            if chinese_compatibility:
                                result += f"ğŸ‡¨ğŸ‡³ ğŸ’• æ˜Ÿåº§é…å¯¹: {chinese_compatibility}"
                        
                        return result
                        
                except Exception as e:
                    self.logger.warning(f"è§£æAztro APIå¤±è´¥: {e}")
                    
        except Exception as e:
            self.logger.warning(f"Aztro APIå¤±è´¥: {e}")
        
        return ""
    
    def crawl_xingzuo_com(self) -> str:
        """çˆ¬å–æ˜Ÿåº§è¿åŠ¿ç½‘ç«™ - ä½¿ç”¨Seleniumæ¨¡æ‹Ÿç‚¹å‡»"""
        driver = None
        try:
            driver = self.setup_selenium_driver()
            if not driver:
                return ""
            
            # å°è¯•è®¿é—®æ˜Ÿåº§è¿åŠ¿ç½‘ç«™
            fortune_sources = [
                self.crawl_xingzuo_com_selenium,
                self.crawl_astro_fortune_selenium,
                self.crawl_horoscope_fortune_selenium
            ]
            
            for source_func in fortune_sources:
                try:
                    result = source_func(driver)
                    if result and len(result) > 20:
                        return result
                except Exception as e:
                    self.logger.warning(f"çˆ¬å–è¿åŠ¿å¤±è´¥: {e}")
                    continue
                    
        except Exception as e:
            self.logger.warning(f"çˆ¬å–æ˜Ÿåº§è¿åŠ¿å¤±è´¥: {e}")
        finally:
            if driver:
                driver.quit()
        
        return ""
    
    def crawl_xingzuo_com_selenium(self, driver) -> str:
        """ä½¿ç”¨Seleniumçˆ¬å–æ˜Ÿåº§è¿åŠ¿ - æ”¹ç”¨horoscope.com"""
        try:
            driver.get("https://www.horoscope.com/us/index.aspx")
            time.sleep(5)
            
            # ç­‰å¾…é¡µé¢åŠ è½½
            wait = WebDriverWait(driver, 15)
            
            # éšæœºé€‰æ‹©ä¸€ä¸ªæ˜Ÿåº§
            import random
            zodiac_signs = ['aries', 'taurus', 'gemini', 'cancer', 'leo', 'virgo', 
                           'libra', 'scorpio', 'sagittarius', 'capricorn', 'aquarius', 'pisces']
            selected_sign = random.choice(zodiac_signs)
            
            # å°è¯•ç‚¹å‡»æ˜Ÿåº§
            click_selectors = [
                f"a[href*='{selected_sign}']",
                f"[class*='{selected_sign}']",
                f"#{selected_sign}",
                ".zodiac-sign",
                ".horoscope-sign"
            ]
            
            for selector in click_selectors:
                try:
                    elements = driver.find_elements(By.CSS_SELECTOR, selector)
                    for element in elements:
                        if element.is_displayed() and element.is_enabled():
                            driver.execute_script("arguments[0].click();", element)
                            time.sleep(3)
                            break
                    if elements:
                        break
                except:
                    continue
            
            # ç­‰å¾…ç»“æœåŠ è½½
            time.sleep(5)
            
            # æŸ¥æ‰¾è¿åŠ¿å†…å®¹
            result_selectors = [
                "[class*='horoscope']",
                "[class*='fortune']",
                "[class*='zodiac']",
                "[class*='astrology']",
                "[class*='daily']",
                ".content",
                ".reading",
                "p",
                "div"
            ]
            
            for selector in result_selectors:
                try:
                    elements = driver.find_elements(By.CSS_SELECTOR, selector)
                    for element in elements:
                        text = element.text.strip()
                        if text and len(text) > 50 and len(text) < 800:
                            if any(keyword in text.lower() for keyword in ['today', 'fortune', 'horoscope', 'zodiac', 'astrology', 'energy', 'stars']):
                                # æ¸…ç†æ–‡æœ¬
                                if not any(skip_word in text.lower() for skip_word in ['cookie', 'privacy', 'terms', 'contact', 'sign up', 'login', 'advertisement']):
                                    return f"ğŸŒŸ ä»Šæ—¥è¿åŠ¿å åœ\n\n{text}\n\n"
                except:
                    continue
                    
        except Exception as e:
            self.logger.warning(f"çˆ¬å–æ˜Ÿåº§è¿åŠ¿å¤±è´¥: {e}")
        
        return ""
    
    def crawl_astro_fortune_selenium(self, driver) -> str:
        """ä½¿ç”¨Seleniumçˆ¬å–å æ˜Ÿè¿åŠ¿ - æ”¹ç”¨astrology.com"""
        try:
            driver.get("https://www.astrology.com/horoscope/daily.html")
            time.sleep(5)
            
            # ç­‰å¾…é¡µé¢åŠ è½½
            wait = WebDriverWait(driver, 15)
            
            # éšæœºé€‰æ‹©ä¸€ä¸ªæ˜Ÿåº§
            import random
            zodiac_signs = ['aries', 'taurus', 'gemini', 'cancer', 'leo', 'virgo', 
                           'libra', 'scorpio', 'sagittarius', 'capricorn', 'aquarius', 'pisces']
            selected_sign = random.choice(zodiac_signs)
            
            # å°è¯•ç‚¹å‡»æ˜Ÿåº§
            click_selectors = [
                f"a[href*='{selected_sign}']",
                f"[class*='{selected_sign}']",
                f"#{selected_sign}",
                ".zodiac-sign",
                ".horoscope-sign",
                "a",
                "button"
            ]
            
            for selector in click_selectors:
                try:
                    elements = driver.find_elements(By.CSS_SELECTOR, selector)
                    for element in elements:
                        if element.is_displayed() and element.is_enabled():
                            text = element.text.strip().lower()
                            if selected_sign in text or any(sign in text for sign in ['aries', 'taurus', 'gemini']):
                                driver.execute_script("arguments[0].click();", element)
                                time.sleep(3)
                                break
                    if elements:
                        break
                except:
                    continue
            
            # ç­‰å¾…ç»“æœåŠ è½½
            time.sleep(5)
            
            # æŸ¥æ‰¾è¿åŠ¿å†…å®¹
            result_selectors = [
                "[class*='horoscope']",
                "[class*='fortune']",
                "[class*='zodiac']",
                "[class*='astrology']",
                "[class*='daily']",
                ".content",
                ".reading",
                "p",
                "div"
            ]
            
            for selector in result_selectors:
                try:
                    elements = driver.find_elements(By.CSS_SELECTOR, selector)
                    for element in elements:
                        text = element.text.strip()
                        if text and len(text) > 50 and len(text) < 800:
                            if any(keyword in text.lower() for keyword in ['today', 'fortune', 'horoscope', 'zodiac', 'astrology', 'energy', 'stars', 'mercury', 'venus']):
                                # æ¸…ç†æ–‡æœ¬
                                if not any(skip_word in text.lower() for skip_word in ['cookie', 'privacy', 'terms', 'contact', 'sign up', 'login', 'advertisement']):
                                    return f"ğŸŒŸ ä»Šæ—¥è¿åŠ¿å åœ\n\n{text}\n\n"
                except:
                    continue
                    
        except Exception as e:
            self.logger.warning(f"çˆ¬å–å æ˜Ÿè¿åŠ¿å¤±è´¥: {e}")
        
        return ""
    
    def crawl_horoscope_fortune_selenium(self, driver) -> str:
        """ä½¿ç”¨Seleniumçˆ¬å–å æ˜Ÿè¿åŠ¿"""
        try:
            driver.get("https://www.horoscope.com/daily-horoscope")
            time.sleep(3)
            
            # ç­‰å¾…é¡µé¢åŠ è½½
            wait = WebDriverWait(driver, 10)
            
            # æŸ¥æ‰¾è¿åŠ¿å†…å®¹
            result_selectors = [
                ".horoscope",
                ".daily-horoscope",
                ".astrology",
                ".zodiac",
                "[class*='horoscope']",
                "[class*='astrology']"
            ]
            
            for selector in result_selectors:
                try:
                    elements = driver.find_elements(By.CSS_SELECTOR, selector)
                    for element in elements:
                        text = element.text.strip()
                        if text and len(text) > 20 and len(text) < 500:
                            if any(keyword in text.lower() for keyword in ['horoscope', 'astrology', 'zodiac', 'fortune']):
                                return f"ğŸŒŸ ä»Šæ—¥è¿åŠ¿å åœ\n\n{text}\n\n"
                except:
                    continue
                    
        except Exception as e:
            self.logger.warning(f"çˆ¬å–å æ˜Ÿè¿åŠ¿å¤±è´¥: {e}")
        
        return ""
    
    def crawl_astro_com(self) -> str:
        """çˆ¬å–å æ˜Ÿè¿åŠ¿ç½‘ç«™"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8'
            }
            
            # å°è¯•è®¿é—®å æ˜Ÿç½‘ç«™
            urls = [
                "https://www.astro.com/daily-horoscope",
                "https://www.horoscope.com/daily-horoscope",
                "https://www.astrology.com/daily"
            ]
            
            for url in urls:
                try:
                    response = requests.get(url, headers=headers, timeout=10)
                    if response.status_code == 200:
                        soup = BeautifulSoup(response.content, 'html.parser')
                        
                        # æŸ¥æ‰¾è¿åŠ¿ä¿¡æ¯
                        fortune_elements = soup.find_all(['div', 'span', 'p'], text=re.compile(r'.*horoscope.*|.*fortune.*|.*lucky.*'))
                        
                        for element in fortune_elements:
                            text = element.get_text().strip()
                            if text and len(text) > 20 and len(text) < 300:
                                return f"ğŸŒŸ ä»Šæ—¥è¿åŠ¿å åœ\n\n{text}\n\n"
                except:
                    continue
                    
        except Exception as e:
            self.logger.warning(f"çˆ¬å–å æ˜Ÿè¿åŠ¿å¤±è´¥: {e}")
        
        return ""
    
    def crawl_fortune_net(self) -> str:
        """çˆ¬å–è¿åŠ¿ç½‘ç«™"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8'
            }
            
            # å°è¯•è®¿é—®è¿åŠ¿ç½‘ç«™
            urls = [
                "https://www.fortune.com/daily",
                "https://www.lucky.net/daily",
                "https://www.fortune-telling.com/daily"
            ]
            
            for url in urls:
                try:
                    response = requests.get(url, headers=headers, timeout=10)
                    if response.status_code == 200:
                        soup = BeautifulSoup(response.content, 'html.parser')
                        
                        # æŸ¥æ‰¾è¿åŠ¿ä¿¡æ¯
                        fortune_elements = soup.find_all(['div', 'span', 'p'], text=re.compile(r'.*fortune.*|.*lucky.*|.*è¿åŠ¿.*'))
                        
                        for element in fortune_elements:
                            text = element.get_text().strip()
                            if text and len(text) > 20 and len(text) < 300:
                                return f"ğŸŒŸ ä»Šæ—¥è¿åŠ¿å åœ\n\n{text}\n\n"
                except:
                    continue
                    
        except Exception as e:
            self.logger.warning(f"çˆ¬å–è¿åŠ¿ç½‘ç«™å¤±è´¥: {e}")
        
        return ""
    
    
    def get_daily_fortune(self) -> str:
        """è·å–æ¯æ—¥è¿åŠ¿ï¼ˆä¸»å…¥å£ï¼‰"""
        return self.crawl_daily_fortune()
    
    def get_daily_random_fun(self) -> str:
        """è·å–æ¯æ—¥éšæœºæœ‰è¶£å†…å®¹"""
        try:
            fun_categories = [
                self.get_daily_joke,
                self.get_daily_quote,
                self.get_daily_fact,
                self.get_daily_riddle
            ]
            
            # éšæœºé€‰æ‹©ä¸€ä¸ªç±»åˆ«
            selected_category = random.choice(fun_categories)
            return selected_category()
            
        except Exception as e:
            self.logger.error(f"è·å–æ¯æ—¥éšæœºæœ‰è¶£å†…å®¹å¤±è´¥: {e}")
            return "ğŸ² æ¯æ—¥éšæœºå†…å®¹è·å–å¤±è´¥"
    
    def crawl_daily_joke(self) -> str:
        """çˆ¬å–æ¯æ—¥ç¬‘è¯"""
        try:
            # å°è¯•å¤šä¸ªç¬‘è¯ç½‘ç«™
            joke_sources = [
                self.crawl_joke_net,
                self.crawl_jokes_com,
                self.crawl_daily_joke_site
            ]
            
            for source_func in joke_sources:
                try:
                    result = source_func()
                    if result and len(result) > 10:  # ç¡®ä¿è·å–åˆ°æœ‰æ•ˆå†…å®¹
                        return result
                except Exception as e:
                    self.logger.warning(f"çˆ¬å–ç¬‘è¯å¤±è´¥: {e}")
                    continue
            
            # å¦‚æœæ‰€æœ‰çˆ¬è™«éƒ½å¤±è´¥ï¼Œå¦‚å®è¿”å›å¤±è´¥ä¿¡æ¯
            return "ğŸ˜„ æ¯æ—¥ç¬‘è¯è·å–å¤±è´¥ - æ‰€æœ‰æ•°æ®æºéƒ½æ— æ³•è®¿é—®"
            
        except Exception as e:
            self.logger.error(f"è·å–æ¯æ—¥ç¬‘è¯å¤±è´¥: {e}")
            return f"ğŸ˜„ æ¯æ—¥ç¬‘è¯è·å–å¤±è´¥ - ç³»ç»Ÿé”™è¯¯: {str(e)}"
    
    def crawl_joke_net(self) -> str:
        """çˆ¬å–ç¬‘è¯ç½‘ç«™"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'application/json,text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                'Accept-Encoding': 'gzip, deflate, br',
                'Connection': 'keep-alive'
            }
            
            # å°è¯•å¤šä¸ªç¬‘è¯API
            joke_sources = [
                self.crawl_jokeapi_api,
                self.crawl_icanhazdadjoke_api,
                self.crawl_jokes_api
            ]
            
            for source_func in joke_sources:
                try:
                    result = source_func()
                    if result and len(result) > 10:
                        return result
                except Exception as e:
                    self.logger.warning(f"çˆ¬å–ç¬‘è¯å¤±è´¥: {e}")
                    continue
                    
        except Exception as e:
            self.logger.warning(f"çˆ¬å–ç¬‘è¯ç½‘ç«™å¤±è´¥: {e}")
        
        return ""
    
    def crawl_jokeapi_api(self) -> str:
        """çˆ¬å–JokeAPI"""
        try:
            url = "https://v2.jokeapi.dev/joke/Any?type=single"
            response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=10)
            if response.status_code == 200:
                data = response.json()
                joke = data.get('joke', '')
                if joke:
                    return f"ğŸ˜„ æ¯æ—¥ä¸€ç¬‘\n\n{joke}"
        except Exception as e:
            self.logger.warning(f"çˆ¬å–JokeAPIå¤±è´¥: {e}")
        return ""
    
    def crawl_icanhazdadjoke_api(self) -> str:
        """çˆ¬å–I Can Haz Dad Joke API"""
        try:
            url = "https://icanhazdadjoke.com/"
            response = requests.get(url, headers={'Accept': 'application/json', 'User-Agent': 'Mozilla/5.0'}, timeout=10)
            if response.status_code == 200:
                data = response.json()
                joke = data.get('joke', '')
                if joke:
                    return f"ğŸ˜„ æ¯æ—¥ä¸€ç¬‘\n\n{joke}"
        except Exception as e:
            self.logger.warning(f"çˆ¬å–I Can Haz Dad Joke APIå¤±è´¥: {e}")
        return ""
    
    def crawl_jokes_api(self) -> str:
        """çˆ¬å–Jokes API"""
        try:
            url = "https://official-joke-api.appspot.com/random_joke"
            response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=10)
            if response.status_code == 200:
                data = response.json()
                setup = data.get('setup', '')
                punchline = data.get('punchline', '')
                if setup and punchline:
                    joke = f"{setup}\n{punchline}"
                    return f"ğŸ˜„ æ¯æ—¥ä¸€ç¬‘\n\n{joke}"
        except Exception as e:
            self.logger.warning(f"çˆ¬å–Jokes APIå¤±è´¥: {e}")
        return ""
    
    def crawl_jokes_com(self) -> str:
        """çˆ¬å–ç¬‘è¯ç½‘ç«™"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8'
            }
            
            # å°è¯•è®¿é—®ç¬‘è¯ç½‘ç«™
            urls = [
                "https://www.jokes.com/daily-joke",
                "https://www.funny-jokes.com/daily",
                "https://www.joke-of-the-day.com"
            ]
            
            for url in urls:
                try:
                    response = requests.get(url, headers=headers, timeout=10)
                    if response.status_code == 200:
                        soup = BeautifulSoup(response.content, 'html.parser')
                        
                        # æŸ¥æ‰¾ç¬‘è¯å†…å®¹
                        joke_elements = soup.find_all(['div', 'span', 'p'], text=re.compile(r'.*joke.*|.*funny.*'))
                        
                        for element in joke_elements:
                            text = element.get_text().strip()
                            if text and len(text) > 10 and len(text) < 200:
                                return f"ğŸ˜„ æ¯æ—¥ä¸€ç¬‘\n\n{text}"
                except:
                    continue
                    
        except Exception as e:
            self.logger.warning(f"çˆ¬å–ç¬‘è¯ç½‘ç«™å¤±è´¥: {e}")
        
        return ""
    
    def crawl_daily_joke_site(self) -> str:
        """çˆ¬å–æ¯æ—¥ç¬‘è¯ç½‘ç«™"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8'
            }
            
            # å°è¯•è®¿é—®æ¯æ—¥ç¬‘è¯ç½‘ç«™
            urls = [
                "https://www.daily-joke.com",
                "https://www.joke-of-the-day.net",
                "https://www.funny-daily.com"
            ]
            
            for url in urls:
                try:
                    response = requests.get(url, headers=headers, timeout=10)
                    if response.status_code == 200:
                        soup = BeautifulSoup(response.content, 'html.parser')
                        
                        # æŸ¥æ‰¾ç¬‘è¯å†…å®¹
                        joke_elements = soup.find_all(['div', 'span', 'p'], text=re.compile(r'.*joke.*|.*funny.*'))
                        
                        for element in joke_elements:
                            text = element.get_text().strip()
                            if text and len(text) > 10 and len(text) < 200:
                                return f"ğŸ˜„ æ¯æ—¥ä¸€ç¬‘\n\n{text}"
                except:
                    continue
                    
        except Exception as e:
            self.logger.warning(f"çˆ¬å–æ¯æ—¥ç¬‘è¯ç½‘ç«™å¤±è´¥: {e}")
        
        return ""
    
    
    def get_daily_joke(self) -> str:
        """è·å–æ¯æ—¥ç¬‘è¯ï¼ˆä¸»å…¥å£ï¼‰"""
        return self.crawl_daily_joke()
    
    def crawl_daily_quote(self) -> str:
        """çˆ¬å–æ¯æ—¥åè¨€"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'application/json,text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                'Accept-Encoding': 'gzip, deflate, br',
                'Connection': 'keep-alive'
            }
            
            # å°è¯•å¤šä¸ªåè¨€API
            quote_sources = [
                self.crawl_quotable_api,
                self.crawl_quotes_api,
                self.crawl_inspirational_api
            ]
            
            for source_func in quote_sources:
                try:
                    result = source_func()
                    if result and len(result) > 20:
                        return result
                except Exception as e:
                    self.logger.warning(f"çˆ¬å–åè¨€å¤±è´¥: {e}")
                    continue
            
            # å¦‚æœæ‰€æœ‰çˆ¬è™«éƒ½å¤±è´¥ï¼Œå¦‚å®è¿”å›å¤±è´¥ä¿¡æ¯
            return "ğŸ’­ æ¯æ—¥åè¨€è·å–å¤±è´¥ - æ‰€æœ‰æ•°æ®æºéƒ½æ— æ³•è®¿é—®"
            
        except Exception as e:
            self.logger.error(f"è·å–æ¯æ—¥åè¨€å¤±è´¥: {e}")
            return f"ğŸ’­ æ¯æ—¥åè¨€è·å–å¤±è´¥ - ç³»ç»Ÿé”™è¯¯: {str(e)}"
    
    def crawl_quotable_api(self) -> str:
        """çˆ¬å–Quotable API"""
        try:
            url = "https://api.quotable.io/random"
            response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=10)
            if response.status_code == 200:
                data = response.json()
                quote = data.get('content', '')
                author = data.get('author', 'æœªçŸ¥')
                if quote:
                    return f"ğŸ’­ æ¯æ—¥åè¨€\n\n\"{quote}\"\n\nâ€”â€” {author}"
        except Exception as e:
            self.logger.warning(f"çˆ¬å–Quotable APIå¤±è´¥: {e}")
        return ""
    
    def crawl_quotes_api(self) -> str:
        """çˆ¬å–Quotes API"""
        try:
            url = "https://zenquotes.io/api/random"
            response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=10)
            if response.status_code == 200:
                data = response.json()
                if data and len(data) > 0:
                    quote = data[0].get('q', '')
                    author = data[0].get('a', 'æœªçŸ¥')
                    if quote:
                        return f"ğŸ’­ æ¯æ—¥åè¨€\n\n\"{quote}\"\n\nâ€”â€” {author}"
        except Exception as e:
            self.logger.warning(f"çˆ¬å–Quotes APIå¤±è´¥: {e}")
        return ""
    
    def crawl_inspirational_api(self) -> str:
        """çˆ¬å–åŠ±å¿—åè¨€API"""
        try:
            url = "https://api.quotable.io/random?tags=inspirational"
            response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=10)
            if response.status_code == 200:
                data = response.json()
                quote = data.get('content', '')
                author = data.get('author', 'æœªçŸ¥')
                if quote:
                    return f"ğŸ’­ æ¯æ—¥åè¨€\n\n\"{quote}\"\n\nâ€”â€” {author}"
        except Exception as e:
            self.logger.warning(f"çˆ¬å–åŠ±å¿—åè¨€APIå¤±è´¥: {e}")
        return ""
    
    def get_daily_quote(self) -> str:
        """è·å–æ¯æ—¥åè¨€ï¼ˆä¸»å…¥å£ï¼‰"""
        return self.crawl_daily_quote()
    
    def crawl_daily_fact(self) -> str:
        """çˆ¬å–æ¯æ—¥å†·çŸ¥è¯†"""
        try:
            # é¦–å…ˆå°è¯•å…è´¹çš„å†·çŸ¥è¯†API
            api_sources = [
                self.crawl_useless_facts_api,
                self.crawl_fun_facts_api
            ]
            
            for source_func in api_sources:
                try:
                    result = source_func()
                    if result and len(result) > 20:
                        return result
                except Exception as e:
                    self.logger.warning(f"çˆ¬å–å†·çŸ¥è¯†APIå¤±è´¥: {e}")
                    continue
            
            # ç„¶åå°è¯•çˆ¬è™«æ–¹æ³•
            fact_sources = [
                self.crawl_fact_net,
                self.crawl_facts_com,
                self.crawl_daily_fact_site
            ]
            
            for source_func in fact_sources:
                try:
                    result = source_func()
                    if result and len(result) > 10:  # ç¡®ä¿è·å–åˆ°æœ‰æ•ˆå†…å®¹
                        return result
                except Exception as e:
                    self.logger.warning(f"çˆ¬å–å†·çŸ¥è¯†å¤±è´¥: {e}")
                    continue
            
            # å¦‚æœæ‰€æœ‰çˆ¬è™«éƒ½å¤±è´¥ï¼Œå¦‚å®è¿”å›å¤±è´¥ä¿¡æ¯
            return "ğŸ¤“ æ¯æ—¥å†·çŸ¥è¯†è·å–å¤±è´¥ - æ‰€æœ‰æ•°æ®æºéƒ½æ— æ³•è®¿é—®"
            
        except Exception as e:
            self.logger.error(f"è·å–æ¯æ—¥å†·çŸ¥è¯†å¤±è´¥: {e}")
            return f"ğŸ¤“ æ¯æ—¥å†·çŸ¥è¯†è·å–å¤±è´¥ - ç³»ç»Ÿé”™è¯¯: {str(e)}"
    
    def crawl_useless_facts_api(self) -> str:
        """ä½¿ç”¨Useless Facts APIè·å–å†·çŸ¥è¯†"""
        try:
            api_url = "https://uselessfacts.jsph.pl/api/v2/facts/random?language=en"
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            }
            
            response = requests.get(api_url, headers=headers, timeout=10)
            if response.status_code == 200:
                try:
                    data = response.json()
                    fact_text = data.get('text', '')
                    source = data.get('source', '')
                    
                    if fact_text:
                        # å°è¯•ç¿»è¯‘æˆä¸­æ–‡
                        chinese_text = self.translate_text(fact_text)
                        
                        result = f"ğŸ§  æ¯æ—¥å†·çŸ¥è¯†\n\n"
                        result += f"ğŸ‡¬ğŸ‡§ {fact_text}\n\n"
                        if chinese_text:
                            result += f"ğŸ‡¨ğŸ‡³ {chinese_text}"
                        else:
                            result += f"ğŸ‡¨ğŸ‡³ ç¿»è¯‘å¤±è´¥ï¼Œä»…æ˜¾ç¤ºè‹±æ–‡åŸæ–‡"
                        
                        if source:
                            result += f"\n\nğŸ“š æ¥æº: {source}"
                        
                        return result
                        
                except Exception as e:
                    self.logger.warning(f"è§£æå†·çŸ¥è¯†APIå¤±è´¥: {e}")
                    
        except Exception as e:
            self.logger.warning(f"å†·çŸ¥è¯†APIå¤±è´¥: {e}")
        
        return ""
    
    def crawl_fun_facts_api(self) -> str:
        """ä½¿ç”¨å¤‡ç”¨å†·çŸ¥è¯†API"""
        try:
            # å¯ä»¥æ·»åŠ å…¶ä»–å†·çŸ¥è¯†APIä½œä¸ºå¤‡ç”¨
            api_url = "https://api.api-ninjas.com/v1/facts"
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            }
            
            response = requests.get(api_url, headers=headers, timeout=10)
            if response.status_code == 200:
                try:
                    data = response.json()
                    if isinstance(data, list) and len(data) > 0:
                        fact_text = data[0].get('fact', '')
                        
                        if fact_text:
                            # å°è¯•ç¿»è¯‘æˆä¸­æ–‡
                            chinese_text = self.translate_text(fact_text)
                            
                            result = f"ğŸ§  æ¯æ—¥å†·çŸ¥è¯†\n\n"
                            result += f"ğŸ‡¬ğŸ‡§ {fact_text}\n\n"
                            if chinese_text:
                                result += f"ğŸ‡¨ğŸ‡³ {chinese_text}"
                            else:
                                result += f"ğŸ‡¨ğŸ‡³ ç¿»è¯‘å¤±è´¥ï¼Œä»…æ˜¾ç¤ºè‹±æ–‡åŸæ–‡"
                            
                            return result
                        
                except Exception as e:
                    self.logger.warning(f"è§£æå¤‡ç”¨å†·çŸ¥è¯†APIå¤±è´¥: {e}")
                    
        except Exception as e:
            self.logger.warning(f"å¤‡ç”¨å†·çŸ¥è¯†APIå¤±è´¥: {e}")
        
        return ""
    
    def crawl_fact_net(self) -> str:
        """çˆ¬å–å†·çŸ¥è¯†ç½‘ç«™"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8'
            }
            
            # å°è¯•è®¿é—®å†·çŸ¥è¯†ç½‘ç«™
            urls = [
                "https://www.fact.net/daily",
                "https://www.facts.com/daily",
                "https://www.interesting-facts.com/daily"
            ]
            
            for url in urls:
                try:
                    response = requests.get(url, headers=headers, timeout=10)
                    if response.status_code == 200:
                        soup = BeautifulSoup(response.content, 'html.parser')
                        
                        # æŸ¥æ‰¾å†·çŸ¥è¯†å†…å®¹
                        fact_elements = soup.find_all(['div', 'span', 'p'], text=re.compile(r'.*fact.*|.*å†·çŸ¥è¯†.*|.*interesting.*'))
                        
                        for element in fact_elements:
                            text = element.get_text().strip()
                            if text and len(text) > 10 and len(text) < 200:
                                return f"ğŸ¤“ æ¯æ—¥å†·çŸ¥è¯†\n\n{text}\n\nğŸ’¡ çŸ¥è¯†å°±æ˜¯åŠ›é‡ï¼Œæ¯å¤©å­¦ä¸€ç‚¹æ–°çŸ¥è¯†ï¼"
                except:
                    continue
                    
        except Exception as e:
            self.logger.warning(f"çˆ¬å–å†·çŸ¥è¯†ç½‘ç«™å¤±è´¥: {e}")
        
        return ""
    
    def crawl_facts_com(self) -> str:
        """çˆ¬å–å†·çŸ¥è¯†ç½‘ç«™"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8'
            }
            
            # å°è¯•è®¿é—®å†·çŸ¥è¯†ç½‘ç«™
            urls = [
                "https://www.facts.com/daily-fact",
                "https://www.interesting-facts.net/daily",
                "https://www.fact-of-the-day.com"
            ]
            
            for url in urls:
                try:
                    response = requests.get(url, headers=headers, timeout=10)
                    if response.status_code == 200:
                        soup = BeautifulSoup(response.content, 'html.parser')
                        
                        # æŸ¥æ‰¾å†·çŸ¥è¯†å†…å®¹
                        fact_elements = soup.find_all(['div', 'span', 'p'], text=re.compile(r'.*fact.*|.*interesting.*'))
                        
                        for element in fact_elements:
                            text = element.get_text().strip()
                            if text and len(text) > 10 and len(text) < 200:
                                return f"ğŸ¤“ æ¯æ—¥å†·çŸ¥è¯†\n\n{text}\n\nğŸ’¡ çŸ¥è¯†å°±æ˜¯åŠ›é‡ï¼Œæ¯å¤©å­¦ä¸€ç‚¹æ–°çŸ¥è¯†ï¼"
                except:
                    continue
                    
        except Exception as e:
            self.logger.warning(f"çˆ¬å–å†·çŸ¥è¯†ç½‘ç«™å¤±è´¥: {e}")
        
        return ""
    
    def crawl_daily_fact_site(self) -> str:
        """çˆ¬å–æ¯æ—¥å†·çŸ¥è¯†ç½‘ç«™"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8'
            }
            
            # å°è¯•è®¿é—®æ¯æ—¥å†·çŸ¥è¯†ç½‘ç«™
            urls = [
                "https://www.daily-fact.com",
                "https://www.fact-of-the-day.net",
                "https://www.interesting-daily.com"
            ]
            
            for url in urls:
                try:
                    response = requests.get(url, headers=headers, timeout=10)
                    if response.status_code == 200:
                        soup = BeautifulSoup(response.content, 'html.parser')
                        
                        # æŸ¥æ‰¾å†·çŸ¥è¯†å†…å®¹
                        fact_elements = soup.find_all(['div', 'span', 'p'], text=re.compile(r'.*fact.*|.*interesting.*'))
                        
                        for element in fact_elements:
                            text = element.get_text().strip()
                            if text and len(text) > 10 and len(text) < 200:
                                return f"ğŸ¤“ æ¯æ—¥å†·çŸ¥è¯†\n\n{text}\n\nğŸ’¡ çŸ¥è¯†å°±æ˜¯åŠ›é‡ï¼Œæ¯å¤©å­¦ä¸€ç‚¹æ–°çŸ¥è¯†ï¼"
                except:
                    continue
                    
        except Exception as e:
            self.logger.warning(f"çˆ¬å–æ¯æ—¥å†·çŸ¥è¯†ç½‘ç«™å¤±è´¥: {e}")
        
        return ""
    
    
    def get_daily_fact(self) -> str:
        """è·å–æ¯æ—¥å†·çŸ¥è¯†ï¼ˆä¸»å…¥å£ï¼‰"""
        return self.crawl_daily_fact()
    
    def crawl_daily_riddle(self) -> str:
        """çˆ¬å–æ¯æ—¥è°œè¯­"""
        try:
            # é¦–å…ˆå°è¯•å…è´¹çš„è°œè¯­API
            api_sources = [
                self.crawl_riddles_api,
                self.crawl_brain_teasers_api
            ]
            
            for source_func in api_sources:
                try:
                    result = source_func()
                    if result and len(result) > 20:
                        return result
                except Exception as e:
                    self.logger.warning(f"çˆ¬å–è°œè¯­APIå¤±è´¥: {e}")
                    continue
            
            # ç„¶åå°è¯•çˆ¬è™«æ–¹æ³•
            riddle_sources = [
                self.crawl_riddle_net,
                self.crawl_riddles_com,
                self.crawl_daily_riddle_site
            ]
            
            for source_func in riddle_sources:
                try:
                    result = source_func()
                    if result and len(result) > 10:  # ç¡®ä¿è·å–åˆ°æœ‰æ•ˆå†…å®¹
                        return result
                except Exception as e:
                    self.logger.warning(f"çˆ¬å–è°œè¯­å¤±è´¥: {e}")
                    continue
            
            # å¦‚æœæ‰€æœ‰çˆ¬è™«éƒ½å¤±è´¥ï¼Œå¦‚å®è¿”å›å¤±è´¥ä¿¡æ¯
            return "ğŸ¤” æ¯æ—¥è°œè¯­è·å–å¤±è´¥ - æ‰€æœ‰æ•°æ®æºéƒ½æ— æ³•è®¿é—®"
            
        except Exception as e:
            self.logger.error(f"è·å–æ¯æ—¥è°œè¯­å¤±è´¥: {e}")
            return f"ğŸ¤” æ¯æ—¥è°œè¯­è·å–å¤±è´¥ - ç³»ç»Ÿé”™è¯¯: {str(e)}"
    
    def crawl_riddles_api(self) -> str:
        """ä½¿ç”¨å…è´¹çš„è°œè¯­API"""
        try:
            api_url = "https://riddles-api.vercel.app/random"
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            }
            
            response = requests.get(api_url, headers=headers, timeout=10)
            if response.status_code == 200:
                try:
                    data = response.json()
                    riddle = data.get('riddle', '')
                    answer = data.get('answer', '')
                    
                    if riddle and answer:
                        # ç¿»è¯‘è°œè¯­å’Œç­”æ¡ˆ
                        chinese_riddle = self.translate_text(riddle)
                        chinese_answer = self.translate_text(answer)
                        
                        result = f"ğŸ¤” æ¯æ—¥è°œè¯­\n\n"
                        result += f"ğŸ‡¬ğŸ‡§ â“ {riddle}\n"
                        if chinese_riddle:
                            result += f"ğŸ‡¨ğŸ‡³ â“ {chinese_riddle}\n\n"
                        else:
                            result += f"ğŸ‡¨ğŸ‡³ â“ ç¿»è¯‘å¤±è´¥\n\n"
                            
                        result += f"ğŸ‡¬ğŸ‡§ ğŸ’¡ ç­”æ¡ˆ: {answer}\n"
                        if chinese_answer:
                            result += f"ğŸ‡¨ğŸ‡³ ğŸ’¡ ç­”æ¡ˆ: {chinese_answer}"
                        else:
                            result += f"ğŸ‡¨ğŸ‡³ ğŸ’¡ ç­”æ¡ˆç¿»è¯‘å¤±è´¥"
                        
                        return result
                        
                except Exception as e:
                    self.logger.warning(f"è§£æè°œè¯­APIå¤±è´¥: {e}")
                    
        except Exception as e:
            self.logger.warning(f"è°œè¯­APIå¤±è´¥: {e}")
        
        return ""
    
    def crawl_brain_teasers_api(self) -> str:
        """ä½¿ç”¨å¤‡ç”¨è„‘ç­‹æ€¥è½¬å¼¯API"""
        try:
            # å¯ä»¥æ·»åŠ å…¶ä»–è°œè¯­APIä½œä¸ºå¤‡ç”¨
            api_url = "https://api.api-ninjas.com/v1/riddles"
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            }
            
            response = requests.get(api_url, headers=headers, timeout=10)
            if response.status_code == 200:
                try:
                    data = response.json()
                    if isinstance(data, list) and len(data) > 0:
                        riddle_data = data[0]
                        title = riddle_data.get('title', '')
                        question = riddle_data.get('question', '')
                        answer = riddle_data.get('answer', '')
                        
                        if question and answer:
                            # ç¿»è¯‘è°œè¯­å’Œç­”æ¡ˆ
                            chinese_question = self.translate_text(question)
                            chinese_answer = self.translate_text(answer)
                            chinese_title = self.translate_text(title) if title else ""
                            
                            result = f"ğŸ¤” æ¯æ—¥è°œè¯­\n\n"
                            if title:
                                result += f"ğŸ‡¬ğŸ‡§ ğŸ“ {title}\n"
                                if chinese_title:
                                    result += f"ğŸ‡¨ğŸ‡³ ğŸ“ {chinese_title}\n\n"
                                else:
                                    result += f"ğŸ‡¨ğŸ‡³ ğŸ“ æ ‡é¢˜ç¿»è¯‘å¤±è´¥\n\n"
                            
                            result += f"ğŸ‡¬ğŸ‡§ â“ {question}\n"
                            if chinese_question:
                                result += f"ğŸ‡¨ğŸ‡³ â“ {chinese_question}\n\n"
                            else:
                                result += f"ğŸ‡¨ğŸ‡³ â“ ç¿»è¯‘å¤±è´¥\n\n"
                                
                            result += f"ğŸ‡¬ğŸ‡§ ğŸ’¡ ç­”æ¡ˆ: {answer}\n"
                            if chinese_answer:
                                result += f"ğŸ‡¨ğŸ‡³ ğŸ’¡ ç­”æ¡ˆ: {chinese_answer}"
                            else:
                                result += f"ğŸ‡¨ğŸ‡³ ğŸ’¡ ç­”æ¡ˆç¿»è¯‘å¤±è´¥"
                            
                            return result
                        
                except Exception as e:
                    self.logger.warning(f"è§£æå¤‡ç”¨è°œè¯­APIå¤±è´¥: {e}")
                    
        except Exception as e:
            self.logger.warning(f"å¤‡ç”¨è°œè¯­APIå¤±è´¥: {e}")
        
        return ""
    
    def crawl_riddle_net(self) -> str:
        """çˆ¬å–è°œè¯­ç½‘ç«™"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8'
            }
            
            # å°è¯•è®¿é—®è°œè¯­ç½‘ç«™
            urls = [
                "https://www.riddle.net/daily",
                "https://www.riddles.com/daily",
                "https://www.brain-teasers.com/daily"
            ]
            
            for url in urls:
                try:
                    response = requests.get(url, headers=headers, timeout=10)
                    if response.status_code == 200:
                        soup = BeautifulSoup(response.content, 'html.parser')
                        
                        # æŸ¥æ‰¾è°œè¯­å†…å®¹
                        riddle_elements = soup.find_all(['div', 'span', 'p'], text=re.compile(r'.*riddle.*|.*è°œè¯­.*|.*puzzle.*'))
                        
                        for element in riddle_elements:
                            text = element.get_text().strip()
                            if text and len(text) > 10 and len(text) < 200:
                                return f"ğŸ¤” æ¯æ—¥è°œè¯­\n\nâ“ {text}\n\nğŸ§  åŠ¨åŠ¨è„‘ç­‹ï¼Œä¿æŒæ€ç»´æ´»è·ƒï¼"
                except:
                    continue
                    
        except Exception as e:
            self.logger.warning(f"çˆ¬å–è°œè¯­ç½‘ç«™å¤±è´¥: {e}")
        
        return ""
    
    def crawl_riddles_com(self) -> str:
        """çˆ¬å–è°œè¯­ç½‘ç«™"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8'
            }
            
            # å°è¯•è®¿é—®è°œè¯­ç½‘ç«™
            urls = [
                "https://www.riddles.com/daily-riddle",
                "https://www.brain-teasers.net/daily",
                "https://www.riddle-of-the-day.com"
            ]
            
            for url in urls:
                try:
                    response = requests.get(url, headers=headers, timeout=10)
                    if response.status_code == 200:
                        soup = BeautifulSoup(response.content, 'html.parser')
                        
                        # æŸ¥æ‰¾è°œè¯­å†…å®¹
                        riddle_elements = soup.find_all(['div', 'span', 'p'], text=re.compile(r'.*riddle.*|.*puzzle.*'))
                        
                        for element in riddle_elements:
                            text = element.get_text().strip()
                            if text and len(text) > 10 and len(text) < 200:
                                return f"ğŸ¤” æ¯æ—¥è°œè¯­\n\nâ“ {text}\n\nğŸ§  åŠ¨åŠ¨è„‘ç­‹ï¼Œä¿æŒæ€ç»´æ´»è·ƒï¼"
                except:
                    continue
                    
        except Exception as e:
            self.logger.warning(f"çˆ¬å–è°œè¯­ç½‘ç«™å¤±è´¥: {e}")
        
        return ""
    
    def crawl_daily_riddle_site(self) -> str:
        """çˆ¬å–æ¯æ—¥è°œè¯­ç½‘ç«™"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8'
            }
            
            # å°è¯•è®¿é—®æ¯æ—¥è°œè¯­ç½‘ç«™
            urls = [
                "https://www.daily-riddle.com",
                "https://www.riddle-of-the-day.net",
                "https://www.brain-daily.com"
            ]
            
            for url in urls:
                try:
                    response = requests.get(url, headers=headers, timeout=10)
                    if response.status_code == 200:
                        soup = BeautifulSoup(response.content, 'html.parser')
                        
                        # æŸ¥æ‰¾è°œè¯­å†…å®¹
                        riddle_elements = soup.find_all(['div', 'span', 'p'], text=re.compile(r'.*riddle.*|.*puzzle.*'))
                        
                        for element in riddle_elements:
                            text = element.get_text().strip()
                            if text and len(text) > 10 and len(text) < 200:
                                return f"ğŸ¤” æ¯æ—¥è°œè¯­\n\nâ“ {text}\n\nğŸ§  åŠ¨åŠ¨è„‘ç­‹ï¼Œä¿æŒæ€ç»´æ´»è·ƒï¼"
                except:
                    continue
                    
        except Exception as e:
            self.logger.warning(f"çˆ¬å–æ¯æ—¥è°œè¯­ç½‘ç«™å¤±è´¥: {e}")
        
        return ""
    
    
    def get_daily_riddle(self) -> str:
        """è·å–æ¯æ—¥è°œè¯­ï¼ˆä¸»å…¥å£ï¼‰"""
        return self.crawl_daily_riddle()

    def crawl_hot_search(self) -> str:
        """çˆ¬å–å„å¹³å°çƒ­æœ"""
        try:
            # å°è¯•å¤šä¸ªçƒ­æœç½‘ç«™
            hot_search_sources = [
                self.crawl_weibo_hot,
                self.crawl_baidu_hot,
                self.crawl_zhihu_hot
            ]
            
            hot_search_data = {}
            
            for source_func in hot_search_sources:
                try:
                    result = source_func()
                    if result:
                        hot_search_data.update(result)
                except Exception as e:
                    self.logger.warning(f"çˆ¬å–çƒ­æœå¤±è´¥: {e}")
                    continue
            
            # å¦‚æœçˆ¬è™«éƒ½å¤±è´¥ï¼Œå¦‚å®è¿”å›å¤±è´¥ä¿¡æ¯
            if not hot_search_data:
                return "ğŸ”¥ çƒ­æœä¿¡æ¯è·å–å¤±è´¥ - æ‰€æœ‰æ•°æ®æºéƒ½æ— æ³•è®¿é—®"
            
            # æ ¼å¼åŒ–çƒ­æœæ•°æ®
            hot_search_msg = "ğŸ”¥ ä»Šæ—¥çƒ­æœæ¦œ\n\n"
            for platform, topics in hot_search_data.items():
                hot_search_msg += f"ğŸ“± {platform}çƒ­æœ:\n"
                for i, topic in enumerate(topics[:3], 1):
                    hot_search_msg += f"   {i}. {topic}\n"
                hot_search_msg += "\n"
            
            return hot_search_msg.strip()
            
        except Exception as e:
            self.logger.error(f"è·å–çƒ­æœå¤±è´¥: {e}")
            return f"ğŸ”¥ çƒ­æœä¿¡æ¯è·å–å¤±è´¥ - ç³»ç»Ÿé”™è¯¯: {str(e)}"
    
    def crawl_weibo_hot(self) -> dict:
        """çˆ¬å–å¾®åšçƒ­æœ"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8'
            }
            
            # å°è¯•è®¿é—®å¾®åšçƒ­æœ
            urls = [
                "https://s.weibo.com/top/summary",
                "https://weibo.com/hot",
                "https://trends.weibo.com"
            ]
            
            for url in urls:
                try:
                    response = requests.get(url, headers=headers, timeout=10)
                    if response.status_code == 200:
                        soup = BeautifulSoup(response.content, 'html.parser')
                        
                        # æŸ¥æ‰¾çƒ­æœå†…å®¹
                        hot_elements = soup.find_all(['a', 'span', 'div'], text=re.compile(r'.*çƒ­æœ.*|.*çƒ­é—¨.*|.*trending.*'))
                        
                        topics = []
                        for element in hot_elements:
                            text = element.get_text().strip()
                            if text and len(text) > 2 and len(text) < 50:
                                topics.append(text)
                        
                        if topics:
                            return {"å¾®åš": topics[:5]}
                except:
                    continue
                    
        except Exception as e:
            self.logger.warning(f"çˆ¬å–å¾®åšçƒ­æœå¤±è´¥: {e}")
        
        return {}
    
    def crawl_baidu_hot(self) -> dict:
        """çˆ¬å–ç™¾åº¦çƒ­æœ"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8'
            }
            
            # å°è¯•è®¿é—®ç™¾åº¦çƒ­æœ
            urls = [
                "https://top.baidu.com/board?tab=realtime",
                "https://www.baidu.com/s?wd=çƒ­æœ",
                "https://trends.baidu.com"
            ]
            
            for url in urls:
                try:
                    response = requests.get(url, headers=headers, timeout=10)
                    if response.status_code == 200:
                        soup = BeautifulSoup(response.content, 'html.parser')
                        
                        # æŸ¥æ‰¾çƒ­æœå†…å®¹
                        hot_elements = soup.find_all(['a', 'span', 'div'], text=re.compile(r'.*çƒ­æœ.*|.*çƒ­é—¨.*|.*trending.*'))
                        
                        topics = []
                        # è¿‡æ»¤æ— ç”¨å†…å®¹çš„å…³é”®è¯
                        filter_keywords = [
                            'çƒ­æœæ¦œ', 'çƒ­æœæŒ‡æ•°', 'çƒ­é—¨è¯é¢˜', 'çƒ­æœ', 'çƒ­é—¨',
                            'æ¨è', 'å¹¿å‘Š', 'èµåŠ©', 'ç™»å½•', 'æ³¨å†Œ', 'ä¸‹è½½', 'APP',
                            'æ›´å¤š', 'æŸ¥çœ‹æ›´å¤š', 'å±•å¼€', 'æ”¶èµ·', 'åˆ·æ–°', 'åŠ è½½',
                            'ç™¾åº¦', 'æœç´¢', 'é¦–é¡µ', 'æ–°é—»', 'è´´å§'
                        ]
                        
                        for element in hot_elements:
                            text = element.get_text().strip()
                            # åŸºæœ¬é•¿åº¦å’Œå†…å®¹è¿‡æ»¤
                            if text and len(text) > 4 and len(text) < 50:
                                # è¿‡æ»¤æ‰æ— ç”¨å…³é”®è¯
                                if not any(keyword in text for keyword in filter_keywords):
                                    # ç¡®ä¿æ˜¯æœ‰æ„ä¹‰çš„å†…å®¹ï¼ˆåŒ…å«ä¸­æ–‡å­—ç¬¦ï¼‰
                                    if any('\u4e00' <= char <= '\u9fff' for char in text):
                                        topics.append(text)
                        
                        # å»é‡å¹¶é™åˆ¶æ•°é‡
                        unique_topics = list(dict.fromkeys(topics))
                        if unique_topics:
                            return {"ç™¾åº¦": unique_topics[:5]}
                except:
                    continue
                    
        except Exception as e:
            self.logger.warning(f"çˆ¬å–ç™¾åº¦çƒ­æœå¤±è´¥: {e}")
        
        return {}
    
    def crawl_zhihu_hot(self) -> dict:
        """çˆ¬å–çŸ¥ä¹çƒ­æœ"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8'
            }
            
            # å°è¯•è®¿é—®çŸ¥ä¹çƒ­æœ
            urls = [
                "https://www.zhihu.com/hot",
                "https://www.zhihu.com/trending",
                "https://www.zhihu.com/explore"
            ]
            
            for url in urls:
                try:
                    response = requests.get(url, headers=headers, timeout=10)
                    if response.status_code == 200:
                        soup = BeautifulSoup(response.content, 'html.parser')
                        
                        # æŸ¥æ‰¾çƒ­æœå†…å®¹
                        hot_elements = soup.find_all(['a', 'span', 'div'], text=re.compile(r'.*çƒ­æœ.*|.*çƒ­é—¨.*|.*trending.*'))
                        
                        topics = []
                        # è¿‡æ»¤æ— ç”¨å†…å®¹çš„å…³é”®è¯
                        filter_keywords = [
                            'çƒ­é—¨æ”¶è—å¤¹', 'æ”¶è—å¤¹', 'çƒ­æœæ¦œ', 'çƒ­æœæŒ‡æ•°', 'çƒ­é—¨è¯é¢˜',
                            'æ¨è', 'å¹¿å‘Š', 'èµåŠ©', 'ç™»å½•', 'æ³¨å†Œ', 'ä¸‹è½½', 'APP',
                            'æ›´å¤š', 'æŸ¥çœ‹æ›´å¤š', 'å±•å¼€', 'æ”¶èµ·', 'åˆ·æ–°', 'åŠ è½½'
                        ]
                        
                        for element in hot_elements:
                            text = element.get_text().strip()
                            # åŸºæœ¬é•¿åº¦å’Œå†…å®¹è¿‡æ»¤
                            if text and len(text) > 4 and len(text) < 50:
                                # è¿‡æ»¤æ‰æ— ç”¨å…³é”®è¯
                                if not any(keyword in text for keyword in filter_keywords):
                                    # ç¡®ä¿æ˜¯æœ‰æ„ä¹‰çš„å†…å®¹ï¼ˆåŒ…å«ä¸­æ–‡å­—ç¬¦ï¼‰
                                    if any('\u4e00' <= char <= '\u9fff' for char in text):
                                        topics.append(text)
                        
                        # å»é‡å¹¶é™åˆ¶æ•°é‡
                        unique_topics = list(dict.fromkeys(topics))
                        if unique_topics:
                            return {"çŸ¥ä¹": unique_topics[:5]}
                except:
                    continue
                    
        except Exception as e:
            self.logger.warning(f"çˆ¬å–çŸ¥ä¹çƒ­æœå¤±è´¥: {e}")
        
        return {}
    
    
    def get_hot_search(self) -> str:
        """è·å–å„å¹³å°çƒ­æœï¼ˆä¸»å…¥å£ï¼‰"""
        return self.crawl_hot_search()

    def get_daily_tips(self) -> str:
        """è·å–æ¯æ—¥å°è´´å£«"""
        tips_categories = {
            "å¥åº·": [
                "ğŸ’§ æ¯å¤©è‡³å°‘å–8æ¯æ°´ï¼Œä¿æŒèº«ä½“æ°´åˆ†å……è¶³",
                "ğŸš¶â€â™‚ï¸ æ¯å°æ—¶èµ·èº«æ´»åŠ¨5åˆ†é’Ÿï¼Œé¢„é˜²ä¹…åå±å®³",
                "ğŸ˜´ ä¿è¯7-8å°æ—¶ç¡çœ ï¼Œæé«˜å…ç–«åŠ›",
                "ğŸ¥— å¤šåƒè”¬èœæ°´æœï¼Œè¡¥å……ç»´ç”Ÿç´ ",
                "ğŸ§˜â€â™€ï¸ æ·±å‘¼å¸æ”¾æ¾ï¼Œç¼“è§£å·¥ä½œå‹åŠ›"
            ],
            "å·¥ä½œ": [
                "ğŸ“ ä½¿ç”¨ç•ªèŒ„å·¥ä½œæ³•ï¼Œæé«˜ä¸“æ³¨åŠ›",
                "ğŸ“Š åˆ¶å®šæ¯æ—¥ä»»åŠ¡æ¸…å•ï¼Œæé«˜æ•ˆç‡",
                "ğŸ’» å®šæœŸæ•´ç†ç”µè„‘æ–‡ä»¶ï¼Œä¿æŒæ¡Œé¢æ•´æ´",
                "ğŸ“ é‡è¦äº‹æƒ…ä¼˜å…ˆå¤„ç†ï¼Œé¿å…æ‹–å»¶",
                "ğŸ¤ ä¸»åŠ¨æ²Ÿé€šåä½œï¼Œæå‡å›¢é˜Ÿæ•ˆç‡"
            ],
            "ç”Ÿæ´»": [
                "ğŸŒ± å…»ä¸€ç›†ç»¿æ¤ï¼Œå‡€åŒ–ç©ºæ°”ç¾åŒ–ç¯å¢ƒ",
                "ğŸ“š æ¯å¤©é˜…è¯»30åˆ†é’Ÿï¼Œä¸°å¯ŒçŸ¥è¯†å‚¨å¤‡",
                "ğŸµ å¬éŸ³ä¹æ”¾æ¾å¿ƒæƒ…ï¼Œç¼“è§£ç–²åŠ³",
                "ğŸ‘¨â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦ å¤šé™ªä¼´å®¶äººæœ‹å‹ï¼Œå¢è¿›æ„Ÿæƒ…",
                "ğŸ’° è®°å½•æ¯æ—¥æ”¯å‡ºï¼ŒåŸ¹å…»ç†è´¢ä¹ æƒ¯"
            ]
        }
        
        # éšæœºé€‰æ‹©ä¸€ä¸ªç±»åˆ«å’Œä¸€æ¡å»ºè®®
        category = random.choice(list(tips_categories.keys()))
        tip = random.choice(tips_categories[category])
        
        return f"ğŸ’¡ ä»Šæ—¥{category}å°è´´å£«\n{tip}"

    def get_holiday_info(self) -> str:
        """è·å–èŠ‚å‡æ—¥ä¿¡æ¯"""
        try:
            today = datetime.now()
            
            # 2024å¹´èŠ‚å‡æ—¥
            holidays = {
                "2024-01-01": "å…ƒæ—¦",
                "2024-02-10": "æ˜¥èŠ‚",
                "2024-04-04": "æ¸…æ˜èŠ‚",
                "2024-05-01": "åŠ³åŠ¨èŠ‚",
                "2024-06-10": "ç«¯åˆèŠ‚",
                "2024-09-17": "ä¸­ç§‹èŠ‚",
                "2024-10-01": "å›½åº†èŠ‚"
            }
            
            today_str = today.strftime("%Y-%m-%d")
            
            # æ£€æŸ¥ä»Šå¤©æ˜¯å¦æ˜¯èŠ‚å‡æ—¥
            if today_str in holidays:
                return f"ğŸ‰ ä»Šå¤©æ˜¯{holidays[today_str]}ï¼Œç¥æ‚¨èŠ‚æ—¥å¿«ä¹ï¼"
            
            # æ£€æŸ¥æœªæ¥7å¤©å†…çš„èŠ‚å‡æ—¥
            for i in range(1, 8):
                future_date = (today + timedelta(days=i)).strftime("%Y-%m-%d")
                if future_date in holidays:
                    return f"ğŸ“… {holidays[future_date]}è¿˜æœ‰{i}å¤©ï¼Œè®°å¾—æå‰å®‰æ’å“¦ï¼"
            
            return ""
            
        except Exception as e:
            self.logger.error(f"è·å–èŠ‚å‡æ—¥ä¿¡æ¯å¤±è´¥: {e}")
            return ""

    def get_stock_index_brief(self) -> str:
        """è·å–è‚¡å¸‚æŒ‡æ•°ç®€æŠ¥"""
        try:
            # æ¨¡æ‹Ÿè‚¡å¸‚æ•°æ®
            indices = {
                "ä¸Šè¯æŒ‡æ•°": {"value": 3200 + random.randint(-100, 100), "change": round(random.uniform(-2, 2), 2)},
                "æ·±è¯æˆæŒ‡": {"value": 12000 + random.randint(-500, 500), "change": round(random.uniform(-2, 2), 2)},
                "åˆ›ä¸šæ¿æŒ‡": {"value": 2500 + random.randint(-100, 100), "change": round(random.uniform(-3, 3), 2)}
            }
            
            stock_msg = "ğŸ“ˆ è‚¡å¸‚ç®€æŠ¥\n\n"
            for name, data in indices.items():
                change_emoji = "ğŸ“ˆ" if data["change"] >= 0 else "ğŸ“‰"
                change_sign = "+" if data["change"] >= 0 else ""
                stock_msg += f"{change_emoji} {name}: {data['value']:.2f} ({change_sign}{data['change']}%)\n"
            
            return stock_msg.strip()
            
        except Exception as e:
            self.logger.error(f"è·å–è‚¡å¸‚ä¿¡æ¯å¤±è´¥: {e}")
            return ""

    def send_morning_report(self):
        """å‘é€æ—©é—´ç»¼åˆæŠ¥å‘Š"""
        try:
            current_time = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M")
            
            # æ”¶é›†æ‰€æœ‰ä¿¡æ¯
            weather = self.get_weather_info()
            taboo = self.get_daily_taboo()
            tarot = self.get_tarot_card()
            hot_search = self.get_hot_search()
            daily_tip = self.get_daily_tips()
            holiday_info = self.get_holiday_info()
            stock_info = self.get_stock_index_brief()
            
            # ç»„åˆæ¶ˆæ¯
            message = f"""ğŸŒ… æ—©å®‰ï¼ç»¼åˆç”Ÿæ´»åŠ©æ‰‹ä¸ºæ‚¨æ’­æŠ¥
â° {current_time}

{weather}

{taboo}

{tarot}

{hot_search}

{daily_tip}"""
            
            if holiday_info:
                message += f"\n\n{holiday_info}"
            
            if stock_info:
                message += f"\n\n{stock_info}"
            
            message += "\n\nç¥æ‚¨ä»Šå¤©å¿ƒæƒ…æ„‰å¿«ï¼Œå·¥ä½œé¡ºåˆ©ï¼ğŸŒˆ"
            
            self.send_message(message)
            
        except Exception as e:
            self.logger.error(f"å‘é€æ—©é—´æŠ¥å‘Šå¤±è´¥: {e}")

    def send_evening_report(self):
        """å‘é€æ™šé—´ç®€æŠ¥"""
        try:
            current_time = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M")
            
            # éšæœºé€‰æ‹©æ™šé—´å†…å®¹
            evening_functions = [
                self.get_daily_fortune,
                self.get_tarot_card,
                self.get_daily_tips
            ]
            
            # éšæœºé€‰æ‹©2ä¸ªåŠŸèƒ½
            selected_functions = random.sample(evening_functions, 2)
            
            # è·å–å†…å®¹
            content1 = selected_functions[0]()
            content2 = selected_functions[1]()
            
            message = f"""ğŸŒ™ æ™šå®‰ï¼ä»Šæ—¥æ€»ç»“
â° {current_time}

{content1}

{content2}

ğŸŒŸ ä»Šæ—¥æ„Ÿæ‚Ÿ:
æ¯ä¸€å¤©éƒ½æ˜¯æ–°çš„å¼€å§‹ï¼Œæ„Ÿè°¢ä»Šå¤©çš„åŠªåŠ›å’Œæ”¶è·ã€‚
æ˜å¤©åˆæ˜¯å……æ»¡å¸Œæœ›çš„ä¸€å¤©ï¼

ğŸ’¤ æ—©ç‚¹ä¼‘æ¯ï¼Œä¿è¯å……è¶³ç¡çœ å“¦ï½"""
            
            self.send_message(message)
            
        except Exception as e:
            self.logger.error(f"å‘é€æ™šé—´æŠ¥å‘Šå¤±è´¥: {e}")

    def send_noon_reminder(self):
        """å‘é€åˆé—´æé†’"""
        try:
            # éšæœºé€‰æ‹©åˆé—´å†…å®¹
            noon_functions = [
                self.get_noon_tip,
                self.get_daily_joke,
                self.get_daily_fact,
                self.get_daily_quote
            ]
            
            # éšæœºé€‰æ‹©1-2ä¸ªåŠŸèƒ½
            selected_functions = random.sample(noon_functions, random.randint(1, 2))
            
            current_time = datetime.now().strftime("%H:%M")
            
            message = f"""â˜€ï¸ åˆé—´æé†’
â° {current_time}

"""

            for func in selected_functions:
                message += f"{func()}\n\n"
            
            message += "ä¸‹åˆç»§ç»­åŠ æ²¹ï¼ğŸ’ª"
            
            self.send_message(message)
            
        except Exception as e:
            self.logger.error(f"å‘é€åˆé—´æé†’å¤±è´¥: {e}")
    
    def get_noon_tip(self) -> str:
        """è·å–åˆé—´å°è´´å£«"""
        tips = [
            "ğŸ½ï¸ åˆé¤æ—¶é—´åˆ°äº†ï¼Œè®°å¾—æŒ‰æ—¶åƒé¥­å“¦ï¼",
            "ğŸ˜´ åˆä¼‘ä¸€ä¸‹ï¼Œä¸‹åˆæ›´æœ‰ç²¾ç¥ï¼",
            "ğŸ’§ è®°å¾—å¤šå–æ°´ï¼Œä¿æŒèº«ä½“æ°´åˆ†å……è¶³ï¼",
            "ğŸš¶â€â™‚ï¸ é¥­åæ•£æ•£æ­¥ï¼Œæœ‰åŠ©æ¶ˆåŒ–ï¼",
            "ğŸ§˜â€â™€ï¸ æ”¾æ¾ä¸€ä¸‹ï¼Œç¼“è§£ä¸Šåˆçš„å·¥ä½œå‹åŠ›ï¼",
            "ğŸ‘€ çœ‹çœ‹çª—å¤–ï¼Œè®©çœ¼ç›ä¼‘æ¯ä¸€ä¸‹ï¼",
            "ğŸµ å¬é¦–å–œæ¬¢çš„æ­Œï¼Œæ”¾æ¾å¿ƒæƒ…ï¼",
            "ğŸ“± æ”¾ä¸‹æ‰‹æœºï¼Œç»™å¤§è„‘ä¸€ä¸ªä¼‘æ¯ï¼"
        ]
        
        tip = random.choice(tips)
        return f"ğŸ’¡ åˆé—´å°è´´å£«\n\n{tip}"

    def run_scheduler(self):
        """è¿è¡Œå®šæ—¶ä»»åŠ¡"""
        # æ—©é—´æŠ¥å‘Š - æ¯å¤©8:00
        schedule.every().day.at("08:00").do(self.send_morning_report)
        
        # åˆé—´æé†’ - æ¯å¤©12:00
        schedule.every().day.at("12:00").do(self.send_noon_reminder)
        
        # æ™šé—´ç®€æŠ¥ - æ¯å¤©21:00
        schedule.every().day.at("21:00").do(self.send_evening_report)
        
        self.logger.info("ç»¼åˆç”Ÿæ´»æœºå™¨äººå¯åŠ¨æˆåŠŸ")
        self.send_message("ğŸ¤– ç»¼åˆç”Ÿæ´»åŠ©æ‰‹å·²å¯åŠ¨ï¼\nå°†ä¸ºæ‚¨æä¾›å¤©æ°”ã€ç”µå½±ã€çƒ­æœã€ç”Ÿæ´»å°è´´å£«ç­‰æœåŠ¡ï½")
        
        while True:
            try:
                schedule.run_pending()
                time.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
            except KeyboardInterrupt:
                self.logger.info("æœºå™¨äººåœæ­¢è¿è¡Œ")
                break
            except Exception as e:
                self.logger.error(f"è¿è¡Œå¼‚å¸¸: {e}")
                time.sleep(60)

if __name__ == "__main__":
    webhook_url = "https://qyapi.weixin.qq.com/cgi-bin/webhook/send?key=fc1b1b81-35ef-4c44-83a2-3ad3a4c2f516"
    bot = LifestyleBot(webhook_url)
    bot.run_scheduler()