"""
æ™ºèƒ½æ•°æ®ç®¡ç†å™¨
å®ç°ç¼“å­˜ã€é™æµã€é‡è¯•ç­‰æœºåˆ¶ï¼Œå¹³è¡¡é€Ÿåº¦å’Œç¨³å®šæ€§
"""

import time
import json
import hashlib
import threading
from datetime import datetime, timedelta
from typing import Dict, Any, Optional
import pandas as pd
import os
from functools import wraps

class SmartDataManager:
    """æ™ºèƒ½æ•°æ®ç®¡ç†å™¨"""
    
    def __init__(self, cache_dir="data_cache", max_requests_per_minute=15):
        self.cache_dir = cache_dir
        self.max_requests_per_minute = max_requests_per_minute
        self.request_times = []
        self.lock = threading.Lock()
        
        # åˆ›å»ºç¼“å­˜ç›®å½•
        os.makedirs(cache_dir, exist_ok=True)
        
        # ç¼“å­˜é…ç½®
        self.cache_config = {
            'stock_basic': 24 * 60,      # åŸºç¡€ä¿¡æ¯ç¼“å­˜24å°æ—¶
            'daily_data': 30,            # æ—¥çº¿æ•°æ®ç¼“å­˜30åˆ†é’Ÿ
            'realtime_data': 1,          # å®æ—¶æ•°æ®ç¼“å­˜1åˆ†é’Ÿ
            'financial_data': 60 * 24,   # è´¢åŠ¡æ•°æ®ç¼“å­˜1å¤©
            'news_data': 10,             # æ–°é—»æ•°æ®ç¼“å­˜10åˆ†é’Ÿ
        }
    
    def _get_cache_key(self, func_name: str, *args, **kwargs) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        key_data = {
            'func': func_name,
            'args': args,
            'kwargs': kwargs
        }
        key_str = json.dumps(key_data, sort_keys=True, default=str)
        return hashlib.md5(key_str.encode()).hexdigest()
    
    def _get_cache_path(self, cache_key: str) -> str:
        """è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        return os.path.join(self.cache_dir, f"{cache_key}.json")
    
    def _is_cache_valid(self, cache_path: str, cache_minutes: int) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ"""
        if not os.path.exists(cache_path):
            return False
        
        file_time = datetime.fromtimestamp(os.path.getmtime(cache_path))
        expire_time = file_time + timedelta(minutes=cache_minutes)
        return datetime.now() < expire_time
    
    def _load_cache(self, cache_path: str) -> Optional[Any]:
        """åŠ è½½ç¼“å­˜æ•°æ®"""
        try:
            with open(cache_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                if 'dataframe' in data:
                    return pd.read_json(data['dataframe'])
                return data['result']
        except Exception as e:
            print(f"ç¼“å­˜åŠ è½½å¤±è´¥: {e}")
            return None
    
    def _save_cache(self, cache_path: str, result: Any):
        """ä¿å­˜ç¼“å­˜æ•°æ®"""
        try:
            cache_data = {}
            if isinstance(result, pd.DataFrame):
                cache_data['dataframe'] = result.to_json()
            else:
                cache_data['result'] = result
            
            with open(cache_path, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, default=str)
        except Exception as e:
            print(f"ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")
    
    def _wait_for_rate_limit(self):
        """ç­‰å¾…æ»¡è¶³é™æµè¦æ±‚"""
        with self.lock:
            now = time.time()
            # æ¸…ç†1åˆ†é’Ÿå‰çš„è¯·æ±‚è®°å½•
            self.request_times = [t for t in self.request_times if now - t < 60]
            
            # å¦‚æœè¯·æ±‚è¿‡å¤šï¼Œç­‰å¾…
            if len(self.request_times) >= self.max_requests_per_minute:
                sleep_time = 60 - (now - self.request_times[0]) + 1
                print(f"â³ è¾¾åˆ°é™æµé˜ˆå€¼ï¼Œç­‰å¾… {sleep_time:.1f} ç§’...")
                time.sleep(sleep_time)
                self.request_times = []
            
            # è®°å½•æœ¬æ¬¡è¯·æ±‚æ—¶é—´
            self.request_times.append(now)
    
    def cached_request(self, cache_type: str = 'daily_data', retry_times: int = 3):
        """ç¼“å­˜è£…é¥°å™¨"""
        def decorator(func):
            @wraps(func)
            def wrapper(*args, **kwargs):
                # ç”Ÿæˆç¼“å­˜é”®
                cache_key = self._get_cache_key(func.__name__, *args, **kwargs)
                cache_path = self._get_cache_path(cache_key)
                cache_minutes = self.cache_config.get(cache_type, 30)
                
                # æ£€æŸ¥ç¼“å­˜
                if self._is_cache_valid(cache_path, cache_minutes):
                    cached_result = self._load_cache(cache_path)
                    if cached_result is not None:
                        print(f"ğŸ“¦ ä½¿ç”¨ç¼“å­˜æ•°æ®: {func.__name__}")
                        return cached_result
                
                # ç¼“å­˜æ— æ•ˆï¼Œå‘èµ·è¯·æ±‚
                print(f"ğŸŒ å‘èµ·ç½‘ç»œè¯·æ±‚: {func.__name__}")
                
                # é™æµæ§åˆ¶
                self._wait_for_rate_limit()
                
                # é‡è¯•æœºåˆ¶
                last_error = None
                for attempt in range(retry_times):
                    try:
                        result = func(*args, **kwargs)
                        
                        # ä¿å­˜ç¼“å­˜
                        if result is not None and not (isinstance(result, pd.DataFrame) and result.empty):
                            self._save_cache(cache_path, result)
                        
                        return result
                        
                    except Exception as e:
                        last_error = e
                        if attempt < retry_times - 1:
                            wait_time = (attempt + 1) * 2  # æŒ‡æ•°é€€é¿
                            print(f"âŒ è¯·æ±‚å¤±è´¥ï¼Œ{wait_time}ç§’åé‡è¯• (ç¬¬{attempt+1}æ¬¡): {e}")
                            time.sleep(wait_time)
                        else:
                            print(f"âŒ è¯·æ±‚æœ€ç»ˆå¤±è´¥: {e}")
                
                # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼Œå°è¯•è¿”å›è¿‡æœŸç¼“å­˜
                if os.path.exists(cache_path):
                    print("ğŸ”„ ä½¿ç”¨è¿‡æœŸç¼“å­˜æ•°æ®")
                    return self._load_cache(cache_path)
                
                raise last_error
            
            return wrapper
        return decorator
    
    def clear_cache(self, older_than_hours: int = 24):
        """æ¸…ç†è¿‡æœŸç¼“å­˜"""
        try:
            cutoff_time = time.time() - (older_than_hours * 3600)
            cleared_count = 0
            
            for filename in os.listdir(self.cache_dir):
                if filename.endswith('.json'):
                    filepath = os.path.join(self.cache_dir, filename)
                    if os.path.getmtime(filepath) < cutoff_time:
                        os.remove(filepath)
                        cleared_count += 1
            
            print(f"ğŸ§¹ æ¸…ç†äº† {cleared_count} ä¸ªè¿‡æœŸç¼“å­˜æ–‡ä»¶")
            
        except Exception as e:
            print(f"ç¼“å­˜æ¸…ç†å¤±è´¥: {e}")
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        try:
            cache_files = [f for f in os.listdir(self.cache_dir) if f.endswith('.json')]
            total_size = sum(os.path.getsize(os.path.join(self.cache_dir, f)) for f in cache_files)
            
            return {
                'cache_files': len(cache_files),
                'total_size_mb': round(total_size / 1024 / 1024, 2),
                'recent_requests': len(self.request_times),
                'rate_limit': self.max_requests_per_minute
            }
        except Exception as e:
            return {'error': str(e)}

# å…¨å±€å®ä¾‹
smart_data_manager = SmartDataManager()

# ä½¿ç”¨ç¤ºä¾‹è£…é¥°å™¨
def cached_stock_data(cache_type='daily_data'):
    """è‚¡ç¥¨æ•°æ®ç¼“å­˜è£…é¥°å™¨"""
    return smart_data_manager.cached_request(cache_type=cache_type)

def cached_realtime_data():
    """å®æ—¶æ•°æ®ç¼“å­˜è£…é¥°å™¨"""
    return smart_data_manager.cached_request(cache_type='realtime_data')

def cached_financial_data():
    """è´¢åŠ¡æ•°æ®ç¼“å­˜è£…é¥°å™¨"""
    return smart_data_manager.cached_request(cache_type='financial_data')